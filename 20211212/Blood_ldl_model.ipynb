{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blood_ldl_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lkC-pw0r-iXW",
        "outputId": "3c4a6fbe-c3af-46a7-e5f4-13975a112c4d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wandb\n",
            "  Downloading wandb-0.12.7-py2.py3-none-any.whl (1.7 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.7 MB 5.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n",
            "\u001b[K     |████████████████████████████████| 97 kB 6.7 MB/s \n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Collecting sentry-sdk>=1.0.0\n",
            "  Downloading sentry_sdk-1.5.0-py2.py3-none-any.whl (140 kB)\n",
            "\u001b[K     |████████████████████████████████| 140 kB 51.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading shortuuid-1.0.8-py3-none-any.whl (9.5 kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Collecting yaspin>=1.0.0\n",
            "  Downloading yaspin-2.1.0-py3-none-any.whl (18 kB)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading configparser-5.2.0-py3-none-any.whl (19 kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n",
            "\u001b[K     |████████████████████████████████| 180 kB 63.1 MB/s \n",
            "\u001b[?25hCollecting pathtools\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n",
            "\u001b[K     |████████████████████████████████| 63 kB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Collecting smmap<6,>=3.0.1\n",
            "  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=28932cf43b350c5929f21637d7cc11263374b0c3c294c99bfe6be00460479590\n",
            "  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8807 sha256=da191b3022c7876764803cd209a994b8e3b20c6be9451ebbf0678ad8d22e48d6\n",
            "  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: smmap, gitdb, yaspin, subprocess32, shortuuid, sentry-sdk, pathtools, GitPython, docker-pycreds, configparser, wandb\n",
            "Successfully installed GitPython-3.1.24 configparser-5.2.0 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.5.0 shortuuid-1.0.8 smmap-5.0.0 subprocess32-3.5.4 wandb-0.12.7 yaspin-2.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "__PROJECT_SOURCE__=\"COLAB\""
      ],
      "metadata": {
        "id": "k_VTudIT_T2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ZNkYLOFrij5B"
      },
      "outputs": [],
      "source": [
        "if __PROJECT_SOURCE__==\"COLAB\":\n",
        "    # Import PyDrive and associated libraries.\n",
        "    # This only needs to be done once per notebook.\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    # This only needs to be done once per notebook.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    fname_dir=\"/content/blood/\"\n",
        "    fname_url=\"/content/drive/MyDrive/blood/rework/*agg.csv\"\n",
        "    fname=fname_url.split(\"/\")[-1]\n",
        "elif __PROJECT_SOURCE__==\"LOCAL\":\n",
        "    fname_dir=\"sdfsdfsd\" #working dir\n",
        "    fname_url=\"/content/drive/MyDrive/blood/rework/*agg.csv\" #data source dir\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7yB5l-7i7dS",
        "outputId": "3d63b0f2-4359-4e61-d19b-983b32cc9063"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z3v1BBbJjBqa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir $fname_dir"
      ],
      "metadata": {
        "id": "zU1ZwzodjSD4"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp $fname_url $fname_dir"
      ],
      "metadata": {
        "id": "vXyl_8ELjgrT"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __PROJECT_SOURCE__==\"COLAB\":\n",
        "    drive.flush_and_unmount()\n",
        "    print('Unmount Google Drive :-(')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Y7z49_kBNx",
        "outputId": "23c016cf-c175-4baa-aeb4-c54d3a381d9a"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unmount Google Drive :-(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feldolgozás"
      ],
      "metadata": {
        "id": "JCOsDGPtxztL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy1mbPDRizA2",
        "outputId": "b133866c-6f9c-47f4-8513-a4ce402ed067"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.hdf5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LjJtfapexzDo"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F6TPT1jWknJD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames_list=[\"ldl1_agg.csv\",\"ldl2_agg.csv\"]"
      ],
      "metadata": {
        "id": "5MmhW6aekVHb"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name1=fname_dir+fnames_list[0]\n",
        "print(file_name1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YCcXLaMkrlf",
        "outputId": "3181f521-7b1f-41eb-9c8a-d14d25f4eb8b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blood/ldl1_agg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name2=fname_dir+fnames_list[1]\n",
        "print(file_name2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiMNf4HPBAnV",
        "outputId": "c860b65f-a0a5-42a7-9a86-e3539168e3cb"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blood/ldl2_agg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg1= pd.read_csv(file_name1)\n",
        "df_agg1.describe()\n",
        "df_agg1.drop(df_agg1[df_agg1.absorbance0 < 0].index, inplace=True) # kill the negative elements\n"
      ],
      "metadata": {
        "id": "mPW32Ow0k9BQ"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg2= pd.read_csv(file_name2)\n",
        "df_agg2.describe()\n",
        "df_agg2.drop(df_agg2[df_agg2.absorbance0 < 0].index, inplace=True) # kill the negative elements\n"
      ],
      "metadata": {
        "id": "VbOP7ZtiBKBM"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg=pd.concat([df_agg1,df_agg2], ignore_index=True)"
      ],
      "metadata": {
        "id": "6hFjkYViA-Di"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Y5xTiFTT0r-n",
        "outputId": "3c2d339d-3c23-4a41-8230-c35acb7a3d1b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance134</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>405</td>\n",
              "      <td>0.559508</td>\n",
              "      <td>0.557981</td>\n",
              "      <td>0.562073</td>\n",
              "      <td>0.566272</td>\n",
              "      <td>0.571244</td>\n",
              "      <td>0.577397</td>\n",
              "      <td>0.583213</td>\n",
              "      <td>0.590163</td>\n",
              "      <td>0.598694</td>\n",
              "      <td>0.610475</td>\n",
              "      <td>0.623472</td>\n",
              "      <td>0.636641</td>\n",
              "      <td>0.642579</td>\n",
              "      <td>0.645626</td>\n",
              "      <td>0.647103</td>\n",
              "      <td>0.645668</td>\n",
              "      <td>0.643687</td>\n",
              "      <td>0.640753</td>\n",
              "      <td>0.637975</td>\n",
              "      <td>0.635578</td>\n",
              "      <td>0.633089</td>\n",
              "      <td>0.630470</td>\n",
              "      <td>0.627600</td>\n",
              "      <td>0.624106</td>\n",
              "      <td>0.622028</td>\n",
              "      <td>0.619228</td>\n",
              "      <td>0.617372</td>\n",
              "      <td>0.616006</td>\n",
              "      <td>0.614953</td>\n",
              "      <td>0.614089</td>\n",
              "      <td>0.614401</td>\n",
              "      <td>0.614494</td>\n",
              "      <td>0.615416</td>\n",
              "      <td>0.617037</td>\n",
              "      <td>0.619308</td>\n",
              "      <td>0.623084</td>\n",
              "      <td>0.626258</td>\n",
              "      <td>0.629867</td>\n",
              "      <td>0.634516</td>\n",
              "      <td>...</td>\n",
              "      <td>1.451427</td>\n",
              "      <td>1.440180</td>\n",
              "      <td>1.429945</td>\n",
              "      <td>1.421697</td>\n",
              "      <td>1.412292</td>\n",
              "      <td>1.403772</td>\n",
              "      <td>1.395936</td>\n",
              "      <td>1.385628</td>\n",
              "      <td>1.377189</td>\n",
              "      <td>1.370285</td>\n",
              "      <td>1.363224</td>\n",
              "      <td>1.354198</td>\n",
              "      <td>1.347067</td>\n",
              "      <td>1.339531</td>\n",
              "      <td>1.332175</td>\n",
              "      <td>1.326958</td>\n",
              "      <td>1.320780</td>\n",
              "      <td>1.315768</td>\n",
              "      <td>1.311262</td>\n",
              "      <td>1.304993</td>\n",
              "      <td>1.299778</td>\n",
              "      <td>1.296999</td>\n",
              "      <td>1.292156</td>\n",
              "      <td>1.289398</td>\n",
              "      <td>1.285569</td>\n",
              "      <td>1.286972</td>\n",
              "      <td>1.283523</td>\n",
              "      <td>1.281386</td>\n",
              "      <td>1.279686</td>\n",
              "      <td>1.274868</td>\n",
              "      <td>1.271164</td>\n",
              "      <td>1.265842</td>\n",
              "      <td>1.263005</td>\n",
              "      <td>1.266337</td>\n",
              "      <td>1.297181</td>\n",
              "      <td>1.266201</td>\n",
              "      <td>11886</td>\n",
              "      <td>36.536500</td>\n",
              "      <td>65.203000</td>\n",
              "      <td>88.54285</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>406</td>\n",
              "      <td>0.587427</td>\n",
              "      <td>0.585250</td>\n",
              "      <td>0.589764</td>\n",
              "      <td>0.594061</td>\n",
              "      <td>0.599806</td>\n",
              "      <td>0.604902</td>\n",
              "      <td>0.609408</td>\n",
              "      <td>0.614733</td>\n",
              "      <td>0.620852</td>\n",
              "      <td>0.630668</td>\n",
              "      <td>0.641564</td>\n",
              "      <td>0.653106</td>\n",
              "      <td>0.657908</td>\n",
              "      <td>0.659866</td>\n",
              "      <td>0.660317</td>\n",
              "      <td>0.658655</td>\n",
              "      <td>0.655788</td>\n",
              "      <td>0.652237</td>\n",
              "      <td>0.648927</td>\n",
              "      <td>0.646154</td>\n",
              "      <td>0.643198</td>\n",
              "      <td>0.640632</td>\n",
              "      <td>0.637537</td>\n",
              "      <td>0.633689</td>\n",
              "      <td>0.631356</td>\n",
              "      <td>0.628085</td>\n",
              "      <td>0.625785</td>\n",
              "      <td>0.624119</td>\n",
              "      <td>0.622667</td>\n",
              "      <td>0.621206</td>\n",
              "      <td>0.620691</td>\n",
              "      <td>0.620383</td>\n",
              "      <td>0.620737</td>\n",
              "      <td>0.621599</td>\n",
              "      <td>0.623025</td>\n",
              "      <td>0.625937</td>\n",
              "      <td>0.628400</td>\n",
              "      <td>0.631338</td>\n",
              "      <td>0.635659</td>\n",
              "      <td>...</td>\n",
              "      <td>1.475231</td>\n",
              "      <td>1.463203</td>\n",
              "      <td>1.454463</td>\n",
              "      <td>1.445494</td>\n",
              "      <td>1.436059</td>\n",
              "      <td>1.426811</td>\n",
              "      <td>1.417521</td>\n",
              "      <td>1.406092</td>\n",
              "      <td>1.397415</td>\n",
              "      <td>1.388936</td>\n",
              "      <td>1.383776</td>\n",
              "      <td>1.372381</td>\n",
              "      <td>1.364249</td>\n",
              "      <td>1.355797</td>\n",
              "      <td>1.347775</td>\n",
              "      <td>1.341057</td>\n",
              "      <td>1.333965</td>\n",
              "      <td>1.328754</td>\n",
              "      <td>1.321832</td>\n",
              "      <td>1.315572</td>\n",
              "      <td>1.309635</td>\n",
              "      <td>1.303772</td>\n",
              "      <td>1.299180</td>\n",
              "      <td>1.295138</td>\n",
              "      <td>1.292522</td>\n",
              "      <td>1.289935</td>\n",
              "      <td>1.287213</td>\n",
              "      <td>1.282831</td>\n",
              "      <td>1.277070</td>\n",
              "      <td>1.270260</td>\n",
              "      <td>1.253691</td>\n",
              "      <td>1.242504</td>\n",
              "      <td>1.236326</td>\n",
              "      <td>1.236166</td>\n",
              "      <td>1.250518</td>\n",
              "      <td>1.236380</td>\n",
              "      <td>11889</td>\n",
              "      <td>39.054667</td>\n",
              "      <td>56.012500</td>\n",
              "      <td>159.68645</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>407</td>\n",
              "      <td>0.477241</td>\n",
              "      <td>0.481792</td>\n",
              "      <td>0.483792</td>\n",
              "      <td>0.489861</td>\n",
              "      <td>0.497037</td>\n",
              "      <td>0.504735</td>\n",
              "      <td>0.509637</td>\n",
              "      <td>0.514202</td>\n",
              "      <td>0.520144</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.541345</td>\n",
              "      <td>0.550381</td>\n",
              "      <td>0.555803</td>\n",
              "      <td>0.558563</td>\n",
              "      <td>0.559343</td>\n",
              "      <td>0.558502</td>\n",
              "      <td>0.556676</td>\n",
              "      <td>0.554130</td>\n",
              "      <td>0.551494</td>\n",
              "      <td>0.547095</td>\n",
              "      <td>0.543851</td>\n",
              "      <td>0.540543</td>\n",
              "      <td>0.537108</td>\n",
              "      <td>0.533737</td>\n",
              "      <td>0.530775</td>\n",
              "      <td>0.527208</td>\n",
              "      <td>0.524530</td>\n",
              "      <td>0.523219</td>\n",
              "      <td>0.521271</td>\n",
              "      <td>0.520013</td>\n",
              "      <td>0.519327</td>\n",
              "      <td>0.518718</td>\n",
              "      <td>0.518963</td>\n",
              "      <td>0.519878</td>\n",
              "      <td>0.520868</td>\n",
              "      <td>0.522727</td>\n",
              "      <td>0.525264</td>\n",
              "      <td>0.528919</td>\n",
              "      <td>0.532656</td>\n",
              "      <td>...</td>\n",
              "      <td>1.513493</td>\n",
              "      <td>1.494378</td>\n",
              "      <td>1.478303</td>\n",
              "      <td>1.463623</td>\n",
              "      <td>1.448563</td>\n",
              "      <td>1.434307</td>\n",
              "      <td>1.419192</td>\n",
              "      <td>1.402043</td>\n",
              "      <td>1.388613</td>\n",
              "      <td>1.375551</td>\n",
              "      <td>1.366222</td>\n",
              "      <td>1.351601</td>\n",
              "      <td>1.340437</td>\n",
              "      <td>1.327134</td>\n",
              "      <td>1.318319</td>\n",
              "      <td>1.309765</td>\n",
              "      <td>1.300136</td>\n",
              "      <td>1.292696</td>\n",
              "      <td>1.283508</td>\n",
              "      <td>1.275061</td>\n",
              "      <td>1.269340</td>\n",
              "      <td>1.263197</td>\n",
              "      <td>1.258333</td>\n",
              "      <td>1.253829</td>\n",
              "      <td>1.251677</td>\n",
              "      <td>1.249435</td>\n",
              "      <td>1.246182</td>\n",
              "      <td>1.243464</td>\n",
              "      <td>1.237783</td>\n",
              "      <td>1.227517</td>\n",
              "      <td>1.206936</td>\n",
              "      <td>1.179302</td>\n",
              "      <td>1.162252</td>\n",
              "      <td>1.156460</td>\n",
              "      <td>1.167845</td>\n",
              "      <td>1.159627</td>\n",
              "      <td>11890</td>\n",
              "      <td>46.116000</td>\n",
              "      <td>46.849000</td>\n",
              "      <td>99.20000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>408</td>\n",
              "      <td>0.565670</td>\n",
              "      <td>0.564820</td>\n",
              "      <td>0.569749</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.580790</td>\n",
              "      <td>0.586531</td>\n",
              "      <td>0.591289</td>\n",
              "      <td>0.596792</td>\n",
              "      <td>0.603492</td>\n",
              "      <td>0.613606</td>\n",
              "      <td>0.625469</td>\n",
              "      <td>0.637730</td>\n",
              "      <td>0.642756</td>\n",
              "      <td>0.645300</td>\n",
              "      <td>0.646095</td>\n",
              "      <td>0.644386</td>\n",
              "      <td>0.641853</td>\n",
              "      <td>0.638425</td>\n",
              "      <td>0.635619</td>\n",
              "      <td>0.633162</td>\n",
              "      <td>0.630523</td>\n",
              "      <td>0.627955</td>\n",
              "      <td>0.624878</td>\n",
              "      <td>0.621162</td>\n",
              "      <td>0.618793</td>\n",
              "      <td>0.615519</td>\n",
              "      <td>0.613485</td>\n",
              "      <td>0.611918</td>\n",
              "      <td>0.610617</td>\n",
              "      <td>0.609168</td>\n",
              "      <td>0.608868</td>\n",
              "      <td>0.608688</td>\n",
              "      <td>0.609372</td>\n",
              "      <td>0.610378</td>\n",
              "      <td>0.612109</td>\n",
              "      <td>0.615408</td>\n",
              "      <td>0.618294</td>\n",
              "      <td>0.621513</td>\n",
              "      <td>0.626012</td>\n",
              "      <td>...</td>\n",
              "      <td>1.520369</td>\n",
              "      <td>1.506006</td>\n",
              "      <td>1.496877</td>\n",
              "      <td>1.486883</td>\n",
              "      <td>1.475934</td>\n",
              "      <td>1.466819</td>\n",
              "      <td>1.456462</td>\n",
              "      <td>1.445260</td>\n",
              "      <td>1.435506</td>\n",
              "      <td>1.426113</td>\n",
              "      <td>1.419729</td>\n",
              "      <td>1.409409</td>\n",
              "      <td>1.402097</td>\n",
              "      <td>1.391310</td>\n",
              "      <td>1.384826</td>\n",
              "      <td>1.377594</td>\n",
              "      <td>1.371748</td>\n",
              "      <td>1.366276</td>\n",
              "      <td>1.358685</td>\n",
              "      <td>1.354177</td>\n",
              "      <td>1.348492</td>\n",
              "      <td>1.342599</td>\n",
              "      <td>1.337098</td>\n",
              "      <td>1.332178</td>\n",
              "      <td>1.329692</td>\n",
              "      <td>1.329856</td>\n",
              "      <td>1.327782</td>\n",
              "      <td>1.323786</td>\n",
              "      <td>1.317777</td>\n",
              "      <td>1.305306</td>\n",
              "      <td>1.290804</td>\n",
              "      <td>1.276198</td>\n",
              "      <td>1.268350</td>\n",
              "      <td>1.263184</td>\n",
              "      <td>1.284706</td>\n",
              "      <td>1.269695</td>\n",
              "      <td>11891</td>\n",
              "      <td>40.384833</td>\n",
              "      <td>51.917167</td>\n",
              "      <td>164.71290</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>409</td>\n",
              "      <td>0.462006</td>\n",
              "      <td>0.463919</td>\n",
              "      <td>0.466006</td>\n",
              "      <td>0.470398</td>\n",
              "      <td>0.474160</td>\n",
              "      <td>0.479535</td>\n",
              "      <td>0.484806</td>\n",
              "      <td>0.490237</td>\n",
              "      <td>0.499680</td>\n",
              "      <td>0.509926</td>\n",
              "      <td>0.522268</td>\n",
              "      <td>0.532148</td>\n",
              "      <td>0.538059</td>\n",
              "      <td>0.542097</td>\n",
              "      <td>0.543162</td>\n",
              "      <td>0.542517</td>\n",
              "      <td>0.541707</td>\n",
              "      <td>0.539483</td>\n",
              "      <td>0.537062</td>\n",
              "      <td>0.533966</td>\n",
              "      <td>0.531556</td>\n",
              "      <td>0.529130</td>\n",
              "      <td>0.525741</td>\n",
              "      <td>0.521571</td>\n",
              "      <td>0.516842</td>\n",
              "      <td>0.513043</td>\n",
              "      <td>0.510353</td>\n",
              "      <td>0.507669</td>\n",
              "      <td>0.505915</td>\n",
              "      <td>0.503540</td>\n",
              "      <td>0.502311</td>\n",
              "      <td>0.502070</td>\n",
              "      <td>0.501865</td>\n",
              "      <td>0.502888</td>\n",
              "      <td>0.503878</td>\n",
              "      <td>0.506279</td>\n",
              "      <td>0.508641</td>\n",
              "      <td>0.511671</td>\n",
              "      <td>0.515179</td>\n",
              "      <td>...</td>\n",
              "      <td>1.343350</td>\n",
              "      <td>1.332708</td>\n",
              "      <td>1.319521</td>\n",
              "      <td>1.308515</td>\n",
              "      <td>1.298146</td>\n",
              "      <td>1.288379</td>\n",
              "      <td>1.277194</td>\n",
              "      <td>1.267351</td>\n",
              "      <td>1.254786</td>\n",
              "      <td>1.246378</td>\n",
              "      <td>1.239163</td>\n",
              "      <td>1.228400</td>\n",
              "      <td>1.219838</td>\n",
              "      <td>1.210519</td>\n",
              "      <td>1.203041</td>\n",
              "      <td>1.196190</td>\n",
              "      <td>1.189647</td>\n",
              "      <td>1.183361</td>\n",
              "      <td>1.174566</td>\n",
              "      <td>1.170406</td>\n",
              "      <td>1.165284</td>\n",
              "      <td>1.159469</td>\n",
              "      <td>1.155536</td>\n",
              "      <td>1.153178</td>\n",
              "      <td>1.149640</td>\n",
              "      <td>1.149195</td>\n",
              "      <td>1.147773</td>\n",
              "      <td>1.148173</td>\n",
              "      <td>1.147194</td>\n",
              "      <td>1.146067</td>\n",
              "      <td>1.146250</td>\n",
              "      <td>1.149830</td>\n",
              "      <td>1.153864</td>\n",
              "      <td>1.162086</td>\n",
              "      <td>1.177496</td>\n",
              "      <td>1.168120</td>\n",
              "      <td>11966</td>\n",
              "      <td>36.485833</td>\n",
              "      <td>50.900167</td>\n",
              "      <td>98.99000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 175 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  absorbance0  ...   humidity  cholesterol_ldl_value\n",
              "537         405     0.559508  ...  65.203000               88.54285\n",
              "538         406     0.587427  ...  56.012500              159.68645\n",
              "539         407     0.477241  ...  46.849000               99.20000\n",
              "540         408     0.565670  ...  51.917167              164.71290\n",
              "541         409     0.462006  ...  50.900167               98.99000\n",
              "\n",
              "[5 rows x 175 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "NzXlFWOu0W2K",
        "outputId": "5b023acc-7715-4857-a7a7-11554321cf31"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance134</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>170.702952</td>\n",
              "      <td>0.520451</td>\n",
              "      <td>0.522427</td>\n",
              "      <td>0.525972</td>\n",
              "      <td>0.530680</td>\n",
              "      <td>0.536854</td>\n",
              "      <td>0.543551</td>\n",
              "      <td>0.549648</td>\n",
              "      <td>0.555232</td>\n",
              "      <td>0.563119</td>\n",
              "      <td>0.574407</td>\n",
              "      <td>0.587313</td>\n",
              "      <td>0.598591</td>\n",
              "      <td>0.605160</td>\n",
              "      <td>0.608182</td>\n",
              "      <td>0.609431</td>\n",
              "      <td>0.609150</td>\n",
              "      <td>0.607543</td>\n",
              "      <td>0.605090</td>\n",
              "      <td>0.602129</td>\n",
              "      <td>0.598775</td>\n",
              "      <td>0.595773</td>\n",
              "      <td>0.592707</td>\n",
              "      <td>0.589487</td>\n",
              "      <td>0.585683</td>\n",
              "      <td>0.582415</td>\n",
              "      <td>0.579220</td>\n",
              "      <td>0.576781</td>\n",
              "      <td>0.574689</td>\n",
              "      <td>0.572943</td>\n",
              "      <td>0.571480</td>\n",
              "      <td>0.570644</td>\n",
              "      <td>0.570247</td>\n",
              "      <td>0.570480</td>\n",
              "      <td>0.571400</td>\n",
              "      <td>0.572933</td>\n",
              "      <td>0.575169</td>\n",
              "      <td>0.577869</td>\n",
              "      <td>0.581283</td>\n",
              "      <td>0.585061</td>\n",
              "      <td>...</td>\n",
              "      <td>1.510277</td>\n",
              "      <td>1.497814</td>\n",
              "      <td>1.485553</td>\n",
              "      <td>1.474109</td>\n",
              "      <td>1.463112</td>\n",
              "      <td>1.450626</td>\n",
              "      <td>1.439371</td>\n",
              "      <td>1.428135</td>\n",
              "      <td>1.417230</td>\n",
              "      <td>1.406798</td>\n",
              "      <td>1.398556</td>\n",
              "      <td>1.385953</td>\n",
              "      <td>1.377019</td>\n",
              "      <td>1.368533</td>\n",
              "      <td>1.360171</td>\n",
              "      <td>1.351819</td>\n",
              "      <td>1.344645</td>\n",
              "      <td>1.337181</td>\n",
              "      <td>1.330049</td>\n",
              "      <td>1.323455</td>\n",
              "      <td>1.316835</td>\n",
              "      <td>1.311906</td>\n",
              "      <td>1.307739</td>\n",
              "      <td>1.304195</td>\n",
              "      <td>1.301982</td>\n",
              "      <td>1.301178</td>\n",
              "      <td>1.300802</td>\n",
              "      <td>1.300394</td>\n",
              "      <td>1.299197</td>\n",
              "      <td>1.295539</td>\n",
              "      <td>1.289453</td>\n",
              "      <td>1.282270</td>\n",
              "      <td>1.275940</td>\n",
              "      <td>1.272953</td>\n",
              "      <td>1.287608</td>\n",
              "      <td>1.275556</td>\n",
              "      <td>6927.533210</td>\n",
              "      <td>42.111418</td>\n",
              "      <td>39.222955</td>\n",
              "      <td>111.436592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>120.507914</td>\n",
              "      <td>0.040351</td>\n",
              "      <td>0.039984</td>\n",
              "      <td>0.040093</td>\n",
              "      <td>0.040035</td>\n",
              "      <td>0.040144</td>\n",
              "      <td>0.040521</td>\n",
              "      <td>0.040435</td>\n",
              "      <td>0.040376</td>\n",
              "      <td>0.040171</td>\n",
              "      <td>0.039981</td>\n",
              "      <td>0.039891</td>\n",
              "      <td>0.039888</td>\n",
              "      <td>0.039833</td>\n",
              "      <td>0.039825</td>\n",
              "      <td>0.039900</td>\n",
              "      <td>0.039866</td>\n",
              "      <td>0.039767</td>\n",
              "      <td>0.039718</td>\n",
              "      <td>0.039647</td>\n",
              "      <td>0.039748</td>\n",
              "      <td>0.039703</td>\n",
              "      <td>0.039702</td>\n",
              "      <td>0.039726</td>\n",
              "      <td>0.039846</td>\n",
              "      <td>0.040008</td>\n",
              "      <td>0.040042</td>\n",
              "      <td>0.040114</td>\n",
              "      <td>0.040156</td>\n",
              "      <td>0.040191</td>\n",
              "      <td>0.040191</td>\n",
              "      <td>0.040187</td>\n",
              "      <td>0.040152</td>\n",
              "      <td>0.040178</td>\n",
              "      <td>0.040161</td>\n",
              "      <td>0.040140</td>\n",
              "      <td>0.040145</td>\n",
              "      <td>0.040169</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.040271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.093496</td>\n",
              "      <td>0.091700</td>\n",
              "      <td>0.090305</td>\n",
              "      <td>0.088924</td>\n",
              "      <td>0.087603</td>\n",
              "      <td>0.085971</td>\n",
              "      <td>0.084668</td>\n",
              "      <td>0.083410</td>\n",
              "      <td>0.082486</td>\n",
              "      <td>0.081525</td>\n",
              "      <td>0.080697</td>\n",
              "      <td>0.079354</td>\n",
              "      <td>0.078591</td>\n",
              "      <td>0.077906</td>\n",
              "      <td>0.077287</td>\n",
              "      <td>0.076565</td>\n",
              "      <td>0.076015</td>\n",
              "      <td>0.075402</td>\n",
              "      <td>0.074832</td>\n",
              "      <td>0.074297</td>\n",
              "      <td>0.073843</td>\n",
              "      <td>0.073528</td>\n",
              "      <td>0.073310</td>\n",
              "      <td>0.073180</td>\n",
              "      <td>0.073283</td>\n",
              "      <td>0.073591</td>\n",
              "      <td>0.074264</td>\n",
              "      <td>0.075230</td>\n",
              "      <td>0.076718</td>\n",
              "      <td>0.079323</td>\n",
              "      <td>0.083310</td>\n",
              "      <td>0.088549</td>\n",
              "      <td>0.093928</td>\n",
              "      <td>0.098151</td>\n",
              "      <td>0.104133</td>\n",
              "      <td>0.101605</td>\n",
              "      <td>3094.955454</td>\n",
              "      <td>3.420590</td>\n",
              "      <td>8.948404</td>\n",
              "      <td>44.306810</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400606</td>\n",
              "      <td>0.403625</td>\n",
              "      <td>0.407266</td>\n",
              "      <td>0.411518</td>\n",
              "      <td>0.416126</td>\n",
              "      <td>0.421518</td>\n",
              "      <td>0.426629</td>\n",
              "      <td>0.433139</td>\n",
              "      <td>0.444069</td>\n",
              "      <td>0.456182</td>\n",
              "      <td>0.469249</td>\n",
              "      <td>0.480239</td>\n",
              "      <td>0.486473</td>\n",
              "      <td>0.489661</td>\n",
              "      <td>0.490777</td>\n",
              "      <td>0.490226</td>\n",
              "      <td>0.488814</td>\n",
              "      <td>0.486476</td>\n",
              "      <td>0.483668</td>\n",
              "      <td>0.479653</td>\n",
              "      <td>0.476146</td>\n",
              "      <td>0.473155</td>\n",
              "      <td>0.469834</td>\n",
              "      <td>0.466424</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.459335</td>\n",
              "      <td>0.457297</td>\n",
              "      <td>0.455538</td>\n",
              "      <td>0.453853</td>\n",
              "      <td>0.452486</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>0.451624</td>\n",
              "      <td>0.452026</td>\n",
              "      <td>0.453133</td>\n",
              "      <td>0.454851</td>\n",
              "      <td>0.457279</td>\n",
              "      <td>0.460600</td>\n",
              "      <td>0.463820</td>\n",
              "      <td>0.467916</td>\n",
              "      <td>...</td>\n",
              "      <td>1.212829</td>\n",
              "      <td>1.202957</td>\n",
              "      <td>1.193580</td>\n",
              "      <td>1.184336</td>\n",
              "      <td>1.175013</td>\n",
              "      <td>1.163368</td>\n",
              "      <td>1.154944</td>\n",
              "      <td>1.145616</td>\n",
              "      <td>1.137301</td>\n",
              "      <td>1.128859</td>\n",
              "      <td>1.118666</td>\n",
              "      <td>1.110864</td>\n",
              "      <td>1.102883</td>\n",
              "      <td>1.096296</td>\n",
              "      <td>1.089810</td>\n",
              "      <td>1.080864</td>\n",
              "      <td>1.075002</td>\n",
              "      <td>1.068992</td>\n",
              "      <td>1.063488</td>\n",
              "      <td>1.057937</td>\n",
              "      <td>1.052125</td>\n",
              "      <td>1.047685</td>\n",
              "      <td>1.044301</td>\n",
              "      <td>1.041746</td>\n",
              "      <td>1.039873</td>\n",
              "      <td>1.038416</td>\n",
              "      <td>1.039063</td>\n",
              "      <td>1.037324</td>\n",
              "      <td>1.036100</td>\n",
              "      <td>1.032403</td>\n",
              "      <td>1.026585</td>\n",
              "      <td>1.020585</td>\n",
              "      <td>1.004983</td>\n",
              "      <td>0.981338</td>\n",
              "      <td>0.969416</td>\n",
              "      <td>0.950952</td>\n",
              "      <td>1974.000000</td>\n",
              "      <td>31.384333</td>\n",
              "      <td>16.136667</td>\n",
              "      <td>5.300000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>68.000000</td>\n",
              "      <td>0.492162</td>\n",
              "      <td>0.494457</td>\n",
              "      <td>0.498396</td>\n",
              "      <td>0.502493</td>\n",
              "      <td>0.508495</td>\n",
              "      <td>0.513840</td>\n",
              "      <td>0.520323</td>\n",
              "      <td>0.526687</td>\n",
              "      <td>0.535248</td>\n",
              "      <td>0.547601</td>\n",
              "      <td>0.561213</td>\n",
              "      <td>0.572464</td>\n",
              "      <td>0.579220</td>\n",
              "      <td>0.582228</td>\n",
              "      <td>0.583382</td>\n",
              "      <td>0.583044</td>\n",
              "      <td>0.581269</td>\n",
              "      <td>0.578919</td>\n",
              "      <td>0.576212</td>\n",
              "      <td>0.572606</td>\n",
              "      <td>0.569006</td>\n",
              "      <td>0.565879</td>\n",
              "      <td>0.562660</td>\n",
              "      <td>0.558955</td>\n",
              "      <td>0.555664</td>\n",
              "      <td>0.552372</td>\n",
              "      <td>0.549645</td>\n",
              "      <td>0.547153</td>\n",
              "      <td>0.545535</td>\n",
              "      <td>0.543872</td>\n",
              "      <td>0.543126</td>\n",
              "      <td>0.542971</td>\n",
              "      <td>0.543221</td>\n",
              "      <td>0.544237</td>\n",
              "      <td>0.545786</td>\n",
              "      <td>0.548325</td>\n",
              "      <td>0.551150</td>\n",
              "      <td>0.554563</td>\n",
              "      <td>0.558216</td>\n",
              "      <td>...</td>\n",
              "      <td>1.448925</td>\n",
              "      <td>1.438055</td>\n",
              "      <td>1.427390</td>\n",
              "      <td>1.418313</td>\n",
              "      <td>1.408198</td>\n",
              "      <td>1.397676</td>\n",
              "      <td>1.386605</td>\n",
              "      <td>1.375685</td>\n",
              "      <td>1.365357</td>\n",
              "      <td>1.355396</td>\n",
              "      <td>1.347791</td>\n",
              "      <td>1.337279</td>\n",
              "      <td>1.328478</td>\n",
              "      <td>1.320503</td>\n",
              "      <td>1.312188</td>\n",
              "      <td>1.303734</td>\n",
              "      <td>1.297646</td>\n",
              "      <td>1.291290</td>\n",
              "      <td>1.283525</td>\n",
              "      <td>1.277472</td>\n",
              "      <td>1.271808</td>\n",
              "      <td>1.266912</td>\n",
              "      <td>1.263202</td>\n",
              "      <td>1.259202</td>\n",
              "      <td>1.257043</td>\n",
              "      <td>1.255547</td>\n",
              "      <td>1.254069</td>\n",
              "      <td>1.251106</td>\n",
              "      <td>1.250801</td>\n",
              "      <td>1.245398</td>\n",
              "      <td>1.236956</td>\n",
              "      <td>1.228441</td>\n",
              "      <td>1.216640</td>\n",
              "      <td>1.214438</td>\n",
              "      <td>1.219523</td>\n",
              "      <td>1.210930</td>\n",
              "      <td>4150.500000</td>\n",
              "      <td>39.420750</td>\n",
              "      <td>33.587333</td>\n",
              "      <td>81.347500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>136.500000</td>\n",
              "      <td>0.517637</td>\n",
              "      <td>0.519890</td>\n",
              "      <td>0.524229</td>\n",
              "      <td>0.528808</td>\n",
              "      <td>0.534595</td>\n",
              "      <td>0.541708</td>\n",
              "      <td>0.547380</td>\n",
              "      <td>0.553991</td>\n",
              "      <td>0.561483</td>\n",
              "      <td>0.572200</td>\n",
              "      <td>0.585533</td>\n",
              "      <td>0.597325</td>\n",
              "      <td>0.603621</td>\n",
              "      <td>0.607155</td>\n",
              "      <td>0.608601</td>\n",
              "      <td>0.608395</td>\n",
              "      <td>0.606871</td>\n",
              "      <td>0.604356</td>\n",
              "      <td>0.601704</td>\n",
              "      <td>0.598210</td>\n",
              "      <td>0.594825</td>\n",
              "      <td>0.591652</td>\n",
              "      <td>0.588568</td>\n",
              "      <td>0.584530</td>\n",
              "      <td>0.581085</td>\n",
              "      <td>0.577706</td>\n",
              "      <td>0.575218</td>\n",
              "      <td>0.573276</td>\n",
              "      <td>0.571611</td>\n",
              "      <td>0.570549</td>\n",
              "      <td>0.569648</td>\n",
              "      <td>0.569021</td>\n",
              "      <td>0.568656</td>\n",
              "      <td>0.569177</td>\n",
              "      <td>0.570510</td>\n",
              "      <td>0.572768</td>\n",
              "      <td>0.575309</td>\n",
              "      <td>0.578820</td>\n",
              "      <td>0.582439</td>\n",
              "      <td>...</td>\n",
              "      <td>1.522105</td>\n",
              "      <td>1.509993</td>\n",
              "      <td>1.496886</td>\n",
              "      <td>1.484712</td>\n",
              "      <td>1.472726</td>\n",
              "      <td>1.459901</td>\n",
              "      <td>1.447849</td>\n",
              "      <td>1.434799</td>\n",
              "      <td>1.422255</td>\n",
              "      <td>1.411197</td>\n",
              "      <td>1.402132</td>\n",
              "      <td>1.389828</td>\n",
              "      <td>1.380030</td>\n",
              "      <td>1.371024</td>\n",
              "      <td>1.361727</td>\n",
              "      <td>1.351912</td>\n",
              "      <td>1.344241</td>\n",
              "      <td>1.337637</td>\n",
              "      <td>1.330476</td>\n",
              "      <td>1.324922</td>\n",
              "      <td>1.318474</td>\n",
              "      <td>1.313972</td>\n",
              "      <td>1.309751</td>\n",
              "      <td>1.306991</td>\n",
              "      <td>1.304351</td>\n",
              "      <td>1.303537</td>\n",
              "      <td>1.303173</td>\n",
              "      <td>1.302108</td>\n",
              "      <td>1.300589</td>\n",
              "      <td>1.296182</td>\n",
              "      <td>1.291905</td>\n",
              "      <td>1.282205</td>\n",
              "      <td>1.273889</td>\n",
              "      <td>1.269692</td>\n",
              "      <td>1.286520</td>\n",
              "      <td>1.271123</td>\n",
              "      <td>7086.500000</td>\n",
              "      <td>41.806083</td>\n",
              "      <td>38.467583</td>\n",
              "      <td>109.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>273.750000</td>\n",
              "      <td>0.545690</td>\n",
              "      <td>0.547908</td>\n",
              "      <td>0.552079</td>\n",
              "      <td>0.557400</td>\n",
              "      <td>0.564152</td>\n",
              "      <td>0.571383</td>\n",
              "      <td>0.577585</td>\n",
              "      <td>0.582518</td>\n",
              "      <td>0.590188</td>\n",
              "      <td>0.600986</td>\n",
              "      <td>0.614931</td>\n",
              "      <td>0.626134</td>\n",
              "      <td>0.632340</td>\n",
              "      <td>0.635539</td>\n",
              "      <td>0.636588</td>\n",
              "      <td>0.635807</td>\n",
              "      <td>0.634326</td>\n",
              "      <td>0.632133</td>\n",
              "      <td>0.629180</td>\n",
              "      <td>0.625997</td>\n",
              "      <td>0.623065</td>\n",
              "      <td>0.619732</td>\n",
              "      <td>0.616301</td>\n",
              "      <td>0.611912</td>\n",
              "      <td>0.609308</td>\n",
              "      <td>0.606632</td>\n",
              "      <td>0.604324</td>\n",
              "      <td>0.602069</td>\n",
              "      <td>0.600773</td>\n",
              "      <td>0.599572</td>\n",
              "      <td>0.598980</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.599201</td>\n",
              "      <td>0.600384</td>\n",
              "      <td>0.602131</td>\n",
              "      <td>0.604517</td>\n",
              "      <td>0.607264</td>\n",
              "      <td>0.610573</td>\n",
              "      <td>0.614266</td>\n",
              "      <td>...</td>\n",
              "      <td>1.575025</td>\n",
              "      <td>1.559759</td>\n",
              "      <td>1.547204</td>\n",
              "      <td>1.533789</td>\n",
              "      <td>1.521686</td>\n",
              "      <td>1.508628</td>\n",
              "      <td>1.496775</td>\n",
              "      <td>1.483442</td>\n",
              "      <td>1.472454</td>\n",
              "      <td>1.460157</td>\n",
              "      <td>1.452229</td>\n",
              "      <td>1.438763</td>\n",
              "      <td>1.429058</td>\n",
              "      <td>1.419599</td>\n",
              "      <td>1.409979</td>\n",
              "      <td>1.400365</td>\n",
              "      <td>1.392393</td>\n",
              "      <td>1.383955</td>\n",
              "      <td>1.376497</td>\n",
              "      <td>1.369636</td>\n",
              "      <td>1.363443</td>\n",
              "      <td>1.357990</td>\n",
              "      <td>1.353691</td>\n",
              "      <td>1.350482</td>\n",
              "      <td>1.348698</td>\n",
              "      <td>1.347153</td>\n",
              "      <td>1.347032</td>\n",
              "      <td>1.346518</td>\n",
              "      <td>1.345891</td>\n",
              "      <td>1.342652</td>\n",
              "      <td>1.341573</td>\n",
              "      <td>1.338421</td>\n",
              "      <td>1.332192</td>\n",
              "      <td>1.332948</td>\n",
              "      <td>1.351667</td>\n",
              "      <td>1.339838</td>\n",
              "      <td>9618.500000</td>\n",
              "      <td>44.381750</td>\n",
              "      <td>45.301333</td>\n",
              "      <td>136.480000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>409.000000</td>\n",
              "      <td>0.696677</td>\n",
              "      <td>0.696001</td>\n",
              "      <td>0.697815</td>\n",
              "      <td>0.700696</td>\n",
              "      <td>0.704963</td>\n",
              "      <td>0.709175</td>\n",
              "      <td>0.716203</td>\n",
              "      <td>0.722194</td>\n",
              "      <td>0.731920</td>\n",
              "      <td>0.744236</td>\n",
              "      <td>0.757309</td>\n",
              "      <td>0.771393</td>\n",
              "      <td>0.778315</td>\n",
              "      <td>0.782179</td>\n",
              "      <td>0.782835</td>\n",
              "      <td>0.782078</td>\n",
              "      <td>0.779667</td>\n",
              "      <td>0.776754</td>\n",
              "      <td>0.774371</td>\n",
              "      <td>0.771715</td>\n",
              "      <td>0.769261</td>\n",
              "      <td>0.766669</td>\n",
              "      <td>0.763244</td>\n",
              "      <td>0.759328</td>\n",
              "      <td>0.756597</td>\n",
              "      <td>0.752855</td>\n",
              "      <td>0.750512</td>\n",
              "      <td>0.748430</td>\n",
              "      <td>0.746127</td>\n",
              "      <td>0.743960</td>\n",
              "      <td>0.742321</td>\n",
              "      <td>0.741800</td>\n",
              "      <td>0.741419</td>\n",
              "      <td>0.741988</td>\n",
              "      <td>0.743058</td>\n",
              "      <td>0.745721</td>\n",
              "      <td>0.748387</td>\n",
              "      <td>0.751435</td>\n",
              "      <td>0.755251</td>\n",
              "      <td>...</td>\n",
              "      <td>1.766199</td>\n",
              "      <td>1.752160</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.729738</td>\n",
              "      <td>1.718851</td>\n",
              "      <td>1.703801</td>\n",
              "      <td>1.691461</td>\n",
              "      <td>1.676763</td>\n",
              "      <td>1.665650</td>\n",
              "      <td>1.656921</td>\n",
              "      <td>1.639813</td>\n",
              "      <td>1.628050</td>\n",
              "      <td>1.619605</td>\n",
              "      <td>1.606320</td>\n",
              "      <td>1.599001</td>\n",
              "      <td>1.585844</td>\n",
              "      <td>1.576560</td>\n",
              "      <td>1.566763</td>\n",
              "      <td>1.559222</td>\n",
              "      <td>1.548711</td>\n",
              "      <td>1.540025</td>\n",
              "      <td>1.533886</td>\n",
              "      <td>1.530444</td>\n",
              "      <td>1.530373</td>\n",
              "      <td>1.528527</td>\n",
              "      <td>1.529409</td>\n",
              "      <td>1.531209</td>\n",
              "      <td>1.535637</td>\n",
              "      <td>1.539162</td>\n",
              "      <td>1.539344</td>\n",
              "      <td>1.541236</td>\n",
              "      <td>1.543308</td>\n",
              "      <td>1.550344</td>\n",
              "      <td>1.552939</td>\n",
              "      <td>1.599580</td>\n",
              "      <td>1.575339</td>\n",
              "      <td>11966.000000</td>\n",
              "      <td>52.678500</td>\n",
              "      <td>65.659833</td>\n",
              "      <td>310.800000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 175 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  absorbance0  ...    humidity  cholesterol_ldl_value\n",
              "count  542.000000   542.000000  ...  542.000000             542.000000\n",
              "mean   170.702952     0.520451  ...   39.222955             111.436592\n",
              "std    120.507914     0.040351  ...    8.948404              44.306810\n",
              "min      0.000000     0.400606  ...   16.136667               5.300000\n",
              "25%     68.000000     0.492162  ...   33.587333              81.347500\n",
              "50%    136.500000     0.517637  ...   38.467583             109.850000\n",
              "75%    273.750000     0.545690  ...   45.301333             136.480000\n",
              "max    409.000000     0.696677  ...   65.659833             310.800000\n",
              "\n",
              "[8 rows x 175 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout,Embedding, Flatten,LayerNormalization\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "import keras.optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld,mse\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "YTJL576ZmB3U"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_columns=df_agg.columns[1:-4]\n",
        "y_columns=df_agg.columns[-1]\n",
        "X_=df_agg[X_columns]\n",
        "y_=df_agg[y_columns]"
      ],
      "metadata": {
        "id": "tyAOeKvNmIFk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_=np.array(y_)"
      ],
      "metadata": {
        "id": "T1yh_hqpqEt4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_=y_.reshape(-1,1)"
      ],
      "metadata": {
        "id": "Mstvf1lXqSEe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ugaWW0KOq90O",
        "outputId": "26ef6184-67b2-48d9-947b-92a35322f53a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[105.6    ],\n",
              "       [ 72.4    ],\n",
              "       [ 68.4    ],\n",
              "       [106.     ],\n",
              "       [ 31.2    ],\n",
              "       [ 40.6    ],\n",
              "       [ 82.6    ],\n",
              "       [ 32.7    ],\n",
              "       [ 64.2    ],\n",
              "       [151.     ],\n",
              "       [ 94.8    ],\n",
              "       [ 78.6    ],\n",
              "       [110.8    ],\n",
              "       [148.2    ],\n",
              "       [ 82.6    ],\n",
              "       [130.2    ],\n",
              "       [211.6    ],\n",
              "       [ 52.2    ],\n",
              "       [ 91.2    ],\n",
              "       [ 37.6    ],\n",
              "       [121.6    ],\n",
              "       [114.4    ],\n",
              "       [114.6    ],\n",
              "       [118.2    ],\n",
              "       [ 78.2    ],\n",
              "       [102.8    ],\n",
              "       [188.4    ],\n",
              "       [137.8    ],\n",
              "       [134.     ],\n",
              "       [165.4    ],\n",
              "       [ 95.2    ],\n",
              "       [ 68.2    ],\n",
              "       [157.6    ],\n",
              "       [157.6    ],\n",
              "       [ 74.     ],\n",
              "       [ 79.2    ],\n",
              "       [147.4    ],\n",
              "       [119.     ],\n",
              "       [152.8    ],\n",
              "       [127.04   ],\n",
              "       [ 98.     ],\n",
              "       [ 44.96   ],\n",
              "       [ 63.18   ],\n",
              "       [106.62   ],\n",
              "       [ 85.58   ],\n",
              "       [110.08   ],\n",
              "       [ 86.56   ],\n",
              "       [ 91.54   ],\n",
              "       [122.28   ],\n",
              "       [ 47.4    ],\n",
              "       [117.32   ],\n",
              "       [102.6    ],\n",
              "       [110.72   ],\n",
              "       [ 65.82   ],\n",
              "       [ 16.84   ],\n",
              "       [105.     ],\n",
              "       [114.96   ],\n",
              "       [142.1    ],\n",
              "       [ 73.     ],\n",
              "       [ 85.     ],\n",
              "       [138.1    ],\n",
              "       [142.3    ],\n",
              "       [121.48   ],\n",
              "       [145.8    ],\n",
              "       [ 74.38   ],\n",
              "       [183.2    ],\n",
              "       [112.4    ],\n",
              "       [ 94.62   ],\n",
              "       [ 49.02   ],\n",
              "       [106.74   ],\n",
              "       [145.24   ],\n",
              "       [123.8    ],\n",
              "       [139.05   ],\n",
              "       [ 68.     ],\n",
              "       [108.5    ],\n",
              "       [108.76   ],\n",
              "       [106.2    ],\n",
              "       [ 92.     ],\n",
              "       [ 98.     ],\n",
              "       [155.14   ],\n",
              "       [111.     ],\n",
              "       [ 61.52   ],\n",
              "       [139.     ],\n",
              "       [164.     ],\n",
              "       [116.64   ],\n",
              "       [ 92.32   ],\n",
              "       [114.     ],\n",
              "       [125.96   ],\n",
              "       [ 39.38   ],\n",
              "       [138.54   ],\n",
              "       [160.68   ],\n",
              "       [186.94   ],\n",
              "       [107.52   ],\n",
              "       [118.98   ],\n",
              "       [ 91.6    ],\n",
              "       [141.3    ],\n",
              "       [114.72   ],\n",
              "       [103.38   ],\n",
              "       [ 41.4    ],\n",
              "       [122.76   ],\n",
              "       [121.76   ],\n",
              "       [ 61.92   ],\n",
              "       [113.08   ],\n",
              "       [ 89.08   ],\n",
              "       [ 98.52   ],\n",
              "       [113.2    ],\n",
              "       [187.06   ],\n",
              "       [157.58   ],\n",
              "       [116.     ],\n",
              "       [107.     ],\n",
              "       [ 70.     ],\n",
              "       [ 64.     ],\n",
              "       [153.     ],\n",
              "       [121.8    ],\n",
              "       [160.56   ],\n",
              "       [ 93.55   ],\n",
              "       [ 80.     ],\n",
              "       [ 44.68   ],\n",
              "       [123.1    ],\n",
              "       [130.     ],\n",
              "       [152.     ],\n",
              "       [ 74.     ],\n",
              "       [ 92.3    ],\n",
              "       [ 94.     ],\n",
              "       [ 32.6    ],\n",
              "       [ 70.     ],\n",
              "       [152.     ],\n",
              "       [195.     ],\n",
              "       [ 51.8    ],\n",
              "       [ 33.     ],\n",
              "       [  5.3    ],\n",
              "       [  5.3    ],\n",
              "       [ 56.4    ],\n",
              "       [ 31.9    ],\n",
              "       [130.2    ],\n",
              "       [ 77.4    ],\n",
              "       [128.4    ],\n",
              "       [131.4    ],\n",
              "       [155.8    ],\n",
              "       [217.6    ],\n",
              "       [119.2    ],\n",
              "       [135.     ],\n",
              "       [115.2    ],\n",
              "       [106.2    ],\n",
              "       [136.6    ],\n",
              "       [129.     ],\n",
              "       [120.4    ],\n",
              "       [126.     ],\n",
              "       [ 45.2    ],\n",
              "       [ 96.6    ],\n",
              "       [165.6    ],\n",
              "       [211.4    ],\n",
              "       [112.2    ],\n",
              "       [ 70.6    ],\n",
              "       [126.4    ],\n",
              "       [ 67.8    ],\n",
              "       [ 61.8    ],\n",
              "       [103.6    ],\n",
              "       [149.     ],\n",
              "       [146.8    ],\n",
              "       [110.2    ],\n",
              "       [147.     ],\n",
              "       [123.8    ],\n",
              "       [ 97.2    ],\n",
              "       [130.2    ],\n",
              "       [116.2    ],\n",
              "       [ 73.4    ],\n",
              "       [ 52.6    ],\n",
              "       [164.6    ],\n",
              "       [ 94.4    ],\n",
              "       [110.4    ],\n",
              "       [ 72.     ],\n",
              "       [107.6    ],\n",
              "       [ 75.2    ],\n",
              "       [185.2    ],\n",
              "       [ 48.6    ],\n",
              "       [121.4    ],\n",
              "       [200.4    ],\n",
              "       [109.6    ],\n",
              "       [ 76.8    ],\n",
              "       [117.8    ],\n",
              "       [108.6    ],\n",
              "       [ 39.2    ],\n",
              "       [ 99.6    ],\n",
              "       [ 71.4    ],\n",
              "       [ 87.2    ],\n",
              "       [ 69.6    ],\n",
              "       [146.     ],\n",
              "       [ 72.2    ],\n",
              "       [200.4    ],\n",
              "       [ 73.4635 ],\n",
              "       [109.8    ],\n",
              "       [107.8    ],\n",
              "       [ 88.84   ],\n",
              "       [104.     ],\n",
              "       [146.8    ],\n",
              "       [ 47.6    ],\n",
              "       [101.2    ],\n",
              "       [ 46.4    ],\n",
              "       [ 79.6    ],\n",
              "       [ 95.4    ],\n",
              "       [121.     ],\n",
              "       [113.2    ],\n",
              "       [ 64.4    ],\n",
              "       [109.9    ],\n",
              "       [154.     ],\n",
              "       [183.38   ],\n",
              "       [ 93.42   ],\n",
              "       [ 57.76   ],\n",
              "       [ 63.76   ],\n",
              "       [135.     ],\n",
              "       [128.7    ],\n",
              "       [ 86.8    ],\n",
              "       [181.92   ],\n",
              "       [146.96   ],\n",
              "       [137.4    ],\n",
              "       [ 95.     ],\n",
              "       [ 87.     ],\n",
              "       [119.2    ],\n",
              "       [ 74.     ],\n",
              "       [125.89   ],\n",
              "       [ 91.     ],\n",
              "       [136.12   ],\n",
              "       [114.     ],\n",
              "       [ 75.46   ],\n",
              "       [ 46.22   ],\n",
              "       [ 74.     ],\n",
              "       [ 85.     ],\n",
              "       [ 89.     ],\n",
              "       [ 96.     ],\n",
              "       [109.     ],\n",
              "       [109.     ],\n",
              "       [ 70.     ],\n",
              "       [120.     ],\n",
              "       [ 68.     ],\n",
              "       [ 75.     ],\n",
              "       [ 97.     ],\n",
              "       [127.     ],\n",
              "       [ 65.     ],\n",
              "       [130.2    ],\n",
              "       [ 30.88   ],\n",
              "       [ 28.05   ],\n",
              "       [102.     ],\n",
              "       [128.     ],\n",
              "       [160.36   ],\n",
              "       [ 95.02   ],\n",
              "       [131.8    ],\n",
              "       [117.95   ],\n",
              "       [115.     ],\n",
              "       [ 23.     ],\n",
              "       [147.     ],\n",
              "       [ 95.     ],\n",
              "       [ 51.     ],\n",
              "       [ 77.     ],\n",
              "       [ 64.     ],\n",
              "       [ 51.46   ],\n",
              "       [ 55.8    ],\n",
              "       [ 90.02   ],\n",
              "       [ 74.     ],\n",
              "       [102.46   ],\n",
              "       [112.     ],\n",
              "       [ 72.     ],\n",
              "       [125.26   ],\n",
              "       [ 97.9    ],\n",
              "       [ 58.28   ],\n",
              "       [150.     ],\n",
              "       [ 87.     ],\n",
              "       [103.     ],\n",
              "       [127.9    ],\n",
              "       [167.     ],\n",
              "       [103.     ],\n",
              "       [108.     ],\n",
              "       [ 39.     ],\n",
              "       [ 81.25   ],\n",
              "       [ 78.     ],\n",
              "       [114.     ],\n",
              "       [ 83.6    ],\n",
              "       [113.     ],\n",
              "       [135.     ],\n",
              "       [133.2    ],\n",
              "       [135.72   ],\n",
              "       [122.14   ],\n",
              "       [125.3    ],\n",
              "       [ 83.7    ],\n",
              "       [ 33.     ],\n",
              "       [ 92.     ],\n",
              "       [125.26   ],\n",
              "       [ 87.7    ],\n",
              "       [ 81.64   ],\n",
              "       [162.     ],\n",
              "       [ 92.     ],\n",
              "       [120.     ],\n",
              "       [103.     ],\n",
              "       [114.44   ],\n",
              "       [ 92.42   ],\n",
              "       [181.4    ],\n",
              "       [110.     ],\n",
              "       [125.58   ],\n",
              "       [ 61.     ],\n",
              "       [ 75.18   ],\n",
              "       [119.58   ],\n",
              "       [148.4    ],\n",
              "       [ 75.2    ],\n",
              "       [ 96.3    ],\n",
              "       [102.     ],\n",
              "       [ 62.     ],\n",
              "       [ 94.62   ],\n",
              "       [ 88.8    ],\n",
              "       [ 78.52   ],\n",
              "       [116.     ],\n",
              "       [152.     ],\n",
              "       [107.     ],\n",
              "       [ 83.1    ],\n",
              "       [126.     ],\n",
              "       [ 31.     ],\n",
              "       [ 90.     ],\n",
              "       [ 94.     ],\n",
              "       [152.     ],\n",
              "       [ 39.     ],\n",
              "       [ 83.     ],\n",
              "       [ 86.     ],\n",
              "       [114.31   ],\n",
              "       [ 98.5    ],\n",
              "       [ 75.2    ],\n",
              "       [ 95.04   ],\n",
              "       [134.     ],\n",
              "       [120.     ],\n",
              "       [144.94   ],\n",
              "       [ 61.37   ],\n",
              "       [168.41   ],\n",
              "       [ 72.24   ],\n",
              "       [ 75.     ],\n",
              "       [ 66.9    ],\n",
              "       [ 77.     ],\n",
              "       [137.5    ],\n",
              "       [106.12   ],\n",
              "       [150.     ],\n",
              "       [143.     ],\n",
              "       [175.     ],\n",
              "       [115.     ],\n",
              "       [100.44   ],\n",
              "       [ 29.8    ],\n",
              "       [183.     ],\n",
              "       [105.42   ],\n",
              "       [193.82   ],\n",
              "       [260.94   ],\n",
              "       [110.86   ],\n",
              "       [103.76   ],\n",
              "       [190.98   ],\n",
              "       [168.     ],\n",
              "       [110.9    ],\n",
              "       [ 76.     ],\n",
              "       [138.     ],\n",
              "       [138.     ],\n",
              "       [ 68.     ],\n",
              "       [ 26.     ],\n",
              "       [ 65.     ],\n",
              "       [ 49.9    ],\n",
              "       [144.5    ],\n",
              "       [140.     ],\n",
              "       [127.1    ],\n",
              "       [ 69.9    ],\n",
              "       [114.     ],\n",
              "       [ 61.     ],\n",
              "       [ 70.26   ],\n",
              "       [103.46   ],\n",
              "       [243.20285],\n",
              "       [243.20285],\n",
              "       [115.60835],\n",
              "       [106.7154 ],\n",
              "       [108.     ],\n",
              "       [190.     ],\n",
              "       [140.7406 ],\n",
              "       [113.98   ],\n",
              "       [ 43.1    ],\n",
              "       [100.     ],\n",
              "       [ 99.     ],\n",
              "       [135.     ],\n",
              "       [182.9    ],\n",
              "       [ 86.72   ],\n",
              "       [189.84515],\n",
              "       [ 99.36905],\n",
              "       [ 47.1713 ],\n",
              "       [177.0857 ],\n",
              "       [ 57.2242 ],\n",
              "       [158.91315],\n",
              "       [138.80735],\n",
              "       [182.11215],\n",
              "       [187.9119 ],\n",
              "       [137.     ],\n",
              "       [ 75.     ],\n",
              "       [100.44   ],\n",
              "       [130.7    ],\n",
              "       [149.9    ],\n",
              "       [ 41.     ],\n",
              "       [117.5416 ],\n",
              "       [196.03155],\n",
              "       [ 91.63605],\n",
              "       [130.     ],\n",
              "       [ 87.     ],\n",
              "       [ 75.     ],\n",
              "       [104.3955 ],\n",
              "       [ 71.53025],\n",
              "       [142.67385],\n",
              "       [118.70155],\n",
              "       [163.92   ],\n",
              "       [ 93.99   ],\n",
              "       [184.61   ],\n",
              "       [174.67   ],\n",
              "       [189.97   ],\n",
              "       [113.     ],\n",
              "       [ 84.62   ],\n",
              "       [184.24   ],\n",
              "       [139.96   ],\n",
              "       [167.44   ],\n",
              "       [162.04   ],\n",
              "       [125.     ],\n",
              "       [131.46   ],\n",
              "       [ 50.     ],\n",
              "       [132.     ],\n",
              "       [103.82   ],\n",
              "       [103.82   ],\n",
              "       [139.76   ],\n",
              "       [153.83   ],\n",
              "       [135.     ],\n",
              "       [122.     ],\n",
              "       [125.86   ],\n",
              "       [138.54   ],\n",
              "       [ 64.84   ],\n",
              "       [104.02   ],\n",
              "       [155.     ],\n",
              "       [203.92   ],\n",
              "       [107.18   ],\n",
              "       [118.08   ],\n",
              "       [ 92.47   ],\n",
              "       [114.4484 ],\n",
              "       [242.01   ],\n",
              "       [110.49   ],\n",
              "       [202.15   ],\n",
              "       [125.08   ],\n",
              "       [138.68   ],\n",
              "       [103.89   ],\n",
              "       [ 58.     ],\n",
              "       [143.45   ],\n",
              "       [142.     ],\n",
              "       [ 91.2    ],\n",
              "       [ 80.4    ],\n",
              "       [128.8    ],\n",
              "       [ 89.57   ],\n",
              "       [110.     ],\n",
              "       [134.75   ],\n",
              "       [310.8    ],\n",
              "       [200.     ],\n",
              "       [240.     ],\n",
              "       [217.13   ],\n",
              "       [135.75   ],\n",
              "       [117.     ],\n",
              "       [115.63   ],\n",
              "       [123.     ],\n",
              "       [198.46   ],\n",
              "       [129.38   ],\n",
              "       [121.     ],\n",
              "       [217.2973 ],\n",
              "       [106.88   ],\n",
              "       [ 93.     ],\n",
              "       [121.2    ],\n",
              "       [ 86.76   ],\n",
              "       [128.98   ],\n",
              "       [ 32.8    ],\n",
              "       [ 73.2    ],\n",
              "       [137.9    ],\n",
              "       [178.     ],\n",
              "       [146.     ],\n",
              "       [111.     ],\n",
              "       [ 55.     ],\n",
              "       [132.     ],\n",
              "       [236.6    ],\n",
              "       [ 77.     ],\n",
              "       [121.     ],\n",
              "       [133.5    ],\n",
              "       [ 75.68   ],\n",
              "       [ 16.6    ],\n",
              "       [108.64865],\n",
              "       [104.2    ],\n",
              "       [ 80.4232 ],\n",
              "       [167.0328 ],\n",
              "       [165.87285],\n",
              "       [ 59.6    ],\n",
              "       [101.     ],\n",
              "       [ 86.     ],\n",
              "       [137.6    ],\n",
              "       [116.7683 ],\n",
              "       [ 62.7    ],\n",
              "       [125.94   ],\n",
              "       [ 85.4    ],\n",
              "       [169.9    ],\n",
              "       [ 85.     ],\n",
              "       [ 57.5    ],\n",
              "       [ 37.5    ],\n",
              "       [106.7154 ],\n",
              "       [ 87.3829 ],\n",
              "       [144.5    ],\n",
              "       [114.8    ],\n",
              "       [184.     ],\n",
              "       [ 43.     ],\n",
              "       [ 53.1    ],\n",
              "       [ 73.1    ],\n",
              "       [121.3    ],\n",
              "       [209.1    ],\n",
              "       [ 95.2    ],\n",
              "       [128.4    ],\n",
              "       [136.8741 ],\n",
              "       [ 75.5    ],\n",
              "       [147.7003 ],\n",
              "       [143.8338 ],\n",
              "       [135.71415],\n",
              "       [ 82.4    ],\n",
              "       [142.67385],\n",
              "       [107.6    ],\n",
              "       [291.4    ],\n",
              "       [ 90.72   ],\n",
              "       [114.     ],\n",
              "       [108.     ],\n",
              "       [179.8    ],\n",
              "       [ 59.     ],\n",
              "       [115.9    ],\n",
              "       [169.73935],\n",
              "       [117.7    ],\n",
              "       [159.68645],\n",
              "       [121.79475],\n",
              "       [ 96.5    ],\n",
              "       [161.38   ],\n",
              "       [ 48.2    ],\n",
              "       [ 57.3    ],\n",
              "       [154.4    ],\n",
              "       [135.6    ],\n",
              "       [ 21.8    ],\n",
              "       [ 88.54285],\n",
              "       [159.68645],\n",
              "       [ 99.2    ],\n",
              "       [164.7129 ],\n",
              "       [ 98.99   ]])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import Normalizer\n",
        "\n",
        "\n",
        "__SCALER__=\"minmax\"\n",
        "\n",
        "if __SCALER__==\"normal\":\n",
        "    X_normalizer = Normalizer() \n",
        "    X=X_normalizer.fit_transform(X_)\n",
        "    y_normalizer = Normalizer() \n",
        "    y=y_normalizer.fit_transform(y_)\n",
        "\n",
        "\n",
        "elif __SCALER__==\"minmax\":\n",
        "    X_minmax = MinMaxScaler() \n",
        "    X=X_minmax.fit_transform(X_)\n",
        "    y_minmax = MinMaxScaler() \n",
        "    \n",
        "    y=y_minmax.fit_transform(y_)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ__bM5BpIaE"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-ALi-6SrjHS",
        "outputId": "8f49ac41-593b-4056-c65d-417d8310eb82"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.45442072, 0.45946018, 0.46352764, ..., 0.82316512, 0.78118723,\n",
              "        0.83334109],\n",
              "       [0.46412324, 0.46056607, 0.46502857, ..., 0.60919169, 0.58001034,\n",
              "        0.57221906],\n",
              "       [0.2704717 , 0.27493718, 0.28373915, ..., 0.54688144, 0.51864758,\n",
              "        0.55100822],\n",
              "       ...,\n",
              "       [0.25883836, 0.26735063, 0.26338547, ..., 0.30637068, 0.31488477,\n",
              "        0.33420908],\n",
              "       [0.55751512, 0.55132694, 0.55922789, ..., 0.49308185, 0.50032984,\n",
              "        0.51049083],\n",
              "       [0.20738232, 0.20622139, 0.20216907, ..., 0.31621416, 0.3301991 ,\n",
              "        0.34781044]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(config={\"hyper\": \"parameter\"})\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "id": "39NoNiEM-dHG",
        "outputId": "7cf2438b-f54f-48be-df14-46c57fc3a21d"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:3dj2wlw9) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:3dj2wlw9). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/sipoczlaszlo/uncategorized/runs/2t5txjd7\" target=\"_blank\">fragrant-flower-17</a></strong> to <a href=\"https://wandb.ai/sipoczlaszlo/uncategorized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f500fbc5650>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sipoczlaszlo/uncategorized/runs/2t5txjd7?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 109
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blood_model_2():\n",
        "    #clear_session()\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    input_len=170\n",
        "\n",
        "    print(input_len)\n",
        "    output_size=24\n",
        "    drop_frac0=0.0 \n",
        "    drop_frac1=0.0\n",
        "\n",
        "\n",
        "\n",
        "    input1=Input(shape=(input_len,))\n",
        "\n",
        "    #flatt=Flatten()(lstm1)\n",
        "\n",
        "    non=42\n",
        "    #initializer = tf.keras.initializers.LecunNormal()\n",
        "    #initializer=tf.keras.initializers.LecunUniform()\n",
        "    #initializer=tf.keras.initializers.HeUniform(    seed=None)\n",
        "    #initializer= tf.keras.initializers.RandomNormal(    mean=3.0, stddev=0.05, seed=None)\n",
        "\n",
        "    initializer=\"glorot_uniform\"\n",
        "    d1=Dense(5000,activation=\"relu\",kernel_initializer=initializer)(input1)\n",
        "    d1=Dense(500,activation=\"relu\",kernel_initializer=initializer)(d1)\n",
        "    d1=Dense(50,activation=\"relu\",kernel_initializer=initializer)(d1)\n",
        "\n",
        "\n",
        "\n",
        "    pred=Dense(1,)(d1)\n",
        "\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=input1, outputs=pred)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
        "\n",
        "\n",
        "    #lossfn = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "\n",
        "    model.compile(loss=\"mse\",\n",
        "        optimizer=opt,\n",
        "        metrics=[None])\n",
        "    return(model)"
      ],
      "metadata": {
        "id": "2f2U85aQmiaZ"
      },
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name=\"_col\"\n",
        "def scheduler(epoch, lr):\n",
        "    return 0.00001\n",
        "\n",
        "callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks = [callback_LR,\n",
        "        \n",
        "        #savemodela,\n",
        "        ModelCheckpoint(filepath=model_name+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                        verbose=1, save_best_only=True, mode='min')]"
      ],
      "metadata": {
        "id": "DEVcX9Z5mwnz"
      },
      "execution_count": 116,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_2=blood_model_2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yakOYRb_m07P",
        "outputId": "1722e3b8-8a4c-499b-b680-95c4019df3b2"
      },
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHWVim-0vPxT",
        "outputId": "38f4d3c2-1444-42aa-86e8-a7be7441ec48"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 170)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5000)              855000    \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 500)               2500500   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 50)                25050     \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 51        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3,380,601\n",
            "Trainable params: 3,380,601\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-rSra7K3wVB",
        "outputId": "58a954f8-ce7e-465a-94e2-1af0486b220d"
      },
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.hdf5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ihqdnY6Vm2Yd"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_model_2.load_weights(\"./_col_0.00098_0.00173_.xhdf5\")"
      ],
      "metadata": {
        "id": "6lS9W3aAu2v8"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=new_model_2.fit(X,\n",
        "          y,\n",
        "          epochs=500, \n",
        "          batch_size=3,\n",
        "          validation_split=0.2,\n",
        "          verbose=1,\n",
        "          callbacks=[callbacks,WandbCallback()],\n",
        "          shuffle=True\n",
        "          \n",
        "          )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "eSVDad6L_Fgh",
        "outputId": "05f06382-24a6-4442-fbff-e9677c2df9e0"
      },
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0155\n",
            "Epoch 00001: val_loss improved from inf to 0.03046, saving model to _col_0.01554_0.03046_.hdf5\n",
            "145/145 [==============================] - 6s 42ms/step - loss: 0.0155 - val_loss: 0.0305 - lr: 1.0000e-05\n",
            "Epoch 2/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0152\n",
            "Epoch 00002: val_loss did not improve from 0.03046\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0152 - val_loss: 0.0307 - lr: 1.0000e-05\n",
            "Epoch 3/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0151\n",
            "Epoch 00003: val_loss improved from 0.03046 to 0.02985, saving model to _col_0.01509_0.02985_.hdf5\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0151 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 4/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0149\n",
            "Epoch 00004: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0150 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 5/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0149\n",
            "Epoch 00005: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0149 - val_loss: 0.0300 - lr: 1.0000e-05\n",
            "Epoch 6/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0149\n",
            "Epoch 00006: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0149 - val_loss: 0.0300 - lr: 1.0000e-05\n",
            "Epoch 7/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0149\n",
            "Epoch 00007: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0149 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 8/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0149\n",
            "Epoch 00008: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 38ms/step - loss: 0.0149 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 9/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00009: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 5s 37ms/step - loss: 0.0148 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 10/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00010: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 5s 37ms/step - loss: 0.0148 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 11/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00011: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0148 - val_loss: 0.0302 - lr: 1.0000e-05\n",
            "Epoch 12/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00012: val_loss did not improve from 0.02985\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0148 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 13/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00013: val_loss improved from 0.02985 to 0.02984, saving model to _col_0.01475_0.02984_.hdf5\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0148 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 14/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0148\n",
            "Epoch 00014: val_loss did not improve from 0.02984\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0300 - lr: 1.0000e-05\n",
            "Epoch 15/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00015: val_loss did not improve from 0.02984\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 16/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00016: val_loss improved from 0.02984 to 0.02947, saving model to _col_0.01472_0.02947_.hdf5\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 17/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00017: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0147 - val_loss: 0.0304 - lr: 1.0000e-05\n",
            "Epoch 18/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00018: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 19/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00019: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0147 - val_loss: 0.0302 - lr: 1.0000e-05\n",
            "Epoch 20/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00020: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 21/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00021: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 22/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00022: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 23/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00023: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0302 - lr: 1.0000e-05\n",
            "Epoch 24/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00024: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0147 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 25/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00025: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 26/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00026: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0147 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 27/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00027: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 28/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0147\n",
            "Epoch 00028: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 29/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00029: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0146 - val_loss: 0.0300 - lr: 1.0000e-05\n",
            "Epoch 30/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00030: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0300 - lr: 1.0000e-05\n",
            "Epoch 31/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00031: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 32/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00032: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0146 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 33/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00033: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0301 - lr: 1.0000e-05\n",
            "Epoch 34/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00034: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0145 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 35/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00035: val_loss did not improve from 0.02947\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0145 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 36/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00036: val_loss improved from 0.02947 to 0.02936, saving model to _col_0.01457_0.02936_.hdf5\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0294 - lr: 1.0000e-05\n",
            "Epoch 37/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00037: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0294 - lr: 1.0000e-05\n",
            "Epoch 38/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00038: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0144 - val_loss: 0.0303 - lr: 1.0000e-05\n",
            "Epoch 39/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0146\n",
            "Epoch 00039: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0146 - val_loss: 0.0296 - lr: 1.0000e-05\n",
            "Epoch 40/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00040: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0145 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 41/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00041: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0145 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 42/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00042: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0145 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 43/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00043: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0145 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 44/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00044: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0145 - val_loss: 0.0303 - lr: 1.0000e-05\n",
            "Epoch 45/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00045: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0145 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 46/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00046: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0145 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 47/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00047: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0144 - val_loss: 0.0302 - lr: 1.0000e-05\n",
            "Epoch 48/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00048: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0144 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 49/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00049: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0145 - val_loss: 0.0303 - lr: 1.0000e-05\n",
            "Epoch 50/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00050: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0144 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 51/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0145\n",
            "Epoch 00051: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0145 - val_loss: 0.0296 - lr: 1.0000e-05\n",
            "Epoch 52/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00052: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0144 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 53/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00053: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0144 - val_loss: 0.0302 - lr: 1.0000e-05\n",
            "Epoch 54/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00054: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0144 - val_loss: 0.0294 - lr: 1.0000e-05\n",
            "Epoch 55/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00055: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0144 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 56/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00056: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0144 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 57/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00057: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0144 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 58/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00058: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0143 - val_loss: 0.0299 - lr: 1.0000e-05\n",
            "Epoch 59/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00059: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0144 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 60/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00060: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 61/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00061: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0143 - val_loss: 0.0306 - lr: 1.0000e-05\n",
            "Epoch 62/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00062: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0144 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 63/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00063: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0143 - val_loss: 0.0296 - lr: 1.0000e-05\n",
            "Epoch 64/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00064: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0144 - val_loss: 0.0300 - lr: 1.0000e-05\n",
            "Epoch 65/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0144\n",
            "Epoch 00065: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0144 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 66/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00066: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 67/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00067: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0143 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 68/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00068: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0302 - lr: 1.0000e-05\n",
            "Epoch 69/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00069: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0294 - lr: 1.0000e-05\n",
            "Epoch 70/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00070: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 71/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00071: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 72/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00072: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 39ms/step - loss: 0.0143 - val_loss: 0.0295 - lr: 1.0000e-05\n",
            "Epoch 73/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00073: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 41ms/step - loss: 0.0144 - val_loss: 0.0296 - lr: 1.0000e-05\n",
            "Epoch 74/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00074: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0142 - val_loss: 0.0298 - lr: 1.0000e-05\n",
            "Epoch 75/500\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00075: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0297 - lr: 1.0000e-05\n",
            "Epoch 76/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0142\n",
            "Epoch 00076: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0142 - val_loss: 0.0303 - lr: 1.0000e-05\n",
            "Epoch 77/500\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0143\n",
            "Epoch 00077: val_loss did not improve from 0.02936\n",
            "145/145 [==============================] - 6s 40ms/step - loss: 0.0143 - val_loss: 0.0294 - lr: 1.0000e-05\n",
            "Epoch 78/500\n",
            " 79/145 [===============>..............] - ETA: 2s - loss: 0.0150"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-117-3799b8d6f71b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m           \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m           \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mWandbCallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m           )\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/wandb/integration/keras/keras.py\u001b[0m in \u001b[0;36mnew_v2\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mcbk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcbks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0mset_wandb_attrs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcbk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mold_v2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0mtraining_arrays\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0morig_fit_loop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_arrays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1214\u001b[0m                 _r=1):\n\u001b[1;32m   1215\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1216\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1217\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 3131\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3133\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1960\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 603\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    604\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 59\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BKodgIskMiCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Kls-KXivMt5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=new_model_2.predict(X)\n",
        "result_df=pd.DataFrame()\n",
        "y_pred_col=[i[0] for i in y_pred]\n",
        "y_col=[i[0] for i in y]\n",
        "\n",
        "result_df[\"y\"]=y_col\n",
        "result_df[\"y_pred\"]=y_pred_col"
      ],
      "metadata": {
        "id": "4YcPfNhoDkVz"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(result_df, x=\"y\", y=\"y_pred\",width=600, height=400 )\n",
        "fig.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 417
        },
        "id": "8JvY2HWxIQQA",
        "outputId": "3e633b98-4ed4-4862-9446-bc305838c03b"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>\n",
              "            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax) {MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>\n",
              "                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script src=\"https://cdn.plot.ly/plotly-latest.min.js\"></script>    \n",
              "            <div id=\"65003c36-2a1b-4b9e-9b20-a961232560b9\" class=\"plotly-graph-div\" style=\"height:400px; width:600px;\"></div>\n",
              "            <script type=\"text/javascript\">\n",
              "                \n",
              "                    window.PLOTLYENV=window.PLOTLYENV || {};\n",
              "                    \n",
              "                if (document.getElementById(\"65003c36-2a1b-4b9e-9b20-a961232560b9\")) {\n",
              "                    Plotly.newPlot(\n",
              "                        '65003c36-2a1b-4b9e-9b20-a961232560b9',\n",
              "                        [{\"hoverlabel\": {\"namelength\": 0}, \"hovertemplate\": \"y=%{x}<br>y_pred=%{y}\", \"legendgroup\": \"\", \"marker\": {\"color\": \"#636efa\", \"symbol\": \"circle\"}, \"mode\": \"markers\", \"name\": \"\", \"showlegend\": false, \"type\": \"scatter\", \"x\": [0.32831423895253753, 0.21963993453355193, 0.20654664484451762, 0.32962356792144065, 0.08477905073649775, 0.11554828150572828, 0.2530278232405896, 0.08968903436988562, 0.19279869067103111, 0.4769230769230775, 0.29296235679214483, 0.23993453355155503, 0.3453355155482824, 0.46775777414075315, 0.2530278232405896, 0.4088379705400983, 0.6752864157119494, 0.15351882160392805, 0.2811783960720131, 0.10572831423895251, 0.3806873977086752, 0.35711947626841245, 0.35777414075286507, 0.36955810147299506, 0.23862520458265135, 0.3191489361702136, 0.599345335515548, 0.43371522094926446, 0.42127659574468135, 0.5240589198036006, 0.29427168576104734, 0.20589198036006545, 0.49852700490998486, 0.49852700490998486, 0.22487725040916556, 0.24189852700490994, 0.465139116202946, 0.3721767594108024, 0.48281505728314306, 0.39849427168576146, 0.30343698854337187, 0.129819967266776, 0.18945990180032735, 0.33165302782324074, 0.26278232405892, 0.34297872340425556, 0.26599018003273367, 0.2822913256955813, 0.3829132569558104, 0.13780687397708713, 0.36667757774140775, 0.3184942716857617, 0.3450736497545014, 0.19810147299509043, 0.03777414075286423, 0.32635024549918207, 0.35895253682487777, 0.44779050736497655, 0.22160392798690698, 0.26088379705401016, 0.43469721767594216, 0.44844517184942806, 0.38029459901800305, 0.45990180032733313, 0.2261211129296241, 0.5823240589198045, 0.3505728314238952, 0.29237315875613756, 0.1431096563011458, 0.33204582651391157, 0.4580687397708675, 0.3878887070376441, 0.4378068739770876, 0.205237315875614, 0.33780687397708714, 0.33865793780687486, 0.33027823240589194, 0.2837970540098203, 0.30343698854337187, 0.490474631751228, 0.34599018003273363, 0.18402618657937825, 0.4376432078559743, 0.5194762684124392, 0.36445171849427255, 0.28484451718494297, 0.35581014729950944, 0.3949590834697223, 0.11155482815057315, 0.4361374795417354, 0.5086088379705411, 0.5945662847790514, 0.33459901800327424, 0.3721112929623566, 0.282487725040917, 0.4451718494271694, 0.3581669394435358, 0.3210474631751234, 0.11816693944353547, 0.3844844517184952, 0.38121112929623663, 0.18533551554828187, 0.3527986906710313, 0.27423895253682506, 0.3051391162029468, 0.3531914893617021, 0.5949590834697225, 0.4984615384615389, 0.3623567921440266, 0.33289689034369924, 0.2117839607201312, 0.1921440261865796, 0.4834697217675947, 0.38134206219312694, 0.5082160392798699, 0.2888707037643216, 0.24451718494271718, 0.12890343698854342, 0.3855973813420631, 0.40818330605564696, 0.4801963993453361, 0.22487725040916556, 0.28477905073649823, 0.29034369885433753, 0.08936170212765955, 0.2117839607201312, 0.4801963993453361, 0.6209492635024557, 0.15220949263502487, 0.0906710310965631, 0.0, 0.0, 0.1672667757774145, 0.08707037643207875, 0.4088379705400983, 0.23600654664484488, 0.4029459901800328, 0.41276595744680855, 0.49263502454991887, 0.6949263502455009, 0.3728314238952537, 0.42454991816694, 0.3597381342062193, 0.33027823240589194, 0.42978723404255426, 0.40490998363338837, 0.376759410801964, 0.3950900163666126, 0.1306055646481179, 0.2988543371522101, 0.5247135842880536, 0.6746317512274957, 0.3499181669394435, 0.2137479541734862, 0.3963993453355156, 0.20458265139116255, 0.1849427168576109, 0.3217675941080203, 0.47037643207856034, 0.4631751227495917, 0.34337152209492633, 0.4638297872340431, 0.3878887070376441, 0.30081833060556457, 0.4088379705400983, 0.3630114566284779, 0.22291325695581052, 0.1548281505728314, 0.521440261865795, 0.2916530278232407, 0.34402618657937817, 0.2183306055646484, 0.3348608837970547, 0.22880523731587563, 0.5888707037643217, 0.141734860883797, 0.3800327332242226, 0.6386252045826513, 0.3414075286415719, 0.23404255319148998, 0.36824877250409255, 0.33813420621931334, 0.11096563011456644, 0.3086743044189859, 0.21636661211129338, 0.2680851063829787, 0.2104746317512276, 0.4605564648117845, 0.21898527004909982, 0.6386252045826513, 0.22312111292962364, 0.34206219312602376, 0.3355155482815066, 0.27345335515548336, 0.3230769230769235, 0.4631751227495917, 0.13846153846153839, 0.31391162029459896, 0.13453355155482846, 0.24320785597381367, 0.2949263502454993, 0.3787234042553196, 0.3531914893617021, 0.19345335515548326, 0.3423895253682489, 0.4867430441898533, 0.5829132569558103, 0.2884451718494277, 0.17171849427168628, 0.1913584288052379, 0.42454991816694, 0.4039279869067104, 0.26677577741407593, 0.5781342062193134, 0.46369885433715263, 0.43240589198036006, 0.2936170212765961, 0.2674304418985273, 0.3728314238952537, 0.22487725040916556, 0.39472995090016455, 0.2805237315875617, 0.42821603927986934, 0.35581014729950944, 0.2296563011456632, 0.13394435351882164, 0.22487725040916556, 0.26088379705401016, 0.2739770867430445, 0.2968903436988547, 0.33944353518821646, 0.33944353518821646, 0.2117839607201312, 0.375450081833061, 0.205237315875614, 0.22815057283142418, 0.3001636661211133, 0.3983633387888712, 0.1954173486088382, 0.4088379705400983, 0.08373158756137505, 0.07446808510638293, 0.31653027823240626, 0.4016366612111298, 0.507561374795418, 0.29368248772504174, 0.41407528641571284, 0.3687397708674304, 0.359083469721768, 0.057937806873977135, 0.4638297872340431, 0.2936170212765961, 0.14959083469721784, 0.23469721767594137, 0.1921440261865796, 0.1510965630114569, 0.16530278232405926, 0.27731587561374865, 0.22487725040916556, 0.3180360065466453, 0.3492635024549922, 0.2183306055646484, 0.3926677577741417, 0.3031096563011458, 0.17342062193126065, 0.4736497545008189, 0.2674304418985273, 0.3198036006546649, 0.4013093289689035, 0.529296235679215, 0.3198036006546649, 0.3361702127659579, 0.11031096563011468, 0.2486088379705404, 0.23797054009819996, 0.35581014729950944, 0.2563011456628482, 0.35253682487725085, 0.42454991816694, 0.4186579378068741, 0.42690671031096644, 0.38245499181669484, 0.392798690671032, 0.25662847790507365, 0.0906710310965631, 0.2837970540098203, 0.3926677577741417, 0.26972176759410804, 0.24988543371522137, 0.512929623567922, 0.2837970540098203, 0.375450081833061, 0.3198036006546649, 0.3572504091653028, 0.28517184942716906, 0.5764320785597381, 0.34271685761047505, 0.3937152209492638, 0.18232405891980383, 0.22873977086743064, 0.3740752864157122, 0.4684124386252046, 0.22880523731587563, 0.29787234042553273, 0.31653027823240626, 0.1855973813420624, 0.29237315875613756, 0.2733224222585931, 0.23967266775777463, 0.3623567921440266, 0.4801963993453361, 0.33289689034369924, 0.25466448445171885, 0.3950900163666126, 0.08412438625204591, 0.27725040916530314, 0.29034369885433753, 0.4801963993453361, 0.11031096563011468, 0.25433715220949293, 0.26415711947626874, 0.35682487725041, 0.3050736497545012, 0.22880523731587563, 0.29374795417348637, 0.42127659574468135, 0.375450081833061, 0.45708674304418995, 0.18353518821603926, 0.5339116202945994, 0.21911620294599005, 0.22815057283142418, 0.20163666121112975, 0.23469721767594137, 0.4327332242225865, 0.3300163666121114, 0.4736497545008189, 0.4507364975450087, 0.5554828150572838, 0.359083469721768, 0.3114238952536825, 0.08019639934533548, 0.5816693944353526, 0.3277250409165309, 0.6170867430441901, 0.8367921440261881, 0.3455319148936171, 0.3222913256955819, 0.6077905073649752, 0.5325695581014736, 0.3456628477905073, 0.23142389525368276, 0.43436988543371574, 0.43436988543371574, 0.205237315875614, 0.06775777414075293, 0.1954173486088382, 0.14599018003273362, 0.4556464811783966, 0.44091653027823297, 0.3986906710310975, 0.21145662847790547, 0.35581014729950944, 0.18232405891980383, 0.2126350245499188, 0.3213093289689039, 0.7787327332242227, 0.7787327332242227, 0.36107479541734955, 0.3319653027823246, 0.3361702127659579, 0.6045826513911627, 0.44334075286415786, 0.35574468085106364, 0.12373158756137477, 0.3099836333878891, 0.3067103109656305, 0.42454991816694, 0.581342062193126, 0.26651391162029486, 0.6040757774140755, 0.307918330605565, 0.13705826513911631, 0.5623099836333879, 0.16996464811784007, 0.5028253682487736, 0.43701260229132627, 0.5787631751227508, 0.5977476268412437, 0.43109656301145716, 0.22815057283142418, 0.3114238952536825, 0.4104746317512276, 0.4733224222585925, 0.11685761047463188, 0.36740294599017986, 0.6243258592471359, 0.2826057283142394, 0.40818330605564696, 0.2674304418985273, 0.22815057283142418, 0.32437152209492653, 0.2167929623567921, 0.44966890343698923, 0.3711998363338788, 0.5192144026186587, 0.29031096563011444, 0.5869394435351891, 0.5544026186579386, 0.6044844517184946, 0.35253682487725085, 0.2596399345335516, 0.5857283142389527, 0.44078559738134265, 0.5307364975450086, 0.5130605564648127, 0.391816693944354, 0.41296235679214455, 0.14631751227495926, 0.4147299509001642, 0.3224877250409167, 0.3224877250409167, 0.4401309328968914, 0.48618657937806914, 0.42454991816694, 0.3819967266775782, 0.39463175122749594, 0.4361374795417354, 0.19489361702127703, 0.32314238952536917, 0.49001636661211184, 0.6501472995090025, 0.333486088379706, 0.3691653027823243, 0.28533551554828185, 0.3572779050736507, 0.7748281505728329, 0.3443207855973813, 0.6443535188216037, 0.3920785597381345, 0.436595744680852, 0.32271685761047525, 0.17250409165302802, 0.4522094926350248, 0.44746317512275013, 0.2811783960720131, 0.24582651391162064, 0.4042553191489371, 0.27584288052373185, 0.34271685761047505, 0.4237315875613753, 1.0, 0.6373158756137487, 0.7682487725040925, 0.6933878887070376, 0.4270049099836339, 0.36563011456628525, 0.36114566284779126, 0.3852700490998368, 0.6322749590834694, 0.4061538461538469, 0.3787234042553196, 0.6939355155482827, 0.33250409165302847, 0.2870703764320789, 0.37937806873977087, 0.2666448445171857, 0.4048445171849425, 0.09001636661211128, 0.22225859247135843, 0.4340425531914894, 0.5653027823240595, 0.4605564648117845, 0.34599018003273363, 0.16268412438625224, 0.4147299509001642, 0.7571194762684142, 0.23469721767594137, 0.3787234042553196, 0.4196399345335521, 0.23037643207855996, 0.03698854337152217, 0.33829345335515615, 0.3237315875613747, 0.2459024549918173, 0.5294036006546655, 0.525606710310966, 0.17774140752864157, 0.3132569558101477, 0.26415711947626874, 0.43306055646481284, 0.3648716857610477, 0.1878887070376432, 0.39489361702127657, 0.2621931260229134, 0.5387888707037642, 0.26088379705401016, 0.17086743044189873, 0.10540098199672679, 0.3319653027823246, 0.26868379705401, 0.4556464811783966, 0.3584288052373168, 0.5849427168576111, 0.12340425531914907, 0.15646481178396068, 0.22193126022913273, 0.3797054009819976, 0.6671031096563028, 0.29427168576104734, 0.4029459901800328, 0.4306844517184948, 0.22978723404255347, 0.46612209492635165, 0.4534657937806884, 0.42688756137479544, 0.2523731587561377, 0.44966890343698923, 0.3348608837970547, 0.9364975450081837, 0.2796072013093293, 0.35581014729950944, 0.3361702127659579, 0.5711947626841248, 0.17577741407528663, 0.3620294599018003, 0.5382630114566288, 0.3679214402618658, 0.5053566284779055, 0.3813248772504096, 0.298527004909984, 0.5109001636661216, 0.1404255319148937, 0.17021276595744717, 0.4880523731587561, 0.42651391162029567, 0.05400981996726674, 0.2724806873977087, 0.5053566284779055, 0.30736497545008173, 0.5218098199672683, 0.3066775777414074], \"xaxis\": \"x\", \"y\": [0.36330798268318176, 0.36886292695999146, 0.31976819038391113, 0.31369999051094055, 0.33166956901550293, 0.3304313123226166, 0.30625012516975403, 0.31735125184059143, 0.3343220055103302, 0.34748801589012146, 0.30479151010513306, 0.3365718722343445, 0.3371447026729584, 0.3078937530517578, 0.27262648940086365, 0.3037596642971039, 0.4140445291996002, 0.32713863253593445, 0.318744957447052, 0.29561564326286316, 0.3126421868801117, 0.3617717921733856, 0.33710330724716187, 0.3593950569629669, 0.2928798198699951, 0.35236990451812744, 0.3491576313972473, 0.3221201002597809, 0.34529754519462585, 0.349203497171402, 0.3442172110080719, 0.32053643465042114, 0.3212590217590332, 0.3212590217590332, 0.34827807545661926, 0.3404209613800049, 0.3208107650279999, 0.2974441349506378, 0.3436281383037567, 0.3274841010570526, 0.32208141684532166, 0.3062017560005188, 0.29452550411224365, 0.3167898952960968, 0.346052885055542, 0.3376937806606293, 0.3194390833377838, 0.3251849114894867, 0.3550058603286743, 0.3146444261074066, 0.33362966775894165, 0.3449913263320923, 0.3205913305282593, 0.3044812083244324, 0.32139852643013, 0.32781216502189636, 0.33668121695518494, 0.35631269216537476, 0.34924235939979553, 0.34831154346466064, 0.32801228761672974, 0.364238440990448, 0.37801393866539, 0.3343743681907654, 0.32233110070228577, 0.3531608283519745, 0.32798025012016296, 0.34761175513267517, 0.34142792224884033, 0.33994847536087036, 0.341286838054657, 0.3158405125141144, 0.3238534927368164, 0.34490710496902466, 0.3752193748950958, 0.33137398958206177, 0.3437432050704956, 0.3356962502002716, 0.3011642098426819, 0.2615772783756256, 0.3218064308166504, 0.3362661898136139, 0.3358813226222992, 0.3662410378456116, 0.3241714537143707, 0.2976626455783844, 0.3775215148925781, 0.3370005488395691, 0.32762411236763, 0.3594629466533661, 0.336343914270401, 0.43044716119766235, 0.3330237865447998, 0.3377092480659485, 0.3313262462615967, 0.3305303156375885, 0.32181861996650696, 0.33919987082481384, 0.32844650745391846, 0.3381727933883667, 0.36278069019317627, 0.33016467094421387, 0.31370434165000916, 0.33328789472579956, 0.3306353986263275, 0.3233257234096527, 0.37760594487190247, 0.3680375814437866, 0.33228614926338196, 0.3396949768066406, 0.34207507967948914, 0.32129228115081787, 0.3209228217601776, 0.35405397415161133, 0.3531230092048645, 0.3334621787071228, 0.3283534646034241, 0.3364686369895935, 0.36938947439193726, 0.32399076223373413, 0.3416498899459839, 0.305581659078598, 0.3368273377418518, 0.32951411604881287, 0.32075849175453186, 0.2967650294303894, 0.36471521854400635, 0.3201039135456085, 0.31942883133888245, 0.3094742000102997, 0.330404132604599, 0.3473484516143799, 0.324158638715744, 0.3158935606479645, 0.3143371343612671, 0.3403266668319702, 0.3532324731349945, 0.37185022234916687, 0.375542551279068, 0.3581453561782837, 0.3529112637042999, 0.3309975564479828, 0.35572484135627747, 0.3258155882358551, 0.3374665677547455, 0.32437750697135925, 0.3413485288619995, 0.395306795835495, 0.3203639090061188, 0.3310338258743286, 0.3324744999408722, 0.36402758955955505, 0.3150724172592163, 0.32973912358283997, 0.31370729207992554, 0.3046066462993622, 0.317192941904068, 0.3524283170700073, 0.3554997444152832, 0.34218770265579224, 0.3005880117416382, 0.3537401556968689, 0.3223530352115631, 0.3476432263851166, 0.35561656951904297, 0.43599799275398254, 0.31041696667671204, 0.3713113069534302, 0.36464789509773254, 0.31618255376815796, 0.33166950941085815, 0.3007304072380066, 0.3381262719631195, 0.3430540859699249, 0.3101727068424225, 0.3581383228302002, 0.3233878016471863, 0.3747559189796448, 0.3460039496421814, 0.34237203001976013, 0.34384191036224365, 0.3182297945022583, 0.2990521490573883, 0.3188791871070862, 0.3445894122123718, 0.32137027382850647, 0.3535535931587219, 0.31620657444000244, 0.33217740058898926, 0.34067821502685547, 0.367300808429718, 0.3451521098613739, 0.3286280035972595, 0.3254484534263611, 0.34314197301864624, 0.31305131316185, 0.3012131154537201, 0.3149762451648712, 0.30565717816352844, 0.3187578618526459, 0.31526944041252136, 0.3410511910915375, 0.3341473937034607, 0.31113627552986145, 0.32421186566352844, 0.3117852210998535, 0.3288505971431732, 0.3034090995788574, 0.31270211935043335, 0.32628902792930603, 0.35770323872566223, 0.35755088925361633, 0.319385826587677, 0.3548707067966461, 0.3256520628929138, 0.32726407051086426, 0.3414343595504761, 0.3288114666938782, 0.34971168637275696, 0.3518265187740326, 0.35735705494880676, 0.3785703480243683, 0.3599579334259033, 0.324566125869751, 0.35501593351364136, 0.36510682106018066, 0.3545588254928589, 0.3917973041534424, 0.325457364320755, 0.31032341718673706, 0.35246995091438293, 0.3282622694969177, 0.32423534989356995, 0.3311561644077301, 0.36650726199150085, 0.3139576017856598, 0.3364507853984833, 0.3313765823841095, 0.34610819816589355, 0.35438743233680725, 0.33504241704940796, 0.3222360908985138, 0.32556483149528503, 0.3501843214035034, 0.35101479291915894, 0.3243495523929596, 0.2960830628871918, 0.34342584013938904, 0.35140421986579895, 0.33433985710144043, 0.33754605054855347, 0.3418355882167816, 0.36396485567092896, 0.32710200548171997, 0.3193228840827942, 0.3431832790374756, 0.3565770089626312, 0.3441549241542816, 0.36236196756362915, 0.39036819338798523, 0.34882932901382446, 0.341997355222702, 0.3953588604927063, 0.3506740629673004, 0.3058610260486603, 0.33007511496543884, 0.3451133668422699, 0.35671266913414, 0.34270796179771423, 0.3437117636203766, 0.34607458114624023, 0.33827531337738037, 0.3225891888141632, 0.3207497298717499, 0.3299461305141449, 0.3267699182033539, 0.3514486849308014, 0.30490729212760925, 0.33885425329208374, 0.3533775508403778, 0.3702273964881897, 0.34827113151550293, 0.3182378113269806, 0.3284536302089691, 0.2816045880317688, 0.33089107275009155, 0.3404013216495514, 0.3247724175453186, 0.33181002736091614, 0.3481904864311218, 0.33423247933387756, 0.3228728175163269, 0.3382647931575775, 0.35525915026664734, 0.3010556995868683, 0.3556749224662781, 0.3344610333442688, 0.34627074003219604, 0.34464341402053833, 0.33730748295783997, 0.3470892012119293, 0.3836083710193634, 0.3181936740875244, 0.3408500850200653, 0.30748769640922546, 0.35919690132141113, 0.36209186911582947, 0.32610270380973816, 0.3478471636772156, 0.3210020661354065, 0.3517988920211792, 0.3113023638725281, 0.3173099756240845, 0.34400713443756104, 0.3508273661136627, 0.32044023275375366, 0.32833752036094666, 0.3406432867050171, 0.34671205282211304, 0.344834566116333, 0.3364958167076111, 0.32522860169410706, 0.31716763973236084, 0.34190064668655396, 0.3329325318336487, 0.33208081126213074, 0.3266938030719757, 0.3351087272167206, 0.35826605558395386, 0.3523974120616913, 0.3491699993610382, 0.3428976535797119, 0.30355799198150635, 0.36073166131973267, 0.32763469219207764, 0.35038772225379944, 0.32210463285446167, 0.34214383363723755, 0.3348027169704437, 0.31741583347320557, 0.3353142738342285, 0.3395325839519501, 0.3367258608341217, 0.3547377586364746, 0.355020135641098, 0.34747785329818726, 0.3503546714782715, 0.32311731576919556, 0.36336570978164673, 0.33320435881614685, 0.33898186683654785, 0.33047932386398315, 0.3976491689682007, 0.3480629324913025, 0.3377179801464081, 0.334525465965271, 0.32310792803764343, 0.3085096478462219, 0.3486717641353607, 0.32258111238479614, 0.353617399930954, 0.36324232816696167, 0.3584681749343872, 0.3238789141178131, 0.29234105348587036, 0.3346708118915558, 0.3845199942588806, 0.40915602445602417, 0.32934606075286865, 0.3291863799095154, 0.3658367693424225, 0.3604707419872284, 0.42834848165512085, 0.3705882430076599, 0.3183634877204895, 0.3741326928138733, 0.35976698994636536, 0.3619275391101837, 0.361177921295166, 0.3098766505718231, 0.43499627709388733, 0.39343205094337463, 0.3402635157108307, 0.34687715768814087, 0.3669055700302124, 0.374763160943985, 0.34578126668930054, 0.3678895831108093, 0.3623678386211395, 0.3275279104709625, 0.38388586044311523, 0.3477960228919983, 0.377739816904068, 0.3782392740249634, 0.3299996852874756, 0.34691330790519714, 0.36310216784477234, 0.3322473168373108, 0.35948964953422546, 0.3778125047683716, 0.38930144906044006, 0.3557293713092804, 0.3587619662284851, 0.33921006321907043, 0.3513917028903961, 0.39145973324775696, 0.32948049902915955, 0.3533455431461334, 0.33705222606658936, 0.34116417169570923, 0.36919641494750977, 0.3766160309314728, 0.3160393536090851, 0.33773791790008545, 0.34778836369514465, 0.3460761308670044, 0.37374186515808105, 0.3361082375049591, 0.35912948846817017, 0.37637993693351746, 0.3397267460823059, 0.3698837161064148, 0.34310296177864075, 0.3323700428009033, 0.3734852969646454, 0.3101761043071747, 0.350827693939209, 0.34575092792510986, 0.35634705424308777, 0.36292359232902527, 0.36061403155326843, 0.36400145292282104, 0.33735543489456177, 0.40247485041618347, 0.3252045512199402, 0.33800840377807617, 0.34368887543678284, 0.3216913342475891, 0.3283497095108032, 0.32473164796829224, 0.314024955034256, 0.31006160378456116, 0.3653082251548767, 0.3228136897087097, 0.3272612690925598, 0.38467714190483093, 0.3449617028236389, 0.3320459723472595, 0.38315242528915405, 0.31886401772499084, 0.38183772563934326, 0.3169703185558319, 0.34354087710380554, 0.3144940435886383, 0.3285768926143646, 0.3459334969520569, 0.3440893888473511, 0.3231772184371948, 0.345328688621521, 0.35824069380760193, 0.37761610746383667, 0.3331969976425171, 0.34697484970092773, 0.35365059971809387, 0.3353259563446045, 0.34014376997947693, 0.3244839906692505, 0.3015550971031189, 0.3487955927848816, 0.2967779338359833, 0.3665197193622589, 0.33211004734039307, 0.36460810899734497, 0.33944669365882874, 0.3905797004699707, 0.37357696890830994, 0.3354091942310333, 0.3831618130207062, 0.31557637453079224, 0.3458620011806488, 0.3445092439651489, 0.3053992986679077, 0.3651694059371948, 0.3153165578842163, 0.33232465386390686, 0.33645007014274597, 0.3693808317184448, 0.3078247904777527, 0.3661738932132721, 0.3442196846008301, 0.3623391091823578, 0.34398573637008667, 0.3235589861869812, 0.3182853162288666, 0.31813883781433105, 0.310687392950058, 0.30586281418800354, 0.32525551319122314, 0.32305312156677246, 0.3530440926551819, 0.3529904782772064, 0.32671162486076355, 0.32643017172813416, 0.3568877875804901, 0.3235953748226166, 0.32643359899520874, 0.30819112062454224, 0.32013747096061707, 0.32078099250793457, 0.32895344495773315, 0.3414267599582672, 0.3787890374660492, 0.3490823805332184, 0.3372870087623596, 0.33874082565307617, 0.33794495463371277, 0.31579458713531494, 0.3158074915409088, 0.3493311405181885, 0.31794726848602295, 0.32314127683639526, 0.37847304344177246, 0.36865487694740295, 0.34401047229766846, 0.32295718789100647, 0.3370138704776764, 0.352282851934433, 0.3313758969306946, 0.335695743560791, 0.3609458804130554, 0.34040746092796326, 0.34173980355262756, 0.34117424488067627, 0.3442539870738983, 0.3364212214946747, 0.319294810295105, 0.31054800748825073, 0.37963423132896423, 0.3805798590183258, 0.3501107096672058, 0.35284891724586487, 0.3190821409225464], \"yaxis\": \"y\"}],\n",
              "                        {\"height\": 400, \"legend\": {\"tracegroupgap\": 0}, \"margin\": {\"t\": 60}, \"template\": {\"data\": {\"bar\": [{\"error_x\": {\"color\": \"#2a3f5f\"}, \"error_y\": {\"color\": \"#2a3f5f\"}, \"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"bar\"}], \"barpolar\": [{\"marker\": {\"line\": {\"color\": \"#E5ECF6\", \"width\": 0.5}}, \"type\": \"barpolar\"}], \"carpet\": [{\"aaxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"baxis\": {\"endlinecolor\": \"#2a3f5f\", \"gridcolor\": \"white\", \"linecolor\": \"white\", \"minorgridcolor\": \"white\", \"startlinecolor\": \"#2a3f5f\"}, \"type\": \"carpet\"}], \"choropleth\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"choropleth\"}], \"contour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"contour\"}], \"contourcarpet\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"contourcarpet\"}], \"heatmap\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmap\"}], \"heatmapgl\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"heatmapgl\"}], \"histogram\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"histogram\"}], \"histogram2d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2d\"}], \"histogram2dcontour\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"histogram2dcontour\"}], \"mesh3d\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"type\": \"mesh3d\"}], \"parcoords\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"parcoords\"}], \"pie\": [{\"automargin\": true, \"type\": \"pie\"}], \"scatter\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter\"}], \"scatter3d\": [{\"line\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatter3d\"}], \"scattercarpet\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattercarpet\"}], \"scattergeo\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergeo\"}], \"scattergl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattergl\"}], \"scattermapbox\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scattermapbox\"}], \"scatterpolar\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolar\"}], \"scatterpolargl\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterpolargl\"}], \"scatterternary\": [{\"marker\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"type\": \"scatterternary\"}], \"surface\": [{\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}, \"colorscale\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"type\": \"surface\"}], \"table\": [{\"cells\": {\"fill\": {\"color\": \"#EBF0F8\"}, \"line\": {\"color\": \"white\"}}, \"header\": {\"fill\": {\"color\": \"#C8D4E3\"}, \"line\": {\"color\": \"white\"}}, \"type\": \"table\"}]}, \"layout\": {\"annotationdefaults\": {\"arrowcolor\": \"#2a3f5f\", \"arrowhead\": 0, \"arrowwidth\": 1}, \"coloraxis\": {\"colorbar\": {\"outlinewidth\": 0, \"ticks\": \"\"}}, \"colorscale\": {\"diverging\": [[0, \"#8e0152\"], [0.1, \"#c51b7d\"], [0.2, \"#de77ae\"], [0.3, \"#f1b6da\"], [0.4, \"#fde0ef\"], [0.5, \"#f7f7f7\"], [0.6, \"#e6f5d0\"], [0.7, \"#b8e186\"], [0.8, \"#7fbc41\"], [0.9, \"#4d9221\"], [1, \"#276419\"]], \"sequential\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]], \"sequentialminus\": [[0.0, \"#0d0887\"], [0.1111111111111111, \"#46039f\"], [0.2222222222222222, \"#7201a8\"], [0.3333333333333333, \"#9c179e\"], [0.4444444444444444, \"#bd3786\"], [0.5555555555555556, \"#d8576b\"], [0.6666666666666666, \"#ed7953\"], [0.7777777777777778, \"#fb9f3a\"], [0.8888888888888888, \"#fdca26\"], [1.0, \"#f0f921\"]]}, \"colorway\": [\"#636efa\", \"#EF553B\", \"#00cc96\", \"#ab63fa\", \"#FFA15A\", \"#19d3f3\", \"#FF6692\", \"#B6E880\", \"#FF97FF\", \"#FECB52\"], \"font\": {\"color\": \"#2a3f5f\"}, \"geo\": {\"bgcolor\": \"white\", \"lakecolor\": \"white\", \"landcolor\": \"#E5ECF6\", \"showlakes\": true, \"showland\": true, \"subunitcolor\": \"white\"}, \"hoverlabel\": {\"align\": \"left\"}, \"hovermode\": \"closest\", \"mapbox\": {\"style\": \"light\"}, \"paper_bgcolor\": \"white\", \"plot_bgcolor\": \"#E5ECF6\", \"polar\": {\"angularaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"radialaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"scene\": {\"xaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"yaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}, \"zaxis\": {\"backgroundcolor\": \"#E5ECF6\", \"gridcolor\": \"white\", \"gridwidth\": 2, \"linecolor\": \"white\", \"showbackground\": true, \"ticks\": \"\", \"zerolinecolor\": \"white\"}}, \"shapedefaults\": {\"line\": {\"color\": \"#2a3f5f\"}}, \"ternary\": {\"aaxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"baxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}, \"bgcolor\": \"#E5ECF6\", \"caxis\": {\"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\"}}, \"title\": {\"x\": 0.05}, \"xaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}, \"yaxis\": {\"automargin\": true, \"gridcolor\": \"white\", \"linecolor\": \"white\", \"ticks\": \"\", \"title\": {\"standoff\": 15}, \"zerolinecolor\": \"white\", \"zerolinewidth\": 2}}}, \"width\": 600, \"xaxis\": {\"anchor\": \"y\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y\"}}, \"yaxis\": {\"anchor\": \"x\", \"domain\": [0.0, 1.0], \"title\": {\"text\": \"y_pred\"}}},\n",
              "                        {\"responsive\": true}\n",
              "                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('65003c36-2a1b-4b9e-9b20-a961232560b9');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })\n",
              "                };\n",
              "                \n",
              "            </script>\n",
              "        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}