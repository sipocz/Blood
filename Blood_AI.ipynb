{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blood_AI.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "history_visible": true,
      "authorship_tag": "ABX9TyNg8Y2GAZ9aQBWadQs623Hy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/Blood/blob/main/Blood_AI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "601BzKTbnvCK"
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 134,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UjLT9ydrlJi0"
      },
      "source": [
        "fname_url=\"https://github.com/sipocz/Blood/raw/b5fc44c487b0a712d00865014203349fb8690257/orig/train.zip\"\n",
        "fname=fname_url.split(\"/\")[-1]"
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhyXF-D3lf6m"
      },
      "source": [
        ""
      ],
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9UnhqQYAlOxX",
        "outputId": "d6650e57-6f66-4875-d574-ca331ed12d4b"
      },
      "source": [
        "!wget $fname_url\n"
      ],
      "execution_count": 136,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-11-27 15:38:43--  https://github.com/sipocz/Blood/raw/b5fc44c487b0a712d00865014203349fb8690257/orig/train.zip\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/sipocz/Blood/b5fc44c487b0a712d00865014203349fb8690257/orig/train.zip [following]\n",
            "--2021-11-27 15:38:43--  https://raw.githubusercontent.com/sipocz/Blood/b5fc44c487b0a712d00865014203349fb8690257/orig/train.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 16979362 (16M) [application/zip]\n",
            "Saving to: ‘train.zip’\n",
            "\n",
            "train.zip           100%[===================>]  16.19M  --.-KB/s    in 0.1s    \n",
            "\n",
            "2021-11-27 15:38:43 (125 MB/s) - ‘train.zip’ saved [16979362/16979362]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Gw9lljVmT2X",
        "outputId": "1639c34d-c3fd-4520-cf93-4325b34c3f14"
      },
      "source": [
        "!mkdir \"train\"\n",
        "!rm ./train/Train.csv"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘train’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOYlA2DxlvRZ",
        "outputId": "6ff63641-17a6-4f99-8ca8-147975682e9c"
      },
      "source": [
        "!unzip $fname -d \"./train\"\n",
        "!rm train.zip"
      ],
      "execution_count": 138,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  train.zip\n",
            "  inflating: ./train/Train.csv       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WMFGVAucnS0z"
      },
      "source": [
        "fname_train=\"./train/Train.csv\"\n",
        "df=pd.read_csv(fname_train)"
      ],
      "execution_count": 139,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 873
        },
        "id": "hhI8V3Q_n29O",
        "outputId": "cbf443b1-cc2e-4f3f-ed38-bede1234ed0c"
      },
      "source": [
        "df.head(25)"
      ],
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reading_ID</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>hdl_cholesterol_human</th>\n",
              "      <th>hemoglobin(hgb)_human</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_3SSHI56C</td>\n",
              "      <td>0.479669</td>\n",
              "      <td>0.477423</td>\n",
              "      <td>0.487956</td>\n",
              "      <td>0.491831</td>\n",
              "      <td>0.500516</td>\n",
              "      <td>0.502590</td>\n",
              "      <td>0.511561</td>\n",
              "      <td>0.514639</td>\n",
              "      <td>0.524245</td>\n",
              "      <td>0.536170</td>\n",
              "      <td>0.546407</td>\n",
              "      <td>0.561557</td>\n",
              "      <td>0.568417</td>\n",
              "      <td>0.571877</td>\n",
              "      <td>0.570884</td>\n",
              "      <td>0.569032</td>\n",
              "      <td>0.567476</td>\n",
              "      <td>0.565662</td>\n",
              "      <td>0.561901</td>\n",
              "      <td>0.559722</td>\n",
              "      <td>0.557474</td>\n",
              "      <td>0.554371</td>\n",
              "      <td>0.552386</td>\n",
              "      <td>0.548702</td>\n",
              "      <td>0.544238</td>\n",
              "      <td>0.542579</td>\n",
              "      <td>0.540514</td>\n",
              "      <td>0.538980</td>\n",
              "      <td>0.536650</td>\n",
              "      <td>0.536483</td>\n",
              "      <td>0.535447</td>\n",
              "      <td>0.537577</td>\n",
              "      <td>0.535715</td>\n",
              "      <td>0.536895</td>\n",
              "      <td>0.539589</td>\n",
              "      <td>0.541081</td>\n",
              "      <td>0.544893</td>\n",
              "      <td>0.547765</td>\n",
              "      <td>0.551773</td>\n",
              "      <td>...</td>\n",
              "      <td>1.469838</td>\n",
              "      <td>1.462617</td>\n",
              "      <td>1.445696</td>\n",
              "      <td>1.435586</td>\n",
              "      <td>1.417847</td>\n",
              "      <td>1.404205</td>\n",
              "      <td>1.388861</td>\n",
              "      <td>1.377436</td>\n",
              "      <td>1.364444</td>\n",
              "      <td>1.360373</td>\n",
              "      <td>1.341243</td>\n",
              "      <td>1.339632</td>\n",
              "      <td>1.321471</td>\n",
              "      <td>1.317444</td>\n",
              "      <td>1.311209</td>\n",
              "      <td>1.291677</td>\n",
              "      <td>1.285579</td>\n",
              "      <td>1.285488</td>\n",
              "      <td>1.275784</td>\n",
              "      <td>1.271104</td>\n",
              "      <td>1.264029</td>\n",
              "      <td>1.250779</td>\n",
              "      <td>1.254856</td>\n",
              "      <td>1.255224</td>\n",
              "      <td>1.249623</td>\n",
              "      <td>1.244419</td>\n",
              "      <td>1.244437</td>\n",
              "      <td>1.243933</td>\n",
              "      <td>1.226790</td>\n",
              "      <td>1.234055</td>\n",
              "      <td>1.218660</td>\n",
              "      <td>1.213038</td>\n",
              "      <td>1.198317</td>\n",
              "      <td>1.195735</td>\n",
              "      <td>1.180846</td>\n",
              "      <td>42.51</td>\n",
              "      <td>34.01</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_599OOLZA</td>\n",
              "      <td>0.471537</td>\n",
              "      <td>0.474113</td>\n",
              "      <td>0.479981</td>\n",
              "      <td>0.485528</td>\n",
              "      <td>0.491049</td>\n",
              "      <td>0.497942</td>\n",
              "      <td>0.504760</td>\n",
              "      <td>0.510543</td>\n",
              "      <td>0.522328</td>\n",
              "      <td>0.534423</td>\n",
              "      <td>0.548646</td>\n",
              "      <td>0.558420</td>\n",
              "      <td>0.565449</td>\n",
              "      <td>0.569717</td>\n",
              "      <td>0.570999</td>\n",
              "      <td>0.569969</td>\n",
              "      <td>0.568405</td>\n",
              "      <td>0.566628</td>\n",
              "      <td>0.564101</td>\n",
              "      <td>0.559951</td>\n",
              "      <td>0.556193</td>\n",
              "      <td>0.552271</td>\n",
              "      <td>0.550086</td>\n",
              "      <td>0.546207</td>\n",
              "      <td>0.542366</td>\n",
              "      <td>0.539789</td>\n",
              "      <td>0.537221</td>\n",
              "      <td>0.534336</td>\n",
              "      <td>0.533868</td>\n",
              "      <td>0.533018</td>\n",
              "      <td>0.532227</td>\n",
              "      <td>0.530818</td>\n",
              "      <td>0.532171</td>\n",
              "      <td>0.533658</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.538939</td>\n",
              "      <td>0.542399</td>\n",
              "      <td>0.546479</td>\n",
              "      <td>0.550606</td>\n",
              "      <td>...</td>\n",
              "      <td>1.552979</td>\n",
              "      <td>1.541997</td>\n",
              "      <td>1.533186</td>\n",
              "      <td>1.518359</td>\n",
              "      <td>1.498964</td>\n",
              "      <td>1.488043</td>\n",
              "      <td>1.472946</td>\n",
              "      <td>1.465925</td>\n",
              "      <td>1.452647</td>\n",
              "      <td>1.437819</td>\n",
              "      <td>1.423670</td>\n",
              "      <td>1.415103</td>\n",
              "      <td>1.401141</td>\n",
              "      <td>1.403560</td>\n",
              "      <td>1.384169</td>\n",
              "      <td>1.379410</td>\n",
              "      <td>1.374128</td>\n",
              "      <td>1.356969</td>\n",
              "      <td>1.352693</td>\n",
              "      <td>1.342430</td>\n",
              "      <td>1.339714</td>\n",
              "      <td>1.332805</td>\n",
              "      <td>1.336324</td>\n",
              "      <td>1.342537</td>\n",
              "      <td>1.332407</td>\n",
              "      <td>1.326258</td>\n",
              "      <td>1.336874</td>\n",
              "      <td>1.327538</td>\n",
              "      <td>1.311951</td>\n",
              "      <td>1.309399</td>\n",
              "      <td>1.304501</td>\n",
              "      <td>1.323005</td>\n",
              "      <td>1.305992</td>\n",
              "      <td>1.263887</td>\n",
              "      <td>1.262095</td>\n",
              "      <td>44.52</td>\n",
              "      <td>32.09</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_MVJGPQ75</td>\n",
              "      <td>0.444998</td>\n",
              "      <td>0.458034</td>\n",
              "      <td>0.447386</td>\n",
              "      <td>0.456921</td>\n",
              "      <td>0.463225</td>\n",
              "      <td>0.475983</td>\n",
              "      <td>0.476817</td>\n",
              "      <td>0.481565</td>\n",
              "      <td>0.490010</td>\n",
              "      <td>0.505892</td>\n",
              "      <td>0.518125</td>\n",
              "      <td>0.530362</td>\n",
              "      <td>0.538530</td>\n",
              "      <td>0.543128</td>\n",
              "      <td>0.546287</td>\n",
              "      <td>0.547001</td>\n",
              "      <td>0.547120</td>\n",
              "      <td>0.546351</td>\n",
              "      <td>0.544254</td>\n",
              "      <td>0.542802</td>\n",
              "      <td>0.542207</td>\n",
              "      <td>0.539779</td>\n",
              "      <td>0.536417</td>\n",
              "      <td>0.533380</td>\n",
              "      <td>0.531117</td>\n",
              "      <td>0.529093</td>\n",
              "      <td>0.526101</td>\n",
              "      <td>0.524599</td>\n",
              "      <td>0.522952</td>\n",
              "      <td>0.521551</td>\n",
              "      <td>0.521149</td>\n",
              "      <td>0.520478</td>\n",
              "      <td>0.521432</td>\n",
              "      <td>0.521473</td>\n",
              "      <td>0.523567</td>\n",
              "      <td>0.525816</td>\n",
              "      <td>0.527889</td>\n",
              "      <td>0.530697</td>\n",
              "      <td>0.533416</td>\n",
              "      <td>...</td>\n",
              "      <td>1.516723</td>\n",
              "      <td>1.502255</td>\n",
              "      <td>1.489132</td>\n",
              "      <td>1.483308</td>\n",
              "      <td>1.461028</td>\n",
              "      <td>1.453174</td>\n",
              "      <td>1.450412</td>\n",
              "      <td>1.437784</td>\n",
              "      <td>1.422148</td>\n",
              "      <td>1.415880</td>\n",
              "      <td>1.404698</td>\n",
              "      <td>1.388143</td>\n",
              "      <td>1.397241</td>\n",
              "      <td>1.385680</td>\n",
              "      <td>1.376355</td>\n",
              "      <td>1.357758</td>\n",
              "      <td>1.354854</td>\n",
              "      <td>1.345476</td>\n",
              "      <td>1.333810</td>\n",
              "      <td>1.332739</td>\n",
              "      <td>1.335550</td>\n",
              "      <td>1.326775</td>\n",
              "      <td>1.336862</td>\n",
              "      <td>1.316860</td>\n",
              "      <td>1.328051</td>\n",
              "      <td>1.328641</td>\n",
              "      <td>1.323526</td>\n",
              "      <td>1.314124</td>\n",
              "      <td>1.298936</td>\n",
              "      <td>1.289122</td>\n",
              "      <td>1.325059</td>\n",
              "      <td>1.271115</td>\n",
              "      <td>1.337119</td>\n",
              "      <td>1.289877</td>\n",
              "      <td>1.345229</td>\n",
              "      <td>45.77</td>\n",
              "      <td>24.80</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_CK6RF8YV</td>\n",
              "      <td>0.513434</td>\n",
              "      <td>0.513303</td>\n",
              "      <td>0.522609</td>\n",
              "      <td>0.521068</td>\n",
              "      <td>0.523146</td>\n",
              "      <td>0.530132</td>\n",
              "      <td>0.539517</td>\n",
              "      <td>0.546364</td>\n",
              "      <td>0.552414</td>\n",
              "      <td>0.565502</td>\n",
              "      <td>0.581143</td>\n",
              "      <td>0.594354</td>\n",
              "      <td>0.599457</td>\n",
              "      <td>0.604529</td>\n",
              "      <td>0.605267</td>\n",
              "      <td>0.606276</td>\n",
              "      <td>0.604895</td>\n",
              "      <td>0.603716</td>\n",
              "      <td>0.600683</td>\n",
              "      <td>0.598087</td>\n",
              "      <td>0.594303</td>\n",
              "      <td>0.589403</td>\n",
              "      <td>0.585883</td>\n",
              "      <td>0.581369</td>\n",
              "      <td>0.578962</td>\n",
              "      <td>0.575181</td>\n",
              "      <td>0.573274</td>\n",
              "      <td>0.570471</td>\n",
              "      <td>0.568241</td>\n",
              "      <td>0.565671</td>\n",
              "      <td>0.564579</td>\n",
              "      <td>0.563724</td>\n",
              "      <td>0.561978</td>\n",
              "      <td>0.562744</td>\n",
              "      <td>0.563455</td>\n",
              "      <td>0.565163</td>\n",
              "      <td>0.566505</td>\n",
              "      <td>0.569239</td>\n",
              "      <td>0.572075</td>\n",
              "      <td>...</td>\n",
              "      <td>1.442957</td>\n",
              "      <td>1.423349</td>\n",
              "      <td>1.413718</td>\n",
              "      <td>1.403112</td>\n",
              "      <td>1.393964</td>\n",
              "      <td>1.375741</td>\n",
              "      <td>1.369549</td>\n",
              "      <td>1.354179</td>\n",
              "      <td>1.344562</td>\n",
              "      <td>1.333491</td>\n",
              "      <td>1.325002</td>\n",
              "      <td>1.321572</td>\n",
              "      <td>1.305561</td>\n",
              "      <td>1.292637</td>\n",
              "      <td>1.287971</td>\n",
              "      <td>1.283460</td>\n",
              "      <td>1.278300</td>\n",
              "      <td>1.268486</td>\n",
              "      <td>1.268407</td>\n",
              "      <td>1.263479</td>\n",
              "      <td>1.252612</td>\n",
              "      <td>1.254306</td>\n",
              "      <td>1.247635</td>\n",
              "      <td>1.242321</td>\n",
              "      <td>1.247859</td>\n",
              "      <td>1.246749</td>\n",
              "      <td>1.249920</td>\n",
              "      <td>1.265223</td>\n",
              "      <td>1.264013</td>\n",
              "      <td>1.285252</td>\n",
              "      <td>1.298422</td>\n",
              "      <td>1.299873</td>\n",
              "      <td>1.311157</td>\n",
              "      <td>1.303259</td>\n",
              "      <td>1.349833</td>\n",
              "      <td>45.84</td>\n",
              "      <td>36.93</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_82N6QE6I</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>0.519359</td>\n",
              "      <td>0.524225</td>\n",
              "      <td>0.528419</td>\n",
              "      <td>0.535273</td>\n",
              "      <td>0.545342</td>\n",
              "      <td>0.550314</td>\n",
              "      <td>0.557129</td>\n",
              "      <td>0.567030</td>\n",
              "      <td>0.577731</td>\n",
              "      <td>0.589192</td>\n",
              "      <td>0.604401</td>\n",
              "      <td>0.611372</td>\n",
              "      <td>0.614571</td>\n",
              "      <td>0.619713</td>\n",
              "      <td>0.619805</td>\n",
              "      <td>0.622708</td>\n",
              "      <td>0.620036</td>\n",
              "      <td>0.618070</td>\n",
              "      <td>0.616470</td>\n",
              "      <td>0.614592</td>\n",
              "      <td>0.611658</td>\n",
              "      <td>0.609762</td>\n",
              "      <td>0.608088</td>\n",
              "      <td>0.604118</td>\n",
              "      <td>0.602248</td>\n",
              "      <td>0.598901</td>\n",
              "      <td>0.598259</td>\n",
              "      <td>0.597334</td>\n",
              "      <td>0.594730</td>\n",
              "      <td>0.593618</td>\n",
              "      <td>0.593828</td>\n",
              "      <td>0.595201</td>\n",
              "      <td>0.596143</td>\n",
              "      <td>0.597089</td>\n",
              "      <td>0.599811</td>\n",
              "      <td>0.602078</td>\n",
              "      <td>0.607372</td>\n",
              "      <td>0.610382</td>\n",
              "      <td>...</td>\n",
              "      <td>1.433939</td>\n",
              "      <td>1.427050</td>\n",
              "      <td>1.423872</td>\n",
              "      <td>1.409867</td>\n",
              "      <td>1.405768</td>\n",
              "      <td>1.402808</td>\n",
              "      <td>1.388843</td>\n",
              "      <td>1.378993</td>\n",
              "      <td>1.370712</td>\n",
              "      <td>1.364764</td>\n",
              "      <td>1.352886</td>\n",
              "      <td>1.353532</td>\n",
              "      <td>1.346231</td>\n",
              "      <td>1.332744</td>\n",
              "      <td>1.326119</td>\n",
              "      <td>1.317568</td>\n",
              "      <td>1.318767</td>\n",
              "      <td>1.316029</td>\n",
              "      <td>1.298889</td>\n",
              "      <td>1.301750</td>\n",
              "      <td>1.288821</td>\n",
              "      <td>1.299768</td>\n",
              "      <td>1.294653</td>\n",
              "      <td>1.294362</td>\n",
              "      <td>1.294515</td>\n",
              "      <td>1.293155</td>\n",
              "      <td>1.294068</td>\n",
              "      <td>1.296753</td>\n",
              "      <td>1.272020</td>\n",
              "      <td>1.272303</td>\n",
              "      <td>1.272367</td>\n",
              "      <td>1.290032</td>\n",
              "      <td>1.339771</td>\n",
              "      <td>1.322738</td>\n",
              "      <td>1.348964</td>\n",
              "      <td>38.92</td>\n",
              "      <td>23.88</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>ID_3LCCBJVO</td>\n",
              "      <td>0.547021</td>\n",
              "      <td>0.553422</td>\n",
              "      <td>0.563921</td>\n",
              "      <td>0.567715</td>\n",
              "      <td>0.579869</td>\n",
              "      <td>0.590251</td>\n",
              "      <td>0.595393</td>\n",
              "      <td>0.597208</td>\n",
              "      <td>0.604721</td>\n",
              "      <td>0.613363</td>\n",
              "      <td>0.629891</td>\n",
              "      <td>0.637479</td>\n",
              "      <td>0.646378</td>\n",
              "      <td>0.649843</td>\n",
              "      <td>0.650168</td>\n",
              "      <td>0.647816</td>\n",
              "      <td>0.647236</td>\n",
              "      <td>0.641927</td>\n",
              "      <td>0.640055</td>\n",
              "      <td>0.638313</td>\n",
              "      <td>0.635732</td>\n",
              "      <td>0.632370</td>\n",
              "      <td>0.627648</td>\n",
              "      <td>0.623230</td>\n",
              "      <td>0.622756</td>\n",
              "      <td>0.617522</td>\n",
              "      <td>0.617277</td>\n",
              "      <td>0.616496</td>\n",
              "      <td>0.614764</td>\n",
              "      <td>0.614822</td>\n",
              "      <td>0.614925</td>\n",
              "      <td>0.615644</td>\n",
              "      <td>0.614365</td>\n",
              "      <td>0.618069</td>\n",
              "      <td>0.619758</td>\n",
              "      <td>0.622689</td>\n",
              "      <td>0.625404</td>\n",
              "      <td>0.630995</td>\n",
              "      <td>0.634318</td>\n",
              "      <td>...</td>\n",
              "      <td>1.619531</td>\n",
              "      <td>1.601652</td>\n",
              "      <td>1.585859</td>\n",
              "      <td>1.565130</td>\n",
              "      <td>1.547383</td>\n",
              "      <td>1.544327</td>\n",
              "      <td>1.520427</td>\n",
              "      <td>1.510944</td>\n",
              "      <td>1.496858</td>\n",
              "      <td>1.490940</td>\n",
              "      <td>1.474053</td>\n",
              "      <td>1.471241</td>\n",
              "      <td>1.444634</td>\n",
              "      <td>1.439966</td>\n",
              "      <td>1.442507</td>\n",
              "      <td>1.435871</td>\n",
              "      <td>1.425861</td>\n",
              "      <td>1.404564</td>\n",
              "      <td>1.417542</td>\n",
              "      <td>1.406992</td>\n",
              "      <td>1.390208</td>\n",
              "      <td>1.385764</td>\n",
              "      <td>1.396185</td>\n",
              "      <td>1.376363</td>\n",
              "      <td>1.385960</td>\n",
              "      <td>1.372557</td>\n",
              "      <td>1.371252</td>\n",
              "      <td>1.356687</td>\n",
              "      <td>1.335159</td>\n",
              "      <td>1.304294</td>\n",
              "      <td>1.301758</td>\n",
              "      <td>1.273375</td>\n",
              "      <td>1.246971</td>\n",
              "      <td>1.254424</td>\n",
              "      <td>1.288679</td>\n",
              "      <td>43.64</td>\n",
              "      <td>18.85</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>ID_4TK4WAI7</td>\n",
              "      <td>0.517956</td>\n",
              "      <td>0.522156</td>\n",
              "      <td>0.529304</td>\n",
              "      <td>0.544396</td>\n",
              "      <td>0.553598</td>\n",
              "      <td>0.561382</td>\n",
              "      <td>0.564694</td>\n",
              "      <td>0.567961</td>\n",
              "      <td>0.577555</td>\n",
              "      <td>0.587931</td>\n",
              "      <td>0.601742</td>\n",
              "      <td>0.613439</td>\n",
              "      <td>0.618097</td>\n",
              "      <td>0.622032</td>\n",
              "      <td>0.624783</td>\n",
              "      <td>0.624759</td>\n",
              "      <td>0.622802</td>\n",
              "      <td>0.619930</td>\n",
              "      <td>0.615033</td>\n",
              "      <td>0.610743</td>\n",
              "      <td>0.606553</td>\n",
              "      <td>0.603162</td>\n",
              "      <td>0.599086</td>\n",
              "      <td>0.596881</td>\n",
              "      <td>0.593636</td>\n",
              "      <td>0.591159</td>\n",
              "      <td>0.586824</td>\n",
              "      <td>0.586088</td>\n",
              "      <td>0.583941</td>\n",
              "      <td>0.582967</td>\n",
              "      <td>0.583748</td>\n",
              "      <td>0.582958</td>\n",
              "      <td>0.586149</td>\n",
              "      <td>0.586066</td>\n",
              "      <td>0.588322</td>\n",
              "      <td>0.590775</td>\n",
              "      <td>0.594162</td>\n",
              "      <td>0.599934</td>\n",
              "      <td>0.604893</td>\n",
              "      <td>...</td>\n",
              "      <td>1.340094</td>\n",
              "      <td>1.339431</td>\n",
              "      <td>1.334440</td>\n",
              "      <td>1.330255</td>\n",
              "      <td>1.325320</td>\n",
              "      <td>1.316760</td>\n",
              "      <td>1.309547</td>\n",
              "      <td>1.302460</td>\n",
              "      <td>1.298736</td>\n",
              "      <td>1.290510</td>\n",
              "      <td>1.284450</td>\n",
              "      <td>1.280409</td>\n",
              "      <td>1.270666</td>\n",
              "      <td>1.265182</td>\n",
              "      <td>1.271462</td>\n",
              "      <td>1.262053</td>\n",
              "      <td>1.252310</td>\n",
              "      <td>1.253551</td>\n",
              "      <td>1.245541</td>\n",
              "      <td>1.242614</td>\n",
              "      <td>1.247659</td>\n",
              "      <td>1.232958</td>\n",
              "      <td>1.235297</td>\n",
              "      <td>1.236659</td>\n",
              "      <td>1.227565</td>\n",
              "      <td>1.215815</td>\n",
              "      <td>1.213983</td>\n",
              "      <td>1.194176</td>\n",
              "      <td>1.165942</td>\n",
              "      <td>1.123677</td>\n",
              "      <td>1.087181</td>\n",
              "      <td>1.058432</td>\n",
              "      <td>1.058078</td>\n",
              "      <td>1.062706</td>\n",
              "      <td>1.017410</td>\n",
              "      <td>40.82</td>\n",
              "      <td>25.71</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>ID_JH3G89RM</td>\n",
              "      <td>0.497032</td>\n",
              "      <td>0.495025</td>\n",
              "      <td>0.494494</td>\n",
              "      <td>0.503662</td>\n",
              "      <td>0.508607</td>\n",
              "      <td>0.512761</td>\n",
              "      <td>0.518266</td>\n",
              "      <td>0.527049</td>\n",
              "      <td>0.534846</td>\n",
              "      <td>0.551456</td>\n",
              "      <td>0.562976</td>\n",
              "      <td>0.570015</td>\n",
              "      <td>0.576002</td>\n",
              "      <td>0.579930</td>\n",
              "      <td>0.583430</td>\n",
              "      <td>0.584506</td>\n",
              "      <td>0.583918</td>\n",
              "      <td>0.582306</td>\n",
              "      <td>0.578642</td>\n",
              "      <td>0.571674</td>\n",
              "      <td>0.568641</td>\n",
              "      <td>0.565826</td>\n",
              "      <td>0.561967</td>\n",
              "      <td>0.559526</td>\n",
              "      <td>0.555189</td>\n",
              "      <td>0.553055</td>\n",
              "      <td>0.551266</td>\n",
              "      <td>0.549191</td>\n",
              "      <td>0.547121</td>\n",
              "      <td>0.546513</td>\n",
              "      <td>0.545903</td>\n",
              "      <td>0.546567</td>\n",
              "      <td>0.547303</td>\n",
              "      <td>0.547491</td>\n",
              "      <td>0.550442</td>\n",
              "      <td>0.551910</td>\n",
              "      <td>0.556186</td>\n",
              "      <td>0.559042</td>\n",
              "      <td>0.563675</td>\n",
              "      <td>...</td>\n",
              "      <td>1.431714</td>\n",
              "      <td>1.416828</td>\n",
              "      <td>1.406202</td>\n",
              "      <td>1.399577</td>\n",
              "      <td>1.380666</td>\n",
              "      <td>1.370982</td>\n",
              "      <td>1.360586</td>\n",
              "      <td>1.351963</td>\n",
              "      <td>1.341999</td>\n",
              "      <td>1.331654</td>\n",
              "      <td>1.325386</td>\n",
              "      <td>1.316798</td>\n",
              "      <td>1.306133</td>\n",
              "      <td>1.301673</td>\n",
              "      <td>1.286564</td>\n",
              "      <td>1.284383</td>\n",
              "      <td>1.281046</td>\n",
              "      <td>1.276344</td>\n",
              "      <td>1.267225</td>\n",
              "      <td>1.257744</td>\n",
              "      <td>1.262284</td>\n",
              "      <td>1.256494</td>\n",
              "      <td>1.247041</td>\n",
              "      <td>1.240575</td>\n",
              "      <td>1.243706</td>\n",
              "      <td>1.240145</td>\n",
              "      <td>1.242106</td>\n",
              "      <td>1.242746</td>\n",
              "      <td>1.247356</td>\n",
              "      <td>1.252587</td>\n",
              "      <td>1.259352</td>\n",
              "      <td>1.269610</td>\n",
              "      <td>1.242295</td>\n",
              "      <td>1.291084</td>\n",
              "      <td>1.307972</td>\n",
              "      <td>35.61</td>\n",
              "      <td>39.29</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>ID_2PF9JK4O</td>\n",
              "      <td>0.476799</td>\n",
              "      <td>0.481452</td>\n",
              "      <td>0.483358</td>\n",
              "      <td>0.490046</td>\n",
              "      <td>0.491382</td>\n",
              "      <td>0.500921</td>\n",
              "      <td>0.506735</td>\n",
              "      <td>0.512667</td>\n",
              "      <td>0.521872</td>\n",
              "      <td>0.534600</td>\n",
              "      <td>0.545943</td>\n",
              "      <td>0.556446</td>\n",
              "      <td>0.560816</td>\n",
              "      <td>0.564110</td>\n",
              "      <td>0.565412</td>\n",
              "      <td>0.562820</td>\n",
              "      <td>0.561745</td>\n",
              "      <td>0.559606</td>\n",
              "      <td>0.557143</td>\n",
              "      <td>0.551675</td>\n",
              "      <td>0.548093</td>\n",
              "      <td>0.544289</td>\n",
              "      <td>0.540714</td>\n",
              "      <td>0.537499</td>\n",
              "      <td>0.533655</td>\n",
              "      <td>0.530198</td>\n",
              "      <td>0.526903</td>\n",
              "      <td>0.525099</td>\n",
              "      <td>0.522626</td>\n",
              "      <td>0.520039</td>\n",
              "      <td>0.518661</td>\n",
              "      <td>0.518360</td>\n",
              "      <td>0.518170</td>\n",
              "      <td>0.518256</td>\n",
              "      <td>0.519445</td>\n",
              "      <td>0.521316</td>\n",
              "      <td>0.523887</td>\n",
              "      <td>0.526943</td>\n",
              "      <td>0.528759</td>\n",
              "      <td>...</td>\n",
              "      <td>1.360059</td>\n",
              "      <td>1.353342</td>\n",
              "      <td>1.340361</td>\n",
              "      <td>1.330619</td>\n",
              "      <td>1.317893</td>\n",
              "      <td>1.307962</td>\n",
              "      <td>1.296965</td>\n",
              "      <td>1.290124</td>\n",
              "      <td>1.282839</td>\n",
              "      <td>1.266203</td>\n",
              "      <td>1.259283</td>\n",
              "      <td>1.249760</td>\n",
              "      <td>1.243159</td>\n",
              "      <td>1.235632</td>\n",
              "      <td>1.230753</td>\n",
              "      <td>1.221015</td>\n",
              "      <td>1.220120</td>\n",
              "      <td>1.212250</td>\n",
              "      <td>1.201008</td>\n",
              "      <td>1.198230</td>\n",
              "      <td>1.194699</td>\n",
              "      <td>1.195321</td>\n",
              "      <td>1.188920</td>\n",
              "      <td>1.183865</td>\n",
              "      <td>1.176333</td>\n",
              "      <td>1.193434</td>\n",
              "      <td>1.191972</td>\n",
              "      <td>1.188710</td>\n",
              "      <td>1.183331</td>\n",
              "      <td>1.182594</td>\n",
              "      <td>1.193640</td>\n",
              "      <td>1.175582</td>\n",
              "      <td>1.160317</td>\n",
              "      <td>1.177578</td>\n",
              "      <td>1.199889</td>\n",
              "      <td>42.66</td>\n",
              "      <td>36.49</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>ID_TG56LKBY</td>\n",
              "      <td>0.501158</td>\n",
              "      <td>0.511572</td>\n",
              "      <td>0.513042</td>\n",
              "      <td>0.520507</td>\n",
              "      <td>0.532246</td>\n",
              "      <td>0.534862</td>\n",
              "      <td>0.538399</td>\n",
              "      <td>0.542913</td>\n",
              "      <td>0.551720</td>\n",
              "      <td>0.564315</td>\n",
              "      <td>0.576486</td>\n",
              "      <td>0.590521</td>\n",
              "      <td>0.594686</td>\n",
              "      <td>0.598900</td>\n",
              "      <td>0.601410</td>\n",
              "      <td>0.602715</td>\n",
              "      <td>0.599129</td>\n",
              "      <td>0.594219</td>\n",
              "      <td>0.590342</td>\n",
              "      <td>0.586998</td>\n",
              "      <td>0.581868</td>\n",
              "      <td>0.577346</td>\n",
              "      <td>0.574837</td>\n",
              "      <td>0.570588</td>\n",
              "      <td>0.568108</td>\n",
              "      <td>0.565730</td>\n",
              "      <td>0.561265</td>\n",
              "      <td>0.560377</td>\n",
              "      <td>0.558961</td>\n",
              "      <td>0.557671</td>\n",
              "      <td>0.558266</td>\n",
              "      <td>0.556738</td>\n",
              "      <td>0.559574</td>\n",
              "      <td>0.559237</td>\n",
              "      <td>0.562782</td>\n",
              "      <td>0.564828</td>\n",
              "      <td>0.567567</td>\n",
              "      <td>0.572686</td>\n",
              "      <td>0.577591</td>\n",
              "      <td>...</td>\n",
              "      <td>1.507671</td>\n",
              "      <td>1.500153</td>\n",
              "      <td>1.482257</td>\n",
              "      <td>1.460329</td>\n",
              "      <td>1.452718</td>\n",
              "      <td>1.447409</td>\n",
              "      <td>1.428541</td>\n",
              "      <td>1.415482</td>\n",
              "      <td>1.399454</td>\n",
              "      <td>1.391999</td>\n",
              "      <td>1.383576</td>\n",
              "      <td>1.367131</td>\n",
              "      <td>1.364227</td>\n",
              "      <td>1.354270</td>\n",
              "      <td>1.342661</td>\n",
              "      <td>1.340087</td>\n",
              "      <td>1.326571</td>\n",
              "      <td>1.315614</td>\n",
              "      <td>1.314378</td>\n",
              "      <td>1.300013</td>\n",
              "      <td>1.312269</td>\n",
              "      <td>1.293315</td>\n",
              "      <td>1.295108</td>\n",
              "      <td>1.291434</td>\n",
              "      <td>1.283910</td>\n",
              "      <td>1.280559</td>\n",
              "      <td>1.276986</td>\n",
              "      <td>1.273826</td>\n",
              "      <td>1.258434</td>\n",
              "      <td>1.225633</td>\n",
              "      <td>1.216975</td>\n",
              "      <td>1.195616</td>\n",
              "      <td>1.176820</td>\n",
              "      <td>1.194605</td>\n",
              "      <td>1.188291</td>\n",
              "      <td>40.51</td>\n",
              "      <td>27.73</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>ID_T1DP1ZHA</td>\n",
              "      <td>0.533322</td>\n",
              "      <td>0.539424</td>\n",
              "      <td>0.545329</td>\n",
              "      <td>0.546416</td>\n",
              "      <td>0.558224</td>\n",
              "      <td>0.561411</td>\n",
              "      <td>0.567811</td>\n",
              "      <td>0.572183</td>\n",
              "      <td>0.581994</td>\n",
              "      <td>0.593428</td>\n",
              "      <td>0.603944</td>\n",
              "      <td>0.614319</td>\n",
              "      <td>0.619232</td>\n",
              "      <td>0.622418</td>\n",
              "      <td>0.622137</td>\n",
              "      <td>0.621656</td>\n",
              "      <td>0.619891</td>\n",
              "      <td>0.616198</td>\n",
              "      <td>0.614068</td>\n",
              "      <td>0.608942</td>\n",
              "      <td>0.604813</td>\n",
              "      <td>0.601685</td>\n",
              "      <td>0.598516</td>\n",
              "      <td>0.594216</td>\n",
              "      <td>0.590210</td>\n",
              "      <td>0.588286</td>\n",
              "      <td>0.583849</td>\n",
              "      <td>0.582518</td>\n",
              "      <td>0.579421</td>\n",
              "      <td>0.578079</td>\n",
              "      <td>0.576921</td>\n",
              "      <td>0.576606</td>\n",
              "      <td>0.577378</td>\n",
              "      <td>0.578004</td>\n",
              "      <td>0.579369</td>\n",
              "      <td>0.581782</td>\n",
              "      <td>0.584198</td>\n",
              "      <td>0.586963</td>\n",
              "      <td>0.590624</td>\n",
              "      <td>...</td>\n",
              "      <td>1.487913</td>\n",
              "      <td>1.473140</td>\n",
              "      <td>1.459090</td>\n",
              "      <td>1.450959</td>\n",
              "      <td>1.436525</td>\n",
              "      <td>1.416577</td>\n",
              "      <td>1.421391</td>\n",
              "      <td>1.404057</td>\n",
              "      <td>1.396500</td>\n",
              "      <td>1.382821</td>\n",
              "      <td>1.378055</td>\n",
              "      <td>1.366452</td>\n",
              "      <td>1.361914</td>\n",
              "      <td>1.360174</td>\n",
              "      <td>1.348803</td>\n",
              "      <td>1.337057</td>\n",
              "      <td>1.326630</td>\n",
              "      <td>1.320497</td>\n",
              "      <td>1.316880</td>\n",
              "      <td>1.311394</td>\n",
              "      <td>1.302108</td>\n",
              "      <td>1.296543</td>\n",
              "      <td>1.301889</td>\n",
              "      <td>1.293975</td>\n",
              "      <td>1.297103</td>\n",
              "      <td>1.290198</td>\n",
              "      <td>1.279420</td>\n",
              "      <td>1.302608</td>\n",
              "      <td>1.285413</td>\n",
              "      <td>1.270976</td>\n",
              "      <td>1.236460</td>\n",
              "      <td>1.244555</td>\n",
              "      <td>1.232450</td>\n",
              "      <td>1.213745</td>\n",
              "      <td>1.269972</td>\n",
              "      <td>40.43</td>\n",
              "      <td>20.83</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>ID_DVGW9AE2</td>\n",
              "      <td>0.506780</td>\n",
              "      <td>0.503185</td>\n",
              "      <td>0.504951</td>\n",
              "      <td>0.509704</td>\n",
              "      <td>0.516427</td>\n",
              "      <td>0.524663</td>\n",
              "      <td>0.532535</td>\n",
              "      <td>0.538796</td>\n",
              "      <td>0.544318</td>\n",
              "      <td>0.555544</td>\n",
              "      <td>0.564778</td>\n",
              "      <td>0.574878</td>\n",
              "      <td>0.581914</td>\n",
              "      <td>0.584887</td>\n",
              "      <td>0.584803</td>\n",
              "      <td>0.585939</td>\n",
              "      <td>0.583675</td>\n",
              "      <td>0.580917</td>\n",
              "      <td>0.577735</td>\n",
              "      <td>0.574518</td>\n",
              "      <td>0.570219</td>\n",
              "      <td>0.567051</td>\n",
              "      <td>0.564170</td>\n",
              "      <td>0.559190</td>\n",
              "      <td>0.555728</td>\n",
              "      <td>0.552130</td>\n",
              "      <td>0.549197</td>\n",
              "      <td>0.545885</td>\n",
              "      <td>0.544145</td>\n",
              "      <td>0.541022</td>\n",
              "      <td>0.539481</td>\n",
              "      <td>0.538593</td>\n",
              "      <td>0.539111</td>\n",
              "      <td>0.539937</td>\n",
              "      <td>0.540890</td>\n",
              "      <td>0.542186</td>\n",
              "      <td>0.543975</td>\n",
              "      <td>0.547963</td>\n",
              "      <td>0.551250</td>\n",
              "      <td>...</td>\n",
              "      <td>1.489047</td>\n",
              "      <td>1.473832</td>\n",
              "      <td>1.465970</td>\n",
              "      <td>1.455264</td>\n",
              "      <td>1.445179</td>\n",
              "      <td>1.433169</td>\n",
              "      <td>1.422968</td>\n",
              "      <td>1.405365</td>\n",
              "      <td>1.399431</td>\n",
              "      <td>1.388078</td>\n",
              "      <td>1.375385</td>\n",
              "      <td>1.367160</td>\n",
              "      <td>1.358284</td>\n",
              "      <td>1.355689</td>\n",
              "      <td>1.339426</td>\n",
              "      <td>1.333068</td>\n",
              "      <td>1.321853</td>\n",
              "      <td>1.321816</td>\n",
              "      <td>1.310955</td>\n",
              "      <td>1.307531</td>\n",
              "      <td>1.298013</td>\n",
              "      <td>1.295986</td>\n",
              "      <td>1.296194</td>\n",
              "      <td>1.294479</td>\n",
              "      <td>1.298173</td>\n",
              "      <td>1.293773</td>\n",
              "      <td>1.283726</td>\n",
              "      <td>1.274911</td>\n",
              "      <td>1.256333</td>\n",
              "      <td>1.253887</td>\n",
              "      <td>1.247838</td>\n",
              "      <td>1.263763</td>\n",
              "      <td>1.273083</td>\n",
              "      <td>1.263241</td>\n",
              "      <td>1.218833</td>\n",
              "      <td>42.59</td>\n",
              "      <td>27.31</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>ID_XIAE5HYO</td>\n",
              "      <td>0.620263</td>\n",
              "      <td>0.622768</td>\n",
              "      <td>0.621894</td>\n",
              "      <td>0.630549</td>\n",
              "      <td>0.628610</td>\n",
              "      <td>0.636566</td>\n",
              "      <td>0.640318</td>\n",
              "      <td>0.647131</td>\n",
              "      <td>0.650764</td>\n",
              "      <td>0.663678</td>\n",
              "      <td>0.674730</td>\n",
              "      <td>0.685711</td>\n",
              "      <td>0.691660</td>\n",
              "      <td>0.694157</td>\n",
              "      <td>0.693474</td>\n",
              "      <td>0.694469</td>\n",
              "      <td>0.691624</td>\n",
              "      <td>0.689931</td>\n",
              "      <td>0.685000</td>\n",
              "      <td>0.682753</td>\n",
              "      <td>0.677292</td>\n",
              "      <td>0.673782</td>\n",
              "      <td>0.671168</td>\n",
              "      <td>0.666811</td>\n",
              "      <td>0.665180</td>\n",
              "      <td>0.661435</td>\n",
              "      <td>0.658878</td>\n",
              "      <td>0.657317</td>\n",
              "      <td>0.653538</td>\n",
              "      <td>0.652266</td>\n",
              "      <td>0.650949</td>\n",
              "      <td>0.651858</td>\n",
              "      <td>0.651231</td>\n",
              "      <td>0.651610</td>\n",
              "      <td>0.652462</td>\n",
              "      <td>0.654060</td>\n",
              "      <td>0.656629</td>\n",
              "      <td>0.660101</td>\n",
              "      <td>0.664659</td>\n",
              "      <td>...</td>\n",
              "      <td>1.579988</td>\n",
              "      <td>1.574531</td>\n",
              "      <td>1.557708</td>\n",
              "      <td>1.538951</td>\n",
              "      <td>1.532891</td>\n",
              "      <td>1.517946</td>\n",
              "      <td>1.507518</td>\n",
              "      <td>1.500325</td>\n",
              "      <td>1.491174</td>\n",
              "      <td>1.486600</td>\n",
              "      <td>1.470718</td>\n",
              "      <td>1.456234</td>\n",
              "      <td>1.456584</td>\n",
              "      <td>1.454104</td>\n",
              "      <td>1.432112</td>\n",
              "      <td>1.431333</td>\n",
              "      <td>1.419618</td>\n",
              "      <td>1.420286</td>\n",
              "      <td>1.409185</td>\n",
              "      <td>1.405440</td>\n",
              "      <td>1.411985</td>\n",
              "      <td>1.384578</td>\n",
              "      <td>1.392842</td>\n",
              "      <td>1.399018</td>\n",
              "      <td>1.399018</td>\n",
              "      <td>1.401446</td>\n",
              "      <td>1.385326</td>\n",
              "      <td>1.408364</td>\n",
              "      <td>1.390827</td>\n",
              "      <td>1.415412</td>\n",
              "      <td>1.362865</td>\n",
              "      <td>1.398627</td>\n",
              "      <td>1.398989</td>\n",
              "      <td>1.391138</td>\n",
              "      <td>1.439513</td>\n",
              "      <td>36.91</td>\n",
              "      <td>37.81</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>ID_KN93O8AZ</td>\n",
              "      <td>0.514019</td>\n",
              "      <td>0.521742</td>\n",
              "      <td>0.519481</td>\n",
              "      <td>0.526184</td>\n",
              "      <td>0.526774</td>\n",
              "      <td>0.533542</td>\n",
              "      <td>0.540119</td>\n",
              "      <td>0.544166</td>\n",
              "      <td>0.552173</td>\n",
              "      <td>0.562817</td>\n",
              "      <td>0.573925</td>\n",
              "      <td>0.581558</td>\n",
              "      <td>0.586374</td>\n",
              "      <td>0.588967</td>\n",
              "      <td>0.590405</td>\n",
              "      <td>0.591163</td>\n",
              "      <td>0.591231</td>\n",
              "      <td>0.590509</td>\n",
              "      <td>0.586150</td>\n",
              "      <td>0.580760</td>\n",
              "      <td>0.576966</td>\n",
              "      <td>0.573997</td>\n",
              "      <td>0.570140</td>\n",
              "      <td>0.567354</td>\n",
              "      <td>0.564042</td>\n",
              "      <td>0.560779</td>\n",
              "      <td>0.558475</td>\n",
              "      <td>0.557575</td>\n",
              "      <td>0.553973</td>\n",
              "      <td>0.553431</td>\n",
              "      <td>0.552910</td>\n",
              "      <td>0.552345</td>\n",
              "      <td>0.552157</td>\n",
              "      <td>0.552769</td>\n",
              "      <td>0.554519</td>\n",
              "      <td>0.555973</td>\n",
              "      <td>0.559224</td>\n",
              "      <td>0.562331</td>\n",
              "      <td>0.566019</td>\n",
              "      <td>...</td>\n",
              "      <td>1.408206</td>\n",
              "      <td>1.388749</td>\n",
              "      <td>1.379573</td>\n",
              "      <td>1.376561</td>\n",
              "      <td>1.359249</td>\n",
              "      <td>1.344746</td>\n",
              "      <td>1.341146</td>\n",
              "      <td>1.331543</td>\n",
              "      <td>1.314289</td>\n",
              "      <td>1.308593</td>\n",
              "      <td>1.298306</td>\n",
              "      <td>1.288016</td>\n",
              "      <td>1.280663</td>\n",
              "      <td>1.278692</td>\n",
              "      <td>1.264728</td>\n",
              "      <td>1.248277</td>\n",
              "      <td>1.247120</td>\n",
              "      <td>1.235404</td>\n",
              "      <td>1.230064</td>\n",
              "      <td>1.231890</td>\n",
              "      <td>1.226093</td>\n",
              "      <td>1.215046</td>\n",
              "      <td>1.213946</td>\n",
              "      <td>1.216632</td>\n",
              "      <td>1.204638</td>\n",
              "      <td>1.220927</td>\n",
              "      <td>1.207483</td>\n",
              "      <td>1.217560</td>\n",
              "      <td>1.193398</td>\n",
              "      <td>1.205409</td>\n",
              "      <td>1.189064</td>\n",
              "      <td>1.193926</td>\n",
              "      <td>1.187311</td>\n",
              "      <td>1.217152</td>\n",
              "      <td>1.180828</td>\n",
              "      <td>41.90</td>\n",
              "      <td>27.79</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>ID_8LULH20R</td>\n",
              "      <td>0.455756</td>\n",
              "      <td>0.458161</td>\n",
              "      <td>0.445533</td>\n",
              "      <td>0.462332</td>\n",
              "      <td>0.478318</td>\n",
              "      <td>0.485902</td>\n",
              "      <td>0.490047</td>\n",
              "      <td>0.490277</td>\n",
              "      <td>0.495857</td>\n",
              "      <td>0.512207</td>\n",
              "      <td>0.524825</td>\n",
              "      <td>0.534928</td>\n",
              "      <td>0.546134</td>\n",
              "      <td>0.547756</td>\n",
              "      <td>0.550858</td>\n",
              "      <td>0.552520</td>\n",
              "      <td>0.552849</td>\n",
              "      <td>0.551754</td>\n",
              "      <td>0.551600</td>\n",
              "      <td>0.550017</td>\n",
              "      <td>0.549694</td>\n",
              "      <td>0.546170</td>\n",
              "      <td>0.544800</td>\n",
              "      <td>0.541476</td>\n",
              "      <td>0.539689</td>\n",
              "      <td>0.536977</td>\n",
              "      <td>0.535775</td>\n",
              "      <td>0.533454</td>\n",
              "      <td>0.533857</td>\n",
              "      <td>0.532898</td>\n",
              "      <td>0.531468</td>\n",
              "      <td>0.531720</td>\n",
              "      <td>0.533095</td>\n",
              "      <td>0.533748</td>\n",
              "      <td>0.535394</td>\n",
              "      <td>0.538583</td>\n",
              "      <td>0.541361</td>\n",
              "      <td>0.544780</td>\n",
              "      <td>0.547111</td>\n",
              "      <td>...</td>\n",
              "      <td>1.472796</td>\n",
              "      <td>1.466220</td>\n",
              "      <td>1.447030</td>\n",
              "      <td>1.438269</td>\n",
              "      <td>1.435078</td>\n",
              "      <td>1.423082</td>\n",
              "      <td>1.410743</td>\n",
              "      <td>1.399645</td>\n",
              "      <td>1.391060</td>\n",
              "      <td>1.379028</td>\n",
              "      <td>1.367651</td>\n",
              "      <td>1.358038</td>\n",
              "      <td>1.349063</td>\n",
              "      <td>1.347326</td>\n",
              "      <td>1.334819</td>\n",
              "      <td>1.335798</td>\n",
              "      <td>1.327216</td>\n",
              "      <td>1.313709</td>\n",
              "      <td>1.319792</td>\n",
              "      <td>1.301149</td>\n",
              "      <td>1.304351</td>\n",
              "      <td>1.298919</td>\n",
              "      <td>1.282312</td>\n",
              "      <td>1.286067</td>\n",
              "      <td>1.281356</td>\n",
              "      <td>1.288485</td>\n",
              "      <td>1.281253</td>\n",
              "      <td>1.251523</td>\n",
              "      <td>1.253091</td>\n",
              "      <td>1.218990</td>\n",
              "      <td>1.224133</td>\n",
              "      <td>1.176671</td>\n",
              "      <td>1.196759</td>\n",
              "      <td>1.213463</td>\n",
              "      <td>1.148634</td>\n",
              "      <td>36.65</td>\n",
              "      <td>28.55</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>ID_H0QSY05G</td>\n",
              "      <td>0.542999</td>\n",
              "      <td>0.550722</td>\n",
              "      <td>0.551692</td>\n",
              "      <td>0.561606</td>\n",
              "      <td>0.566811</td>\n",
              "      <td>0.570654</td>\n",
              "      <td>0.574588</td>\n",
              "      <td>0.582285</td>\n",
              "      <td>0.592992</td>\n",
              "      <td>0.604535</td>\n",
              "      <td>0.616147</td>\n",
              "      <td>0.626246</td>\n",
              "      <td>0.632173</td>\n",
              "      <td>0.637910</td>\n",
              "      <td>0.636785</td>\n",
              "      <td>0.636222</td>\n",
              "      <td>0.634188</td>\n",
              "      <td>0.630974</td>\n",
              "      <td>0.627830</td>\n",
              "      <td>0.623527</td>\n",
              "      <td>0.619844</td>\n",
              "      <td>0.615741</td>\n",
              "      <td>0.612021</td>\n",
              "      <td>0.609283</td>\n",
              "      <td>0.604031</td>\n",
              "      <td>0.602070</td>\n",
              "      <td>0.599450</td>\n",
              "      <td>0.596964</td>\n",
              "      <td>0.594161</td>\n",
              "      <td>0.593911</td>\n",
              "      <td>0.591610</td>\n",
              "      <td>0.591322</td>\n",
              "      <td>0.592111</td>\n",
              "      <td>0.591287</td>\n",
              "      <td>0.594348</td>\n",
              "      <td>0.596697</td>\n",
              "      <td>0.597648</td>\n",
              "      <td>0.601684</td>\n",
              "      <td>0.606352</td>\n",
              "      <td>...</td>\n",
              "      <td>1.479251</td>\n",
              "      <td>1.464365</td>\n",
              "      <td>1.450654</td>\n",
              "      <td>1.450881</td>\n",
              "      <td>1.432104</td>\n",
              "      <td>1.418697</td>\n",
              "      <td>1.412684</td>\n",
              "      <td>1.409561</td>\n",
              "      <td>1.389428</td>\n",
              "      <td>1.379135</td>\n",
              "      <td>1.368141</td>\n",
              "      <td>1.356745</td>\n",
              "      <td>1.354144</td>\n",
              "      <td>1.345304</td>\n",
              "      <td>1.341310</td>\n",
              "      <td>1.327738</td>\n",
              "      <td>1.325008</td>\n",
              "      <td>1.315855</td>\n",
              "      <td>1.311860</td>\n",
              "      <td>1.308278</td>\n",
              "      <td>1.297543</td>\n",
              "      <td>1.287137</td>\n",
              "      <td>1.300836</td>\n",
              "      <td>1.287387</td>\n",
              "      <td>1.289907</td>\n",
              "      <td>1.292386</td>\n",
              "      <td>1.288627</td>\n",
              "      <td>1.285663</td>\n",
              "      <td>1.271174</td>\n",
              "      <td>1.281379</td>\n",
              "      <td>1.273351</td>\n",
              "      <td>1.291089</td>\n",
              "      <td>1.249775</td>\n",
              "      <td>1.287542</td>\n",
              "      <td>1.282575</td>\n",
              "      <td>42.24</td>\n",
              "      <td>29.26</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>ID_POLULW3R</td>\n",
              "      <td>0.480388</td>\n",
              "      <td>0.484286</td>\n",
              "      <td>0.488911</td>\n",
              "      <td>0.489383</td>\n",
              "      <td>0.497504</td>\n",
              "      <td>0.504784</td>\n",
              "      <td>0.507981</td>\n",
              "      <td>0.508161</td>\n",
              "      <td>0.513248</td>\n",
              "      <td>0.520026</td>\n",
              "      <td>0.532154</td>\n",
              "      <td>0.539283</td>\n",
              "      <td>0.544876</td>\n",
              "      <td>0.549147</td>\n",
              "      <td>0.548640</td>\n",
              "      <td>0.548751</td>\n",
              "      <td>0.546996</td>\n",
              "      <td>0.545740</td>\n",
              "      <td>0.542937</td>\n",
              "      <td>0.539799</td>\n",
              "      <td>0.536567</td>\n",
              "      <td>0.534106</td>\n",
              "      <td>0.530291</td>\n",
              "      <td>0.528857</td>\n",
              "      <td>0.525055</td>\n",
              "      <td>0.522655</td>\n",
              "      <td>0.520620</td>\n",
              "      <td>0.518030</td>\n",
              "      <td>0.517259</td>\n",
              "      <td>0.516750</td>\n",
              "      <td>0.515813</td>\n",
              "      <td>0.514962</td>\n",
              "      <td>0.516050</td>\n",
              "      <td>0.517827</td>\n",
              "      <td>0.519319</td>\n",
              "      <td>0.520252</td>\n",
              "      <td>0.524374</td>\n",
              "      <td>0.527132</td>\n",
              "      <td>0.532292</td>\n",
              "      <td>...</td>\n",
              "      <td>1.530934</td>\n",
              "      <td>1.525525</td>\n",
              "      <td>1.503134</td>\n",
              "      <td>1.485298</td>\n",
              "      <td>1.468455</td>\n",
              "      <td>1.452544</td>\n",
              "      <td>1.443475</td>\n",
              "      <td>1.423642</td>\n",
              "      <td>1.401456</td>\n",
              "      <td>1.398389</td>\n",
              "      <td>1.378824</td>\n",
              "      <td>1.371965</td>\n",
              "      <td>1.344619</td>\n",
              "      <td>1.342829</td>\n",
              "      <td>1.331158</td>\n",
              "      <td>1.324094</td>\n",
              "      <td>1.303596</td>\n",
              "      <td>1.306404</td>\n",
              "      <td>1.298050</td>\n",
              "      <td>1.296506</td>\n",
              "      <td>1.284992</td>\n",
              "      <td>1.273138</td>\n",
              "      <td>1.262420</td>\n",
              "      <td>1.270080</td>\n",
              "      <td>1.275270</td>\n",
              "      <td>1.268224</td>\n",
              "      <td>1.260547</td>\n",
              "      <td>1.257769</td>\n",
              "      <td>1.251147</td>\n",
              "      <td>1.225597</td>\n",
              "      <td>1.240040</td>\n",
              "      <td>1.200212</td>\n",
              "      <td>1.221530</td>\n",
              "      <td>1.213619</td>\n",
              "      <td>1.237070</td>\n",
              "      <td>42.67</td>\n",
              "      <td>50.22</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>ID_PM6IMECB</td>\n",
              "      <td>0.505570</td>\n",
              "      <td>0.512210</td>\n",
              "      <td>0.514343</td>\n",
              "      <td>0.524627</td>\n",
              "      <td>0.527987</td>\n",
              "      <td>0.540032</td>\n",
              "      <td>0.546584</td>\n",
              "      <td>0.552629</td>\n",
              "      <td>0.560304</td>\n",
              "      <td>0.570003</td>\n",
              "      <td>0.584266</td>\n",
              "      <td>0.594172</td>\n",
              "      <td>0.602246</td>\n",
              "      <td>0.606109</td>\n",
              "      <td>0.609507</td>\n",
              "      <td>0.609739</td>\n",
              "      <td>0.608385</td>\n",
              "      <td>0.606670</td>\n",
              "      <td>0.604067</td>\n",
              "      <td>0.600046</td>\n",
              "      <td>0.597531</td>\n",
              "      <td>0.594218</td>\n",
              "      <td>0.590967</td>\n",
              "      <td>0.587860</td>\n",
              "      <td>0.584767</td>\n",
              "      <td>0.582592</td>\n",
              "      <td>0.578941</td>\n",
              "      <td>0.576411</td>\n",
              "      <td>0.574790</td>\n",
              "      <td>0.572614</td>\n",
              "      <td>0.572051</td>\n",
              "      <td>0.572931</td>\n",
              "      <td>0.572608</td>\n",
              "      <td>0.572320</td>\n",
              "      <td>0.572937</td>\n",
              "      <td>0.574153</td>\n",
              "      <td>0.577089</td>\n",
              "      <td>0.581482</td>\n",
              "      <td>0.585025</td>\n",
              "      <td>...</td>\n",
              "      <td>1.482081</td>\n",
              "      <td>1.490677</td>\n",
              "      <td>1.471610</td>\n",
              "      <td>1.457740</td>\n",
              "      <td>1.454651</td>\n",
              "      <td>1.442201</td>\n",
              "      <td>1.427695</td>\n",
              "      <td>1.415470</td>\n",
              "      <td>1.405855</td>\n",
              "      <td>1.407814</td>\n",
              "      <td>1.382632</td>\n",
              "      <td>1.375559</td>\n",
              "      <td>1.366141</td>\n",
              "      <td>1.357276</td>\n",
              "      <td>1.352157</td>\n",
              "      <td>1.340483</td>\n",
              "      <td>1.334118</td>\n",
              "      <td>1.333064</td>\n",
              "      <td>1.330335</td>\n",
              "      <td>1.314870</td>\n",
              "      <td>1.318782</td>\n",
              "      <td>1.305676</td>\n",
              "      <td>1.314922</td>\n",
              "      <td>1.297215</td>\n",
              "      <td>1.305003</td>\n",
              "      <td>1.312237</td>\n",
              "      <td>1.292521</td>\n",
              "      <td>1.295840</td>\n",
              "      <td>1.305302</td>\n",
              "      <td>1.280556</td>\n",
              "      <td>1.264074</td>\n",
              "      <td>1.249191</td>\n",
              "      <td>1.228990</td>\n",
              "      <td>1.285064</td>\n",
              "      <td>1.294423</td>\n",
              "      <td>40.78</td>\n",
              "      <td>24.37</td>\n",
              "      <td>high</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>ID_MK2MX4L8</td>\n",
              "      <td>0.640017</td>\n",
              "      <td>0.634113</td>\n",
              "      <td>0.633602</td>\n",
              "      <td>0.648409</td>\n",
              "      <td>0.647512</td>\n",
              "      <td>0.654290</td>\n",
              "      <td>0.662544</td>\n",
              "      <td>0.671660</td>\n",
              "      <td>0.681424</td>\n",
              "      <td>0.694136</td>\n",
              "      <td>0.709047</td>\n",
              "      <td>0.720572</td>\n",
              "      <td>0.730122</td>\n",
              "      <td>0.735799</td>\n",
              "      <td>0.738313</td>\n",
              "      <td>0.738519</td>\n",
              "      <td>0.734734</td>\n",
              "      <td>0.734556</td>\n",
              "      <td>0.731965</td>\n",
              "      <td>0.729706</td>\n",
              "      <td>0.727349</td>\n",
              "      <td>0.724909</td>\n",
              "      <td>0.723667</td>\n",
              "      <td>0.716530</td>\n",
              "      <td>0.716533</td>\n",
              "      <td>0.712702</td>\n",
              "      <td>0.711535</td>\n",
              "      <td>0.708211</td>\n",
              "      <td>0.706823</td>\n",
              "      <td>0.705972</td>\n",
              "      <td>0.704040</td>\n",
              "      <td>0.706445</td>\n",
              "      <td>0.705469</td>\n",
              "      <td>0.707450</td>\n",
              "      <td>0.707679</td>\n",
              "      <td>0.711955</td>\n",
              "      <td>0.715085</td>\n",
              "      <td>0.719791</td>\n",
              "      <td>0.722711</td>\n",
              "      <td>...</td>\n",
              "      <td>1.572393</td>\n",
              "      <td>1.566039</td>\n",
              "      <td>1.553696</td>\n",
              "      <td>1.547869</td>\n",
              "      <td>1.545280</td>\n",
              "      <td>1.540873</td>\n",
              "      <td>1.523680</td>\n",
              "      <td>1.513518</td>\n",
              "      <td>1.518641</td>\n",
              "      <td>1.498393</td>\n",
              "      <td>1.493394</td>\n",
              "      <td>1.492158</td>\n",
              "      <td>1.487710</td>\n",
              "      <td>1.470232</td>\n",
              "      <td>1.470444</td>\n",
              "      <td>1.473489</td>\n",
              "      <td>1.453391</td>\n",
              "      <td>1.454391</td>\n",
              "      <td>1.457787</td>\n",
              "      <td>1.429909</td>\n",
              "      <td>1.440362</td>\n",
              "      <td>1.435718</td>\n",
              "      <td>1.419917</td>\n",
              "      <td>1.407103</td>\n",
              "      <td>1.436106</td>\n",
              "      <td>1.421840</td>\n",
              "      <td>1.432988</td>\n",
              "      <td>1.443302</td>\n",
              "      <td>1.422942</td>\n",
              "      <td>1.427234</td>\n",
              "      <td>1.442753</td>\n",
              "      <td>1.430408</td>\n",
              "      <td>1.441828</td>\n",
              "      <td>1.485907</td>\n",
              "      <td>1.475184</td>\n",
              "      <td>37.89</td>\n",
              "      <td>23.30</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>ID_RFDE66W9</td>\n",
              "      <td>0.507012</td>\n",
              "      <td>0.510025</td>\n",
              "      <td>0.515282</td>\n",
              "      <td>0.523655</td>\n",
              "      <td>0.534188</td>\n",
              "      <td>0.539592</td>\n",
              "      <td>0.544692</td>\n",
              "      <td>0.546570</td>\n",
              "      <td>0.554436</td>\n",
              "      <td>0.569306</td>\n",
              "      <td>0.582968</td>\n",
              "      <td>0.592032</td>\n",
              "      <td>0.598743</td>\n",
              "      <td>0.602723</td>\n",
              "      <td>0.605539</td>\n",
              "      <td>0.607923</td>\n",
              "      <td>0.604878</td>\n",
              "      <td>0.600378</td>\n",
              "      <td>0.595547</td>\n",
              "      <td>0.590885</td>\n",
              "      <td>0.585824</td>\n",
              "      <td>0.583060</td>\n",
              "      <td>0.581460</td>\n",
              "      <td>0.576218</td>\n",
              "      <td>0.574297</td>\n",
              "      <td>0.571424</td>\n",
              "      <td>0.567601</td>\n",
              "      <td>0.565667</td>\n",
              "      <td>0.565313</td>\n",
              "      <td>0.563325</td>\n",
              "      <td>0.562482</td>\n",
              "      <td>0.563046</td>\n",
              "      <td>0.562794</td>\n",
              "      <td>0.564468</td>\n",
              "      <td>0.567404</td>\n",
              "      <td>0.568658</td>\n",
              "      <td>0.572959</td>\n",
              "      <td>0.575787</td>\n",
              "      <td>0.580551</td>\n",
              "      <td>...</td>\n",
              "      <td>1.507773</td>\n",
              "      <td>1.492745</td>\n",
              "      <td>1.481453</td>\n",
              "      <td>1.469193</td>\n",
              "      <td>1.454413</td>\n",
              "      <td>1.443224</td>\n",
              "      <td>1.429313</td>\n",
              "      <td>1.412188</td>\n",
              "      <td>1.403097</td>\n",
              "      <td>1.394482</td>\n",
              "      <td>1.381723</td>\n",
              "      <td>1.369079</td>\n",
              "      <td>1.364387</td>\n",
              "      <td>1.354210</td>\n",
              "      <td>1.340518</td>\n",
              "      <td>1.342494</td>\n",
              "      <td>1.328245</td>\n",
              "      <td>1.319751</td>\n",
              "      <td>1.308187</td>\n",
              "      <td>1.300688</td>\n",
              "      <td>1.300711</td>\n",
              "      <td>1.301135</td>\n",
              "      <td>1.300411</td>\n",
              "      <td>1.279727</td>\n",
              "      <td>1.285812</td>\n",
              "      <td>1.290260</td>\n",
              "      <td>1.292131</td>\n",
              "      <td>1.278616</td>\n",
              "      <td>1.265786</td>\n",
              "      <td>1.241712</td>\n",
              "      <td>1.204473</td>\n",
              "      <td>1.211558</td>\n",
              "      <td>1.196400</td>\n",
              "      <td>1.195077</td>\n",
              "      <td>1.164541</td>\n",
              "      <td>36.99</td>\n",
              "      <td>27.70</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>ID_3DGBNTWV</td>\n",
              "      <td>0.501692</td>\n",
              "      <td>0.509407</td>\n",
              "      <td>0.511178</td>\n",
              "      <td>0.516916</td>\n",
              "      <td>0.523927</td>\n",
              "      <td>0.530858</td>\n",
              "      <td>0.537304</td>\n",
              "      <td>0.545135</td>\n",
              "      <td>0.552549</td>\n",
              "      <td>0.563067</td>\n",
              "      <td>0.579662</td>\n",
              "      <td>0.592978</td>\n",
              "      <td>0.598706</td>\n",
              "      <td>0.603177</td>\n",
              "      <td>0.605390</td>\n",
              "      <td>0.606107</td>\n",
              "      <td>0.606200</td>\n",
              "      <td>0.604235</td>\n",
              "      <td>0.602694</td>\n",
              "      <td>0.599306</td>\n",
              "      <td>0.595437</td>\n",
              "      <td>0.593513</td>\n",
              "      <td>0.589687</td>\n",
              "      <td>0.586492</td>\n",
              "      <td>0.583672</td>\n",
              "      <td>0.580507</td>\n",
              "      <td>0.577757</td>\n",
              "      <td>0.576040</td>\n",
              "      <td>0.575761</td>\n",
              "      <td>0.572758</td>\n",
              "      <td>0.571691</td>\n",
              "      <td>0.570993</td>\n",
              "      <td>0.573267</td>\n",
              "      <td>0.572819</td>\n",
              "      <td>0.574085</td>\n",
              "      <td>0.577948</td>\n",
              "      <td>0.579874</td>\n",
              "      <td>0.584845</td>\n",
              "      <td>0.587336</td>\n",
              "      <td>...</td>\n",
              "      <td>1.424066</td>\n",
              "      <td>1.411599</td>\n",
              "      <td>1.401013</td>\n",
              "      <td>1.391675</td>\n",
              "      <td>1.371323</td>\n",
              "      <td>1.366132</td>\n",
              "      <td>1.361032</td>\n",
              "      <td>1.351035</td>\n",
              "      <td>1.334600</td>\n",
              "      <td>1.334585</td>\n",
              "      <td>1.317017</td>\n",
              "      <td>1.321033</td>\n",
              "      <td>1.302488</td>\n",
              "      <td>1.282051</td>\n",
              "      <td>1.290357</td>\n",
              "      <td>1.282003</td>\n",
              "      <td>1.281953</td>\n",
              "      <td>1.271185</td>\n",
              "      <td>1.269351</td>\n",
              "      <td>1.256318</td>\n",
              "      <td>1.248913</td>\n",
              "      <td>1.246781</td>\n",
              "      <td>1.246037</td>\n",
              "      <td>1.245212</td>\n",
              "      <td>1.250784</td>\n",
              "      <td>1.240983</td>\n",
              "      <td>1.246481</td>\n",
              "      <td>1.244038</td>\n",
              "      <td>1.261703</td>\n",
              "      <td>1.258512</td>\n",
              "      <td>1.247332</td>\n",
              "      <td>1.268716</td>\n",
              "      <td>1.239587</td>\n",
              "      <td>1.275877</td>\n",
              "      <td>1.257140</td>\n",
              "      <td>41.97</td>\n",
              "      <td>28.54</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>ID_EOQL1GEK</td>\n",
              "      <td>0.470724</td>\n",
              "      <td>0.464901</td>\n",
              "      <td>0.468180</td>\n",
              "      <td>0.485426</td>\n",
              "      <td>0.492571</td>\n",
              "      <td>0.498100</td>\n",
              "      <td>0.502636</td>\n",
              "      <td>0.509155</td>\n",
              "      <td>0.515623</td>\n",
              "      <td>0.533936</td>\n",
              "      <td>0.545876</td>\n",
              "      <td>0.559091</td>\n",
              "      <td>0.564631</td>\n",
              "      <td>0.569806</td>\n",
              "      <td>0.573753</td>\n",
              "      <td>0.574024</td>\n",
              "      <td>0.573819</td>\n",
              "      <td>0.572417</td>\n",
              "      <td>0.569273</td>\n",
              "      <td>0.567100</td>\n",
              "      <td>0.565688</td>\n",
              "      <td>0.561124</td>\n",
              "      <td>0.559313</td>\n",
              "      <td>0.556701</td>\n",
              "      <td>0.553585</td>\n",
              "      <td>0.551637</td>\n",
              "      <td>0.549456</td>\n",
              "      <td>0.547866</td>\n",
              "      <td>0.546164</td>\n",
              "      <td>0.545929</td>\n",
              "      <td>0.545011</td>\n",
              "      <td>0.544741</td>\n",
              "      <td>0.547335</td>\n",
              "      <td>0.548532</td>\n",
              "      <td>0.550520</td>\n",
              "      <td>0.553775</td>\n",
              "      <td>0.555670</td>\n",
              "      <td>0.560191</td>\n",
              "      <td>0.564044</td>\n",
              "      <td>...</td>\n",
              "      <td>1.569722</td>\n",
              "      <td>1.561201</td>\n",
              "      <td>1.524295</td>\n",
              "      <td>1.524026</td>\n",
              "      <td>1.501238</td>\n",
              "      <td>1.489489</td>\n",
              "      <td>1.475165</td>\n",
              "      <td>1.468073</td>\n",
              "      <td>1.460419</td>\n",
              "      <td>1.442275</td>\n",
              "      <td>1.428946</td>\n",
              "      <td>1.427664</td>\n",
              "      <td>1.418660</td>\n",
              "      <td>1.411779</td>\n",
              "      <td>1.401626</td>\n",
              "      <td>1.373458</td>\n",
              "      <td>1.386935</td>\n",
              "      <td>1.372122</td>\n",
              "      <td>1.357645</td>\n",
              "      <td>1.350671</td>\n",
              "      <td>1.344258</td>\n",
              "      <td>1.332903</td>\n",
              "      <td>1.343604</td>\n",
              "      <td>1.334743</td>\n",
              "      <td>1.322305</td>\n",
              "      <td>1.328500</td>\n",
              "      <td>1.332263</td>\n",
              "      <td>1.310154</td>\n",
              "      <td>1.304377</td>\n",
              "      <td>1.265759</td>\n",
              "      <td>1.249456</td>\n",
              "      <td>1.233530</td>\n",
              "      <td>1.250935</td>\n",
              "      <td>1.254809</td>\n",
              "      <td>1.258003</td>\n",
              "      <td>38.70</td>\n",
              "      <td>29.81</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>ID_3PKOL674</td>\n",
              "      <td>0.528669</td>\n",
              "      <td>0.526331</td>\n",
              "      <td>0.527935</td>\n",
              "      <td>0.533068</td>\n",
              "      <td>0.539086</td>\n",
              "      <td>0.547181</td>\n",
              "      <td>0.549392</td>\n",
              "      <td>0.553565</td>\n",
              "      <td>0.560774</td>\n",
              "      <td>0.577845</td>\n",
              "      <td>0.589668</td>\n",
              "      <td>0.600204</td>\n",
              "      <td>0.607302</td>\n",
              "      <td>0.612749</td>\n",
              "      <td>0.616093</td>\n",
              "      <td>0.616432</td>\n",
              "      <td>0.614623</td>\n",
              "      <td>0.610248</td>\n",
              "      <td>0.607628</td>\n",
              "      <td>0.602880</td>\n",
              "      <td>0.599888</td>\n",
              "      <td>0.594900</td>\n",
              "      <td>0.593426</td>\n",
              "      <td>0.589410</td>\n",
              "      <td>0.586172</td>\n",
              "      <td>0.583467</td>\n",
              "      <td>0.580482</td>\n",
              "      <td>0.579379</td>\n",
              "      <td>0.575582</td>\n",
              "      <td>0.574645</td>\n",
              "      <td>0.573484</td>\n",
              "      <td>0.571228</td>\n",
              "      <td>0.572269</td>\n",
              "      <td>0.573207</td>\n",
              "      <td>0.573643</td>\n",
              "      <td>0.576148</td>\n",
              "      <td>0.579632</td>\n",
              "      <td>0.584699</td>\n",
              "      <td>0.586840</td>\n",
              "      <td>...</td>\n",
              "      <td>1.490019</td>\n",
              "      <td>1.479832</td>\n",
              "      <td>1.468665</td>\n",
              "      <td>1.457955</td>\n",
              "      <td>1.446261</td>\n",
              "      <td>1.432090</td>\n",
              "      <td>1.414983</td>\n",
              "      <td>1.403081</td>\n",
              "      <td>1.394045</td>\n",
              "      <td>1.389004</td>\n",
              "      <td>1.379475</td>\n",
              "      <td>1.363433</td>\n",
              "      <td>1.364572</td>\n",
              "      <td>1.356232</td>\n",
              "      <td>1.342094</td>\n",
              "      <td>1.341005</td>\n",
              "      <td>1.332193</td>\n",
              "      <td>1.330431</td>\n",
              "      <td>1.317935</td>\n",
              "      <td>1.308341</td>\n",
              "      <td>1.307556</td>\n",
              "      <td>1.304743</td>\n",
              "      <td>1.303824</td>\n",
              "      <td>1.299060</td>\n",
              "      <td>1.296589</td>\n",
              "      <td>1.307854</td>\n",
              "      <td>1.294584</td>\n",
              "      <td>1.307736</td>\n",
              "      <td>1.304482</td>\n",
              "      <td>1.313955</td>\n",
              "      <td>1.298835</td>\n",
              "      <td>1.303248</td>\n",
              "      <td>1.287314</td>\n",
              "      <td>1.331731</td>\n",
              "      <td>1.282784</td>\n",
              "      <td>41.29</td>\n",
              "      <td>40.39</td>\n",
              "      <td>low</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>ID_A132AJYK</td>\n",
              "      <td>0.521308</td>\n",
              "      <td>0.528730</td>\n",
              "      <td>0.534544</td>\n",
              "      <td>0.541134</td>\n",
              "      <td>0.551629</td>\n",
              "      <td>0.563528</td>\n",
              "      <td>0.569334</td>\n",
              "      <td>0.576578</td>\n",
              "      <td>0.581419</td>\n",
              "      <td>0.591352</td>\n",
              "      <td>0.603177</td>\n",
              "      <td>0.617760</td>\n",
              "      <td>0.623821</td>\n",
              "      <td>0.627658</td>\n",
              "      <td>0.629451</td>\n",
              "      <td>0.630534</td>\n",
              "      <td>0.630431</td>\n",
              "      <td>0.629203</td>\n",
              "      <td>0.625588</td>\n",
              "      <td>0.623105</td>\n",
              "      <td>0.619417</td>\n",
              "      <td>0.616145</td>\n",
              "      <td>0.613194</td>\n",
              "      <td>0.610833</td>\n",
              "      <td>0.606670</td>\n",
              "      <td>0.605946</td>\n",
              "      <td>0.602725</td>\n",
              "      <td>0.600965</td>\n",
              "      <td>0.599844</td>\n",
              "      <td>0.597315</td>\n",
              "      <td>0.598300</td>\n",
              "      <td>0.597124</td>\n",
              "      <td>0.596975</td>\n",
              "      <td>0.597901</td>\n",
              "      <td>0.599369</td>\n",
              "      <td>0.601227</td>\n",
              "      <td>0.603802</td>\n",
              "      <td>0.607329</td>\n",
              "      <td>0.611812</td>\n",
              "      <td>...</td>\n",
              "      <td>1.518584</td>\n",
              "      <td>1.507255</td>\n",
              "      <td>1.491587</td>\n",
              "      <td>1.482281</td>\n",
              "      <td>1.472702</td>\n",
              "      <td>1.449238</td>\n",
              "      <td>1.444698</td>\n",
              "      <td>1.434429</td>\n",
              "      <td>1.421780</td>\n",
              "      <td>1.416805</td>\n",
              "      <td>1.399805</td>\n",
              "      <td>1.388057</td>\n",
              "      <td>1.395524</td>\n",
              "      <td>1.377385</td>\n",
              "      <td>1.379084</td>\n",
              "      <td>1.363362</td>\n",
              "      <td>1.355993</td>\n",
              "      <td>1.353031</td>\n",
              "      <td>1.341690</td>\n",
              "      <td>1.339263</td>\n",
              "      <td>1.342159</td>\n",
              "      <td>1.335625</td>\n",
              "      <td>1.317853</td>\n",
              "      <td>1.321745</td>\n",
              "      <td>1.321170</td>\n",
              "      <td>1.332152</td>\n",
              "      <td>1.315839</td>\n",
              "      <td>1.312456</td>\n",
              "      <td>1.293857</td>\n",
              "      <td>1.273862</td>\n",
              "      <td>1.289259</td>\n",
              "      <td>1.257540</td>\n",
              "      <td>1.278369</td>\n",
              "      <td>1.263140</td>\n",
              "      <td>1.283117</td>\n",
              "      <td>43.04</td>\n",
              "      <td>25.52</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>ID_RP4R33FB</td>\n",
              "      <td>0.508980</td>\n",
              "      <td>0.520489</td>\n",
              "      <td>0.521961</td>\n",
              "      <td>0.524320</td>\n",
              "      <td>0.525000</td>\n",
              "      <td>0.532117</td>\n",
              "      <td>0.537728</td>\n",
              "      <td>0.545325</td>\n",
              "      <td>0.554166</td>\n",
              "      <td>0.565575</td>\n",
              "      <td>0.579381</td>\n",
              "      <td>0.594062</td>\n",
              "      <td>0.601067</td>\n",
              "      <td>0.602887</td>\n",
              "      <td>0.603288</td>\n",
              "      <td>0.603154</td>\n",
              "      <td>0.601868</td>\n",
              "      <td>0.601437</td>\n",
              "      <td>0.598541</td>\n",
              "      <td>0.595647</td>\n",
              "      <td>0.593481</td>\n",
              "      <td>0.590219</td>\n",
              "      <td>0.589125</td>\n",
              "      <td>0.586321</td>\n",
              "      <td>0.582752</td>\n",
              "      <td>0.580719</td>\n",
              "      <td>0.578390</td>\n",
              "      <td>0.575564</td>\n",
              "      <td>0.574541</td>\n",
              "      <td>0.573217</td>\n",
              "      <td>0.571509</td>\n",
              "      <td>0.571758</td>\n",
              "      <td>0.571875</td>\n",
              "      <td>0.572003</td>\n",
              "      <td>0.575706</td>\n",
              "      <td>0.576621</td>\n",
              "      <td>0.578925</td>\n",
              "      <td>0.582947</td>\n",
              "      <td>0.587870</td>\n",
              "      <td>...</td>\n",
              "      <td>1.528798</td>\n",
              "      <td>1.511752</td>\n",
              "      <td>1.505010</td>\n",
              "      <td>1.504191</td>\n",
              "      <td>1.488715</td>\n",
              "      <td>1.475409</td>\n",
              "      <td>1.469576</td>\n",
              "      <td>1.462945</td>\n",
              "      <td>1.449429</td>\n",
              "      <td>1.435752</td>\n",
              "      <td>1.430616</td>\n",
              "      <td>1.416770</td>\n",
              "      <td>1.408246</td>\n",
              "      <td>1.399431</td>\n",
              "      <td>1.392463</td>\n",
              "      <td>1.383971</td>\n",
              "      <td>1.378044</td>\n",
              "      <td>1.372854</td>\n",
              "      <td>1.368449</td>\n",
              "      <td>1.363555</td>\n",
              "      <td>1.353677</td>\n",
              "      <td>1.345444</td>\n",
              "      <td>1.331650</td>\n",
              "      <td>1.329576</td>\n",
              "      <td>1.338672</td>\n",
              "      <td>1.341780</td>\n",
              "      <td>1.350817</td>\n",
              "      <td>1.341978</td>\n",
              "      <td>1.340790</td>\n",
              "      <td>1.337173</td>\n",
              "      <td>1.323614</td>\n",
              "      <td>1.340671</td>\n",
              "      <td>1.331572</td>\n",
              "      <td>1.348094</td>\n",
              "      <td>1.303542</td>\n",
              "      <td>35.09</td>\n",
              "      <td>45.74</td>\n",
              "      <td>high</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>25 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Reading_ID  absorbance0  ...  hemoglobin(hgb)_human  cholesterol_ldl_human\n",
              "0   ID_3SSHI56C     0.479669  ...                     ok                     ok\n",
              "1   ID_599OOLZA     0.471537  ...                   high                   high\n",
              "2   ID_MVJGPQ75     0.444998  ...                     ok                   high\n",
              "3   ID_CK6RF8YV     0.513434  ...                     ok                   high\n",
              "4   ID_82N6QE6I     0.510485  ...                     ok                   high\n",
              "5   ID_3LCCBJVO     0.547021  ...                     ok                     ok\n",
              "6   ID_4TK4WAI7     0.517956  ...                     ok                   high\n",
              "7   ID_JH3G89RM     0.497032  ...                     ok                   high\n",
              "8   ID_2PF9JK4O     0.476799  ...                     ok                   high\n",
              "9   ID_TG56LKBY     0.501158  ...                     ok                   high\n",
              "10  ID_T1DP1ZHA     0.533322  ...                     ok                     ok\n",
              "11  ID_DVGW9AE2     0.506780  ...                   high                   high\n",
              "12  ID_XIAE5HYO     0.620263  ...                     ok                   high\n",
              "13  ID_KN93O8AZ     0.514019  ...                     ok                   high\n",
              "14  ID_8LULH20R     0.455756  ...                     ok                   high\n",
              "15  ID_H0QSY05G     0.542999  ...                     ok                     ok\n",
              "16  ID_POLULW3R     0.480388  ...                     ok                   high\n",
              "17  ID_PM6IMECB     0.505570  ...                     ok                     ok\n",
              "18  ID_MK2MX4L8     0.640017  ...                     ok                     ok\n",
              "19  ID_RFDE66W9     0.507012  ...                     ok                     ok\n",
              "20  ID_3DGBNTWV     0.501692  ...                     ok                     ok\n",
              "21  ID_EOQL1GEK     0.470724  ...                     ok                     ok\n",
              "22  ID_3PKOL674     0.528669  ...                     ok                   high\n",
              "23  ID_A132AJYK     0.521308  ...                     ok                     ok\n",
              "24  ID_RP4R33FB     0.508980  ...                     ok                     ok\n",
              "\n",
              "[25 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cfOHzrSSeQsu"
      },
      "source": [
        "def one_hot_encode_df(df,str_index):\n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    indf=df.copy(True)\n",
        "    one_hot=OneHotEncoder()\n",
        "    col=indf[str_index].values\n",
        "    col=col.reshape(-1,1)\n",
        "    one_hot.fit(col)\n",
        "    out=one_hot.transform(col).toarray()\n",
        "    codes=one_hot.get_feature_names_out()\n",
        "    print(codes)\n",
        "    for ind,code in enumerate(codes):\n",
        "        print(\"Itt:\",ind,codes[ind])\n",
        "        indf[str_index+\"_\"+code]=out[:,ind]\n",
        "    return(indf,one_hot)\n"
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TT7CFsfjhqO"
      },
      "source": [
        ""
      ],
      "execution_count": 141,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ta5rsccBiUDo",
        "outputId": "8e5a9c18-608a-4765-b04a-00bcc54c1cfc"
      },
      "source": [
        "hdl_col_df,hdl_onehot=one_hot_encode_df(df,\"hdl_cholesterol_human\")\n",
        "hemo_df,hemo_onehot=one_hot_encode_df(df,\"hemoglobin(hgb)_human\")\n",
        "col_df,col_onehot=one_hot_encode_df(df,\"cholesterol_ldl_human\")"
      ],
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['x0_high' 'x0_low' 'x0_ok']\n",
            "Itt: 0 x0_high\n",
            "Itt: 1 x0_low\n",
            "Itt: 2 x0_ok\n",
            "['x0_high' 'x0_low' 'x0_ok']\n",
            "Itt: 0 x0_high\n",
            "Itt: 1 x0_low\n",
            "Itt: 2 x0_ok\n",
            "['x0_high' 'x0_low' 'x0_ok']\n",
            "Itt: 0 x0_high\n",
            "Itt: 1 x0_low\n",
            "Itt: 2 x0_ok\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "svHNLLj_1nsb",
        "outputId": "fa6a50ab-99ad-4948-fd94-eb3bfacdd61b"
      },
      "source": [
        "hemo_df.tail()"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reading_ID</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>hdl_cholesterol_human</th>\n",
              "      <th>hemoglobin(hgb)_human</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "      <th>hemoglobin(hgb)_human_x0_high</th>\n",
              "      <th>hemoglobin(hgb)_human_x0_low</th>\n",
              "      <th>hemoglobin(hgb)_human_x0_ok</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>13135</th>\n",
              "      <td>ID_NGPC0DA3</td>\n",
              "      <td>0.483513</td>\n",
              "      <td>0.482732</td>\n",
              "      <td>0.487531</td>\n",
              "      <td>0.497946</td>\n",
              "      <td>0.500697</td>\n",
              "      <td>0.505740</td>\n",
              "      <td>0.511904</td>\n",
              "      <td>0.519018</td>\n",
              "      <td>0.529455</td>\n",
              "      <td>0.540356</td>\n",
              "      <td>0.554324</td>\n",
              "      <td>0.563337</td>\n",
              "      <td>0.568932</td>\n",
              "      <td>0.573057</td>\n",
              "      <td>0.575291</td>\n",
              "      <td>0.574014</td>\n",
              "      <td>0.573149</td>\n",
              "      <td>0.570979</td>\n",
              "      <td>0.567632</td>\n",
              "      <td>0.564020</td>\n",
              "      <td>0.560878</td>\n",
              "      <td>0.557749</td>\n",
              "      <td>0.554752</td>\n",
              "      <td>0.552457</td>\n",
              "      <td>0.548313</td>\n",
              "      <td>0.547369</td>\n",
              "      <td>0.544267</td>\n",
              "      <td>0.542225</td>\n",
              "      <td>0.542338</td>\n",
              "      <td>0.539435</td>\n",
              "      <td>0.539836</td>\n",
              "      <td>0.539883</td>\n",
              "      <td>0.540450</td>\n",
              "      <td>0.541485</td>\n",
              "      <td>0.542836</td>\n",
              "      <td>0.544809</td>\n",
              "      <td>0.548269</td>\n",
              "      <td>0.552457</td>\n",
              "      <td>0.556129</td>\n",
              "      <td>...</td>\n",
              "      <td>1.372840</td>\n",
              "      <td>1.357367</td>\n",
              "      <td>1.342472</td>\n",
              "      <td>1.334482</td>\n",
              "      <td>1.326614</td>\n",
              "      <td>1.314757</td>\n",
              "      <td>1.301887</td>\n",
              "      <td>1.292316</td>\n",
              "      <td>1.296431</td>\n",
              "      <td>1.281496</td>\n",
              "      <td>1.278615</td>\n",
              "      <td>1.268085</td>\n",
              "      <td>1.256004</td>\n",
              "      <td>1.255005</td>\n",
              "      <td>1.245078</td>\n",
              "      <td>1.245569</td>\n",
              "      <td>1.231796</td>\n",
              "      <td>1.228750</td>\n",
              "      <td>1.238677</td>\n",
              "      <td>1.230577</td>\n",
              "      <td>1.218419</td>\n",
              "      <td>1.224957</td>\n",
              "      <td>1.220593</td>\n",
              "      <td>1.218087</td>\n",
              "      <td>1.214382</td>\n",
              "      <td>1.208455</td>\n",
              "      <td>1.202338</td>\n",
              "      <td>1.189651</td>\n",
              "      <td>1.182641</td>\n",
              "      <td>1.206917</td>\n",
              "      <td>1.188173</td>\n",
              "      <td>1.188303</td>\n",
              "      <td>34.79</td>\n",
              "      <td>36.93</td>\n",
              "      <td>high</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13136</th>\n",
              "      <td>ID_XRBUD5U8</td>\n",
              "      <td>0.525435</td>\n",
              "      <td>0.527563</td>\n",
              "      <td>0.528863</td>\n",
              "      <td>0.531776</td>\n",
              "      <td>0.541156</td>\n",
              "      <td>0.547318</td>\n",
              "      <td>0.552549</td>\n",
              "      <td>0.559060</td>\n",
              "      <td>0.566892</td>\n",
              "      <td>0.581571</td>\n",
              "      <td>0.595248</td>\n",
              "      <td>0.603612</td>\n",
              "      <td>0.611313</td>\n",
              "      <td>0.614295</td>\n",
              "      <td>0.617476</td>\n",
              "      <td>0.617077</td>\n",
              "      <td>0.618908</td>\n",
              "      <td>0.616631</td>\n",
              "      <td>0.610841</td>\n",
              "      <td>0.604681</td>\n",
              "      <td>0.601588</td>\n",
              "      <td>0.598501</td>\n",
              "      <td>0.594149</td>\n",
              "      <td>0.589455</td>\n",
              "      <td>0.587673</td>\n",
              "      <td>0.584746</td>\n",
              "      <td>0.581093</td>\n",
              "      <td>0.580218</td>\n",
              "      <td>0.578071</td>\n",
              "      <td>0.577839</td>\n",
              "      <td>0.576454</td>\n",
              "      <td>0.576026</td>\n",
              "      <td>0.576220</td>\n",
              "      <td>0.579007</td>\n",
              "      <td>0.581564</td>\n",
              "      <td>0.584409</td>\n",
              "      <td>0.587786</td>\n",
              "      <td>0.591168</td>\n",
              "      <td>0.595874</td>\n",
              "      <td>...</td>\n",
              "      <td>1.545593</td>\n",
              "      <td>1.521200</td>\n",
              "      <td>1.518922</td>\n",
              "      <td>1.508497</td>\n",
              "      <td>1.501945</td>\n",
              "      <td>1.487737</td>\n",
              "      <td>1.481170</td>\n",
              "      <td>1.457265</td>\n",
              "      <td>1.444710</td>\n",
              "      <td>1.452271</td>\n",
              "      <td>1.428153</td>\n",
              "      <td>1.421319</td>\n",
              "      <td>1.419508</td>\n",
              "      <td>1.411549</td>\n",
              "      <td>1.415561</td>\n",
              "      <td>1.405296</td>\n",
              "      <td>1.390187</td>\n",
              "      <td>1.384455</td>\n",
              "      <td>1.383000</td>\n",
              "      <td>1.380202</td>\n",
              "      <td>1.383244</td>\n",
              "      <td>1.378410</td>\n",
              "      <td>1.377784</td>\n",
              "      <td>1.359367</td>\n",
              "      <td>1.379885</td>\n",
              "      <td>1.378378</td>\n",
              "      <td>1.367057</td>\n",
              "      <td>1.388165</td>\n",
              "      <td>1.375223</td>\n",
              "      <td>1.372435</td>\n",
              "      <td>1.395317</td>\n",
              "      <td>1.387709</td>\n",
              "      <td>43.12</td>\n",
              "      <td>19.14</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13137</th>\n",
              "      <td>ID_2M9L5NV2</td>\n",
              "      <td>0.512718</td>\n",
              "      <td>0.517815</td>\n",
              "      <td>0.524857</td>\n",
              "      <td>0.525466</td>\n",
              "      <td>0.536542</td>\n",
              "      <td>0.542930</td>\n",
              "      <td>0.550628</td>\n",
              "      <td>0.558939</td>\n",
              "      <td>0.567593</td>\n",
              "      <td>0.579192</td>\n",
              "      <td>0.595752</td>\n",
              "      <td>0.609749</td>\n",
              "      <td>0.617717</td>\n",
              "      <td>0.621270</td>\n",
              "      <td>0.625333</td>\n",
              "      <td>0.626705</td>\n",
              "      <td>0.623455</td>\n",
              "      <td>0.622948</td>\n",
              "      <td>0.618986</td>\n",
              "      <td>0.616963</td>\n",
              "      <td>0.614392</td>\n",
              "      <td>0.610574</td>\n",
              "      <td>0.606732</td>\n",
              "      <td>0.604206</td>\n",
              "      <td>0.601527</td>\n",
              "      <td>0.597425</td>\n",
              "      <td>0.595800</td>\n",
              "      <td>0.592601</td>\n",
              "      <td>0.590437</td>\n",
              "      <td>0.589152</td>\n",
              "      <td>0.588726</td>\n",
              "      <td>0.587134</td>\n",
              "      <td>0.588214</td>\n",
              "      <td>0.589699</td>\n",
              "      <td>0.590376</td>\n",
              "      <td>0.592544</td>\n",
              "      <td>0.594104</td>\n",
              "      <td>0.599264</td>\n",
              "      <td>0.603341</td>\n",
              "      <td>...</td>\n",
              "      <td>1.509437</td>\n",
              "      <td>1.497262</td>\n",
              "      <td>1.487541</td>\n",
              "      <td>1.481796</td>\n",
              "      <td>1.460926</td>\n",
              "      <td>1.458425</td>\n",
              "      <td>1.445150</td>\n",
              "      <td>1.430362</td>\n",
              "      <td>1.427516</td>\n",
              "      <td>1.420605</td>\n",
              "      <td>1.414007</td>\n",
              "      <td>1.415796</td>\n",
              "      <td>1.395715</td>\n",
              "      <td>1.399176</td>\n",
              "      <td>1.392111</td>\n",
              "      <td>1.384322</td>\n",
              "      <td>1.376676</td>\n",
              "      <td>1.364141</td>\n",
              "      <td>1.363428</td>\n",
              "      <td>1.366804</td>\n",
              "      <td>1.371499</td>\n",
              "      <td>1.367140</td>\n",
              "      <td>1.364490</td>\n",
              "      <td>1.367668</td>\n",
              "      <td>1.362808</td>\n",
              "      <td>1.385546</td>\n",
              "      <td>1.386140</td>\n",
              "      <td>1.396862</td>\n",
              "      <td>1.377644</td>\n",
              "      <td>1.428150</td>\n",
              "      <td>1.447840</td>\n",
              "      <td>1.384221</td>\n",
              "      <td>42.48</td>\n",
              "      <td>43.41</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13138</th>\n",
              "      <td>ID_C5V5SD2D</td>\n",
              "      <td>0.456747</td>\n",
              "      <td>0.472575</td>\n",
              "      <td>0.466935</td>\n",
              "      <td>0.466698</td>\n",
              "      <td>0.478546</td>\n",
              "      <td>0.486451</td>\n",
              "      <td>0.494838</td>\n",
              "      <td>0.496540</td>\n",
              "      <td>0.508274</td>\n",
              "      <td>0.521304</td>\n",
              "      <td>0.532652</td>\n",
              "      <td>0.542257</td>\n",
              "      <td>0.550854</td>\n",
              "      <td>0.556305</td>\n",
              "      <td>0.558084</td>\n",
              "      <td>0.559439</td>\n",
              "      <td>0.558418</td>\n",
              "      <td>0.554719</td>\n",
              "      <td>0.554852</td>\n",
              "      <td>0.552821</td>\n",
              "      <td>0.551641</td>\n",
              "      <td>0.549722</td>\n",
              "      <td>0.545233</td>\n",
              "      <td>0.542932</td>\n",
              "      <td>0.539894</td>\n",
              "      <td>0.536520</td>\n",
              "      <td>0.534783</td>\n",
              "      <td>0.530619</td>\n",
              "      <td>0.531076</td>\n",
              "      <td>0.530187</td>\n",
              "      <td>0.528713</td>\n",
              "      <td>0.527754</td>\n",
              "      <td>0.529230</td>\n",
              "      <td>0.529761</td>\n",
              "      <td>0.531586</td>\n",
              "      <td>0.533869</td>\n",
              "      <td>0.535897</td>\n",
              "      <td>0.539685</td>\n",
              "      <td>0.543911</td>\n",
              "      <td>...</td>\n",
              "      <td>1.350871</td>\n",
              "      <td>1.331415</td>\n",
              "      <td>1.326455</td>\n",
              "      <td>1.322705</td>\n",
              "      <td>1.310202</td>\n",
              "      <td>1.302262</td>\n",
              "      <td>1.294962</td>\n",
              "      <td>1.287724</td>\n",
              "      <td>1.283147</td>\n",
              "      <td>1.269678</td>\n",
              "      <td>1.257537</td>\n",
              "      <td>1.259994</td>\n",
              "      <td>1.248000</td>\n",
              "      <td>1.253335</td>\n",
              "      <td>1.237789</td>\n",
              "      <td>1.228726</td>\n",
              "      <td>1.224605</td>\n",
              "      <td>1.224872</td>\n",
              "      <td>1.217346</td>\n",
              "      <td>1.225207</td>\n",
              "      <td>1.224785</td>\n",
              "      <td>1.214651</td>\n",
              "      <td>1.214882</td>\n",
              "      <td>1.214873</td>\n",
              "      <td>1.217364</td>\n",
              "      <td>1.208545</td>\n",
              "      <td>1.185192</td>\n",
              "      <td>1.181483</td>\n",
              "      <td>1.172430</td>\n",
              "      <td>1.174544</td>\n",
              "      <td>1.154095</td>\n",
              "      <td>1.212907</td>\n",
              "      <td>41.86</td>\n",
              "      <td>35.10</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>ok</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13139</th>\n",
              "      <td>ID_9R5ZR9NX</td>\n",
              "      <td>0.498372</td>\n",
              "      <td>0.501692</td>\n",
              "      <td>0.505924</td>\n",
              "      <td>0.510975</td>\n",
              "      <td>0.516707</td>\n",
              "      <td>0.520821</td>\n",
              "      <td>0.531242</td>\n",
              "      <td>0.537302</td>\n",
              "      <td>0.547303</td>\n",
              "      <td>0.563789</td>\n",
              "      <td>0.574825</td>\n",
              "      <td>0.585327</td>\n",
              "      <td>0.588966</td>\n",
              "      <td>0.594199</td>\n",
              "      <td>0.596798</td>\n",
              "      <td>0.598474</td>\n",
              "      <td>0.597812</td>\n",
              "      <td>0.596782</td>\n",
              "      <td>0.593260</td>\n",
              "      <td>0.587383</td>\n",
              "      <td>0.583925</td>\n",
              "      <td>0.581256</td>\n",
              "      <td>0.576630</td>\n",
              "      <td>0.574387</td>\n",
              "      <td>0.571707</td>\n",
              "      <td>0.568392</td>\n",
              "      <td>0.566264</td>\n",
              "      <td>0.565816</td>\n",
              "      <td>0.564117</td>\n",
              "      <td>0.563134</td>\n",
              "      <td>0.562717</td>\n",
              "      <td>0.564241</td>\n",
              "      <td>0.564816</td>\n",
              "      <td>0.566884</td>\n",
              "      <td>0.567930</td>\n",
              "      <td>0.571471</td>\n",
              "      <td>0.574409</td>\n",
              "      <td>0.579764</td>\n",
              "      <td>0.584740</td>\n",
              "      <td>...</td>\n",
              "      <td>1.546583</td>\n",
              "      <td>1.527910</td>\n",
              "      <td>1.508845</td>\n",
              "      <td>1.501612</td>\n",
              "      <td>1.493668</td>\n",
              "      <td>1.486875</td>\n",
              "      <td>1.467783</td>\n",
              "      <td>1.458154</td>\n",
              "      <td>1.447286</td>\n",
              "      <td>1.428325</td>\n",
              "      <td>1.429163</td>\n",
              "      <td>1.415660</td>\n",
              "      <td>1.407922</td>\n",
              "      <td>1.410762</td>\n",
              "      <td>1.396728</td>\n",
              "      <td>1.392131</td>\n",
              "      <td>1.384697</td>\n",
              "      <td>1.385565</td>\n",
              "      <td>1.376430</td>\n",
              "      <td>1.372075</td>\n",
              "      <td>1.359203</td>\n",
              "      <td>1.369151</td>\n",
              "      <td>1.369125</td>\n",
              "      <td>1.370765</td>\n",
              "      <td>1.368541</td>\n",
              "      <td>1.379918</td>\n",
              "      <td>1.391229</td>\n",
              "      <td>1.380881</td>\n",
              "      <td>1.401339</td>\n",
              "      <td>1.443170</td>\n",
              "      <td>1.350667</td>\n",
              "      <td>1.420639</td>\n",
              "      <td>42.91</td>\n",
              "      <td>26.30</td>\n",
              "      <td>high</td>\n",
              "      <td>ok</td>\n",
              "      <td>high</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 179 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Reading_ID  ...  hemoglobin(hgb)_human_x0_ok\n",
              "13135  ID_NGPC0DA3  ...                          1.0\n",
              "13136  ID_XRBUD5U8  ...                          1.0\n",
              "13137  ID_2M9L5NV2  ...                          1.0\n",
              "13138  ID_C5V5SD2D  ...                          1.0\n",
              "13139  ID_9R5ZR9NX  ...                          1.0\n",
              "\n",
              "[5 rows x 179 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 143
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tOm2B8lekRVR"
      },
      "source": [
        "del hdl_col_df[\"hdl_cholesterol_human\"]\n",
        "del hdl_col_df[\"hemoglobin(hgb)_human\"]\n",
        "del hdl_col_df[\"cholesterol_ldl_human\"]\n",
        "del hemo_df[\"hdl_cholesterol_human\"]\n",
        "del hemo_df[\"hemoglobin(hgb)_human\"]\n",
        "del hemo_df[\"cholesterol_ldl_human\"]\n",
        "del col_df[\"hdl_cholesterol_human\"]\n",
        "del col_df[\"hemoglobin(hgb)_human\"]\n",
        "del col_df[\"cholesterol_ldl_human\"]\n",
        "\n",
        "\n"
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "id": "64sg2pCEmOde",
        "outputId": "891562d0-c8ae-47bd-ea96-ab179fe2e117"
      },
      "source": [
        "hdl_col_df.head()"
      ],
      "execution_count": 145,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Reading_ID</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>hdl_cholesterol_human_x0_high</th>\n",
              "      <th>hdl_cholesterol_human_x0_low</th>\n",
              "      <th>hdl_cholesterol_human_x0_ok</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ID_3SSHI56C</td>\n",
              "      <td>0.479669</td>\n",
              "      <td>0.477423</td>\n",
              "      <td>0.487956</td>\n",
              "      <td>0.491831</td>\n",
              "      <td>0.500516</td>\n",
              "      <td>0.502590</td>\n",
              "      <td>0.511561</td>\n",
              "      <td>0.514639</td>\n",
              "      <td>0.524245</td>\n",
              "      <td>0.536170</td>\n",
              "      <td>0.546407</td>\n",
              "      <td>0.561557</td>\n",
              "      <td>0.568417</td>\n",
              "      <td>0.571877</td>\n",
              "      <td>0.570884</td>\n",
              "      <td>0.569032</td>\n",
              "      <td>0.567476</td>\n",
              "      <td>0.565662</td>\n",
              "      <td>0.561901</td>\n",
              "      <td>0.559722</td>\n",
              "      <td>0.557474</td>\n",
              "      <td>0.554371</td>\n",
              "      <td>0.552386</td>\n",
              "      <td>0.548702</td>\n",
              "      <td>0.544238</td>\n",
              "      <td>0.542579</td>\n",
              "      <td>0.540514</td>\n",
              "      <td>0.538980</td>\n",
              "      <td>0.536650</td>\n",
              "      <td>0.536483</td>\n",
              "      <td>0.535447</td>\n",
              "      <td>0.537577</td>\n",
              "      <td>0.535715</td>\n",
              "      <td>0.536895</td>\n",
              "      <td>0.539589</td>\n",
              "      <td>0.541081</td>\n",
              "      <td>0.544893</td>\n",
              "      <td>0.547765</td>\n",
              "      <td>0.551773</td>\n",
              "      <td>...</td>\n",
              "      <td>1.469838</td>\n",
              "      <td>1.462617</td>\n",
              "      <td>1.445696</td>\n",
              "      <td>1.435586</td>\n",
              "      <td>1.417847</td>\n",
              "      <td>1.404205</td>\n",
              "      <td>1.388861</td>\n",
              "      <td>1.377436</td>\n",
              "      <td>1.364444</td>\n",
              "      <td>1.360373</td>\n",
              "      <td>1.341243</td>\n",
              "      <td>1.339632</td>\n",
              "      <td>1.321471</td>\n",
              "      <td>1.317444</td>\n",
              "      <td>1.311209</td>\n",
              "      <td>1.291677</td>\n",
              "      <td>1.285579</td>\n",
              "      <td>1.285488</td>\n",
              "      <td>1.275784</td>\n",
              "      <td>1.271104</td>\n",
              "      <td>1.264029</td>\n",
              "      <td>1.250779</td>\n",
              "      <td>1.254856</td>\n",
              "      <td>1.255224</td>\n",
              "      <td>1.249623</td>\n",
              "      <td>1.244419</td>\n",
              "      <td>1.244437</td>\n",
              "      <td>1.243933</td>\n",
              "      <td>1.226790</td>\n",
              "      <td>1.234055</td>\n",
              "      <td>1.218660</td>\n",
              "      <td>1.213038</td>\n",
              "      <td>1.198317</td>\n",
              "      <td>1.195735</td>\n",
              "      <td>1.180846</td>\n",
              "      <td>42.51</td>\n",
              "      <td>34.01</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ID_599OOLZA</td>\n",
              "      <td>0.471537</td>\n",
              "      <td>0.474113</td>\n",
              "      <td>0.479981</td>\n",
              "      <td>0.485528</td>\n",
              "      <td>0.491049</td>\n",
              "      <td>0.497942</td>\n",
              "      <td>0.504760</td>\n",
              "      <td>0.510543</td>\n",
              "      <td>0.522328</td>\n",
              "      <td>0.534423</td>\n",
              "      <td>0.548646</td>\n",
              "      <td>0.558420</td>\n",
              "      <td>0.565449</td>\n",
              "      <td>0.569717</td>\n",
              "      <td>0.570999</td>\n",
              "      <td>0.569969</td>\n",
              "      <td>0.568405</td>\n",
              "      <td>0.566628</td>\n",
              "      <td>0.564101</td>\n",
              "      <td>0.559951</td>\n",
              "      <td>0.556193</td>\n",
              "      <td>0.552271</td>\n",
              "      <td>0.550086</td>\n",
              "      <td>0.546207</td>\n",
              "      <td>0.542366</td>\n",
              "      <td>0.539789</td>\n",
              "      <td>0.537221</td>\n",
              "      <td>0.534336</td>\n",
              "      <td>0.533868</td>\n",
              "      <td>0.533018</td>\n",
              "      <td>0.532227</td>\n",
              "      <td>0.530818</td>\n",
              "      <td>0.532171</td>\n",
              "      <td>0.533658</td>\n",
              "      <td>0.535266</td>\n",
              "      <td>0.538939</td>\n",
              "      <td>0.542399</td>\n",
              "      <td>0.546479</td>\n",
              "      <td>0.550606</td>\n",
              "      <td>...</td>\n",
              "      <td>1.552979</td>\n",
              "      <td>1.541997</td>\n",
              "      <td>1.533186</td>\n",
              "      <td>1.518359</td>\n",
              "      <td>1.498964</td>\n",
              "      <td>1.488043</td>\n",
              "      <td>1.472946</td>\n",
              "      <td>1.465925</td>\n",
              "      <td>1.452647</td>\n",
              "      <td>1.437819</td>\n",
              "      <td>1.423670</td>\n",
              "      <td>1.415103</td>\n",
              "      <td>1.401141</td>\n",
              "      <td>1.403560</td>\n",
              "      <td>1.384169</td>\n",
              "      <td>1.379410</td>\n",
              "      <td>1.374128</td>\n",
              "      <td>1.356969</td>\n",
              "      <td>1.352693</td>\n",
              "      <td>1.342430</td>\n",
              "      <td>1.339714</td>\n",
              "      <td>1.332805</td>\n",
              "      <td>1.336324</td>\n",
              "      <td>1.342537</td>\n",
              "      <td>1.332407</td>\n",
              "      <td>1.326258</td>\n",
              "      <td>1.336874</td>\n",
              "      <td>1.327538</td>\n",
              "      <td>1.311951</td>\n",
              "      <td>1.309399</td>\n",
              "      <td>1.304501</td>\n",
              "      <td>1.323005</td>\n",
              "      <td>1.305992</td>\n",
              "      <td>1.263887</td>\n",
              "      <td>1.262095</td>\n",
              "      <td>44.52</td>\n",
              "      <td>32.09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ID_MVJGPQ75</td>\n",
              "      <td>0.444998</td>\n",
              "      <td>0.458034</td>\n",
              "      <td>0.447386</td>\n",
              "      <td>0.456921</td>\n",
              "      <td>0.463225</td>\n",
              "      <td>0.475983</td>\n",
              "      <td>0.476817</td>\n",
              "      <td>0.481565</td>\n",
              "      <td>0.490010</td>\n",
              "      <td>0.505892</td>\n",
              "      <td>0.518125</td>\n",
              "      <td>0.530362</td>\n",
              "      <td>0.538530</td>\n",
              "      <td>0.543128</td>\n",
              "      <td>0.546287</td>\n",
              "      <td>0.547001</td>\n",
              "      <td>0.547120</td>\n",
              "      <td>0.546351</td>\n",
              "      <td>0.544254</td>\n",
              "      <td>0.542802</td>\n",
              "      <td>0.542207</td>\n",
              "      <td>0.539779</td>\n",
              "      <td>0.536417</td>\n",
              "      <td>0.533380</td>\n",
              "      <td>0.531117</td>\n",
              "      <td>0.529093</td>\n",
              "      <td>0.526101</td>\n",
              "      <td>0.524599</td>\n",
              "      <td>0.522952</td>\n",
              "      <td>0.521551</td>\n",
              "      <td>0.521149</td>\n",
              "      <td>0.520478</td>\n",
              "      <td>0.521432</td>\n",
              "      <td>0.521473</td>\n",
              "      <td>0.523567</td>\n",
              "      <td>0.525816</td>\n",
              "      <td>0.527889</td>\n",
              "      <td>0.530697</td>\n",
              "      <td>0.533416</td>\n",
              "      <td>...</td>\n",
              "      <td>1.516723</td>\n",
              "      <td>1.502255</td>\n",
              "      <td>1.489132</td>\n",
              "      <td>1.483308</td>\n",
              "      <td>1.461028</td>\n",
              "      <td>1.453174</td>\n",
              "      <td>1.450412</td>\n",
              "      <td>1.437784</td>\n",
              "      <td>1.422148</td>\n",
              "      <td>1.415880</td>\n",
              "      <td>1.404698</td>\n",
              "      <td>1.388143</td>\n",
              "      <td>1.397241</td>\n",
              "      <td>1.385680</td>\n",
              "      <td>1.376355</td>\n",
              "      <td>1.357758</td>\n",
              "      <td>1.354854</td>\n",
              "      <td>1.345476</td>\n",
              "      <td>1.333810</td>\n",
              "      <td>1.332739</td>\n",
              "      <td>1.335550</td>\n",
              "      <td>1.326775</td>\n",
              "      <td>1.336862</td>\n",
              "      <td>1.316860</td>\n",
              "      <td>1.328051</td>\n",
              "      <td>1.328641</td>\n",
              "      <td>1.323526</td>\n",
              "      <td>1.314124</td>\n",
              "      <td>1.298936</td>\n",
              "      <td>1.289122</td>\n",
              "      <td>1.325059</td>\n",
              "      <td>1.271115</td>\n",
              "      <td>1.337119</td>\n",
              "      <td>1.289877</td>\n",
              "      <td>1.345229</td>\n",
              "      <td>45.77</td>\n",
              "      <td>24.80</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>ID_CK6RF8YV</td>\n",
              "      <td>0.513434</td>\n",
              "      <td>0.513303</td>\n",
              "      <td>0.522609</td>\n",
              "      <td>0.521068</td>\n",
              "      <td>0.523146</td>\n",
              "      <td>0.530132</td>\n",
              "      <td>0.539517</td>\n",
              "      <td>0.546364</td>\n",
              "      <td>0.552414</td>\n",
              "      <td>0.565502</td>\n",
              "      <td>0.581143</td>\n",
              "      <td>0.594354</td>\n",
              "      <td>0.599457</td>\n",
              "      <td>0.604529</td>\n",
              "      <td>0.605267</td>\n",
              "      <td>0.606276</td>\n",
              "      <td>0.604895</td>\n",
              "      <td>0.603716</td>\n",
              "      <td>0.600683</td>\n",
              "      <td>0.598087</td>\n",
              "      <td>0.594303</td>\n",
              "      <td>0.589403</td>\n",
              "      <td>0.585883</td>\n",
              "      <td>0.581369</td>\n",
              "      <td>0.578962</td>\n",
              "      <td>0.575181</td>\n",
              "      <td>0.573274</td>\n",
              "      <td>0.570471</td>\n",
              "      <td>0.568241</td>\n",
              "      <td>0.565671</td>\n",
              "      <td>0.564579</td>\n",
              "      <td>0.563724</td>\n",
              "      <td>0.561978</td>\n",
              "      <td>0.562744</td>\n",
              "      <td>0.563455</td>\n",
              "      <td>0.565163</td>\n",
              "      <td>0.566505</td>\n",
              "      <td>0.569239</td>\n",
              "      <td>0.572075</td>\n",
              "      <td>...</td>\n",
              "      <td>1.442957</td>\n",
              "      <td>1.423349</td>\n",
              "      <td>1.413718</td>\n",
              "      <td>1.403112</td>\n",
              "      <td>1.393964</td>\n",
              "      <td>1.375741</td>\n",
              "      <td>1.369549</td>\n",
              "      <td>1.354179</td>\n",
              "      <td>1.344562</td>\n",
              "      <td>1.333491</td>\n",
              "      <td>1.325002</td>\n",
              "      <td>1.321572</td>\n",
              "      <td>1.305561</td>\n",
              "      <td>1.292637</td>\n",
              "      <td>1.287971</td>\n",
              "      <td>1.283460</td>\n",
              "      <td>1.278300</td>\n",
              "      <td>1.268486</td>\n",
              "      <td>1.268407</td>\n",
              "      <td>1.263479</td>\n",
              "      <td>1.252612</td>\n",
              "      <td>1.254306</td>\n",
              "      <td>1.247635</td>\n",
              "      <td>1.242321</td>\n",
              "      <td>1.247859</td>\n",
              "      <td>1.246749</td>\n",
              "      <td>1.249920</td>\n",
              "      <td>1.265223</td>\n",
              "      <td>1.264013</td>\n",
              "      <td>1.285252</td>\n",
              "      <td>1.298422</td>\n",
              "      <td>1.299873</td>\n",
              "      <td>1.311157</td>\n",
              "      <td>1.303259</td>\n",
              "      <td>1.349833</td>\n",
              "      <td>45.84</td>\n",
              "      <td>36.93</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>ID_82N6QE6I</td>\n",
              "      <td>0.510485</td>\n",
              "      <td>0.519359</td>\n",
              "      <td>0.524225</td>\n",
              "      <td>0.528419</td>\n",
              "      <td>0.535273</td>\n",
              "      <td>0.545342</td>\n",
              "      <td>0.550314</td>\n",
              "      <td>0.557129</td>\n",
              "      <td>0.567030</td>\n",
              "      <td>0.577731</td>\n",
              "      <td>0.589192</td>\n",
              "      <td>0.604401</td>\n",
              "      <td>0.611372</td>\n",
              "      <td>0.614571</td>\n",
              "      <td>0.619713</td>\n",
              "      <td>0.619805</td>\n",
              "      <td>0.622708</td>\n",
              "      <td>0.620036</td>\n",
              "      <td>0.618070</td>\n",
              "      <td>0.616470</td>\n",
              "      <td>0.614592</td>\n",
              "      <td>0.611658</td>\n",
              "      <td>0.609762</td>\n",
              "      <td>0.608088</td>\n",
              "      <td>0.604118</td>\n",
              "      <td>0.602248</td>\n",
              "      <td>0.598901</td>\n",
              "      <td>0.598259</td>\n",
              "      <td>0.597334</td>\n",
              "      <td>0.594730</td>\n",
              "      <td>0.593618</td>\n",
              "      <td>0.593828</td>\n",
              "      <td>0.595201</td>\n",
              "      <td>0.596143</td>\n",
              "      <td>0.597089</td>\n",
              "      <td>0.599811</td>\n",
              "      <td>0.602078</td>\n",
              "      <td>0.607372</td>\n",
              "      <td>0.610382</td>\n",
              "      <td>...</td>\n",
              "      <td>1.433939</td>\n",
              "      <td>1.427050</td>\n",
              "      <td>1.423872</td>\n",
              "      <td>1.409867</td>\n",
              "      <td>1.405768</td>\n",
              "      <td>1.402808</td>\n",
              "      <td>1.388843</td>\n",
              "      <td>1.378993</td>\n",
              "      <td>1.370712</td>\n",
              "      <td>1.364764</td>\n",
              "      <td>1.352886</td>\n",
              "      <td>1.353532</td>\n",
              "      <td>1.346231</td>\n",
              "      <td>1.332744</td>\n",
              "      <td>1.326119</td>\n",
              "      <td>1.317568</td>\n",
              "      <td>1.318767</td>\n",
              "      <td>1.316029</td>\n",
              "      <td>1.298889</td>\n",
              "      <td>1.301750</td>\n",
              "      <td>1.288821</td>\n",
              "      <td>1.299768</td>\n",
              "      <td>1.294653</td>\n",
              "      <td>1.294362</td>\n",
              "      <td>1.294515</td>\n",
              "      <td>1.293155</td>\n",
              "      <td>1.294068</td>\n",
              "      <td>1.296753</td>\n",
              "      <td>1.272020</td>\n",
              "      <td>1.272303</td>\n",
              "      <td>1.272367</td>\n",
              "      <td>1.290032</td>\n",
              "      <td>1.339771</td>\n",
              "      <td>1.322738</td>\n",
              "      <td>1.348964</td>\n",
              "      <td>38.92</td>\n",
              "      <td>23.88</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    Reading_ID  ...  hdl_cholesterol_human_x0_ok\n",
              "0  ID_3SSHI56C  ...                          1.0\n",
              "1  ID_599OOLZA  ...                          1.0\n",
              "2  ID_MVJGPQ75  ...                          1.0\n",
              "3  ID_CK6RF8YV  ...                          0.0\n",
              "4  ID_82N6QE6I  ...                          1.0\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 145
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        },
        "id": "m8LtMEEq1E00",
        "outputId": "a214fea7-808a-42af-cd6b-db33f03fac30"
      },
      "source": [
        "hdl_col_df.describe()"
      ],
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>absorbance39</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>hdl_cholesterol_human_x0_high</th>\n",
              "      <th>hdl_cholesterol_human_x0_low</th>\n",
              "      <th>hdl_cholesterol_human_x0_ok</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "      <td>13140.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.507347</td>\n",
              "      <td>0.509800</td>\n",
              "      <td>0.513687</td>\n",
              "      <td>0.519391</td>\n",
              "      <td>0.525976</td>\n",
              "      <td>0.533482</td>\n",
              "      <td>0.539453</td>\n",
              "      <td>0.544755</td>\n",
              "      <td>0.552902</td>\n",
              "      <td>0.565279</td>\n",
              "      <td>0.578415</td>\n",
              "      <td>0.590215</td>\n",
              "      <td>0.597083</td>\n",
              "      <td>0.600618</td>\n",
              "      <td>0.602828</td>\n",
              "      <td>0.603434</td>\n",
              "      <td>0.602075</td>\n",
              "      <td>0.599335</td>\n",
              "      <td>0.596222</td>\n",
              "      <td>0.593066</td>\n",
              "      <td>0.589816</td>\n",
              "      <td>0.586821</td>\n",
              "      <td>0.583665</td>\n",
              "      <td>0.580336</td>\n",
              "      <td>0.577325</td>\n",
              "      <td>0.574321</td>\n",
              "      <td>0.571803</td>\n",
              "      <td>0.569906</td>\n",
              "      <td>0.568239</td>\n",
              "      <td>0.566951</td>\n",
              "      <td>0.566204</td>\n",
              "      <td>0.565929</td>\n",
              "      <td>0.566327</td>\n",
              "      <td>0.567358</td>\n",
              "      <td>0.568966</td>\n",
              "      <td>0.571237</td>\n",
              "      <td>0.574091</td>\n",
              "      <td>0.577933</td>\n",
              "      <td>0.581767</td>\n",
              "      <td>0.585947</td>\n",
              "      <td>...</td>\n",
              "      <td>1.494436</td>\n",
              "      <td>1.481857</td>\n",
              "      <td>1.470147</td>\n",
              "      <td>1.458943</td>\n",
              "      <td>1.447029</td>\n",
              "      <td>1.436010</td>\n",
              "      <td>1.424403</td>\n",
              "      <td>1.413153</td>\n",
              "      <td>1.402854</td>\n",
              "      <td>1.394788</td>\n",
              "      <td>1.382297</td>\n",
              "      <td>1.373392</td>\n",
              "      <td>1.364888</td>\n",
              "      <td>1.356420</td>\n",
              "      <td>1.348587</td>\n",
              "      <td>1.341277</td>\n",
              "      <td>1.333918</td>\n",
              "      <td>1.327140</td>\n",
              "      <td>1.320583</td>\n",
              "      <td>1.314208</td>\n",
              "      <td>1.309417</td>\n",
              "      <td>1.305020</td>\n",
              "      <td>1.301597</td>\n",
              "      <td>1.299224</td>\n",
              "      <td>1.298162</td>\n",
              "      <td>1.297279</td>\n",
              "      <td>1.296103</td>\n",
              "      <td>1.292939</td>\n",
              "      <td>1.287715</td>\n",
              "      <td>1.279190</td>\n",
              "      <td>1.271131</td>\n",
              "      <td>1.264782</td>\n",
              "      <td>1.262234</td>\n",
              "      <td>1.274563</td>\n",
              "      <td>1.261993</td>\n",
              "      <td>41.088709</td>\n",
              "      <td>31.534744</td>\n",
              "      <td>0.196347</td>\n",
              "      <td>0.232877</td>\n",
              "      <td>0.570776</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.042075</td>\n",
              "      <td>0.041587</td>\n",
              "      <td>0.041525</td>\n",
              "      <td>0.041676</td>\n",
              "      <td>0.041125</td>\n",
              "      <td>0.040644</td>\n",
              "      <td>0.040715</td>\n",
              "      <td>0.040946</td>\n",
              "      <td>0.040820</td>\n",
              "      <td>0.041038</td>\n",
              "      <td>0.041164</td>\n",
              "      <td>0.041268</td>\n",
              "      <td>0.041501</td>\n",
              "      <td>0.041560</td>\n",
              "      <td>0.041508</td>\n",
              "      <td>0.041379</td>\n",
              "      <td>0.041080</td>\n",
              "      <td>0.040716</td>\n",
              "      <td>0.040453</td>\n",
              "      <td>0.040368</td>\n",
              "      <td>0.040127</td>\n",
              "      <td>0.040101</td>\n",
              "      <td>0.040019</td>\n",
              "      <td>0.039927</td>\n",
              "      <td>0.039915</td>\n",
              "      <td>0.039767</td>\n",
              "      <td>0.039740</td>\n",
              "      <td>0.039792</td>\n",
              "      <td>0.039787</td>\n",
              "      <td>0.039715</td>\n",
              "      <td>0.039708</td>\n",
              "      <td>0.039738</td>\n",
              "      <td>0.039781</td>\n",
              "      <td>0.039860</td>\n",
              "      <td>0.039862</td>\n",
              "      <td>0.039979</td>\n",
              "      <td>0.040065</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.040384</td>\n",
              "      <td>0.040327</td>\n",
              "      <td>...</td>\n",
              "      <td>0.073972</td>\n",
              "      <td>0.072704</td>\n",
              "      <td>0.071497</td>\n",
              "      <td>0.070351</td>\n",
              "      <td>0.069132</td>\n",
              "      <td>0.068116</td>\n",
              "      <td>0.066951</td>\n",
              "      <td>0.066179</td>\n",
              "      <td>0.065450</td>\n",
              "      <td>0.065161</td>\n",
              "      <td>0.063811</td>\n",
              "      <td>0.063188</td>\n",
              "      <td>0.062849</td>\n",
              "      <td>0.062148</td>\n",
              "      <td>0.061847</td>\n",
              "      <td>0.061448</td>\n",
              "      <td>0.060891</td>\n",
              "      <td>0.060457</td>\n",
              "      <td>0.060267</td>\n",
              "      <td>0.059903</td>\n",
              "      <td>0.059883</td>\n",
              "      <td>0.059731</td>\n",
              "      <td>0.059607</td>\n",
              "      <td>0.059692</td>\n",
              "      <td>0.060137</td>\n",
              "      <td>0.060583</td>\n",
              "      <td>0.062033</td>\n",
              "      <td>0.064989</td>\n",
              "      <td>0.069659</td>\n",
              "      <td>0.077427</td>\n",
              "      <td>0.085519</td>\n",
              "      <td>0.092525</td>\n",
              "      <td>0.097079</td>\n",
              "      <td>0.103298</td>\n",
              "      <td>0.102908</td>\n",
              "      <td>2.964205</td>\n",
              "      <td>8.118994</td>\n",
              "      <td>0.397249</td>\n",
              "      <td>0.422680</td>\n",
              "      <td>0.494984</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.237409</td>\n",
              "      <td>0.251987</td>\n",
              "      <td>0.269550</td>\n",
              "      <td>0.284636</td>\n",
              "      <td>0.302883</td>\n",
              "      <td>0.318334</td>\n",
              "      <td>0.333950</td>\n",
              "      <td>0.345124</td>\n",
              "      <td>0.361352</td>\n",
              "      <td>0.374106</td>\n",
              "      <td>0.388672</td>\n",
              "      <td>0.407299</td>\n",
              "      <td>0.416705</td>\n",
              "      <td>0.420315</td>\n",
              "      <td>0.423078</td>\n",
              "      <td>0.423848</td>\n",
              "      <td>0.423839</td>\n",
              "      <td>0.422310</td>\n",
              "      <td>0.421942</td>\n",
              "      <td>0.419578</td>\n",
              "      <td>0.416689</td>\n",
              "      <td>0.414667</td>\n",
              "      <td>0.413794</td>\n",
              "      <td>0.410312</td>\n",
              "      <td>0.408713</td>\n",
              "      <td>0.405934</td>\n",
              "      <td>0.403518</td>\n",
              "      <td>0.402689</td>\n",
              "      <td>0.401398</td>\n",
              "      <td>0.400746</td>\n",
              "      <td>0.399525</td>\n",
              "      <td>0.399710</td>\n",
              "      <td>0.399237</td>\n",
              "      <td>0.400620</td>\n",
              "      <td>0.403256</td>\n",
              "      <td>0.404547</td>\n",
              "      <td>0.407792</td>\n",
              "      <td>0.412233</td>\n",
              "      <td>0.416269</td>\n",
              "      <td>0.421089</td>\n",
              "      <td>...</td>\n",
              "      <td>1.258771</td>\n",
              "      <td>1.252074</td>\n",
              "      <td>1.243098</td>\n",
              "      <td>1.234149</td>\n",
              "      <td>1.228081</td>\n",
              "      <td>1.216854</td>\n",
              "      <td>1.205064</td>\n",
              "      <td>1.198276</td>\n",
              "      <td>1.192913</td>\n",
              "      <td>1.187249</td>\n",
              "      <td>1.173993</td>\n",
              "      <td>1.167197</td>\n",
              "      <td>1.167758</td>\n",
              "      <td>1.159292</td>\n",
              "      <td>1.150155</td>\n",
              "      <td>1.147119</td>\n",
              "      <td>1.142088</td>\n",
              "      <td>1.134481</td>\n",
              "      <td>1.133834</td>\n",
              "      <td>1.124536</td>\n",
              "      <td>1.121238</td>\n",
              "      <td>1.119583</td>\n",
              "      <td>1.117969</td>\n",
              "      <td>1.110535</td>\n",
              "      <td>1.109933</td>\n",
              "      <td>1.101823</td>\n",
              "      <td>1.099943</td>\n",
              "      <td>1.095809</td>\n",
              "      <td>1.074439</td>\n",
              "      <td>1.037112</td>\n",
              "      <td>0.993182</td>\n",
              "      <td>0.987268</td>\n",
              "      <td>0.963837</td>\n",
              "      <td>0.965659</td>\n",
              "      <td>0.951658</td>\n",
              "      <td>30.480000</td>\n",
              "      <td>16.610000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.485665</td>\n",
              "      <td>0.487773</td>\n",
              "      <td>0.491302</td>\n",
              "      <td>0.495686</td>\n",
              "      <td>0.501565</td>\n",
              "      <td>0.508257</td>\n",
              "      <td>0.514780</td>\n",
              "      <td>0.520312</td>\n",
              "      <td>0.528137</td>\n",
              "      <td>0.539653</td>\n",
              "      <td>0.552805</td>\n",
              "      <td>0.564319</td>\n",
              "      <td>0.571027</td>\n",
              "      <td>0.574653</td>\n",
              "      <td>0.576596</td>\n",
              "      <td>0.577122</td>\n",
              "      <td>0.576443</td>\n",
              "      <td>0.574268</td>\n",
              "      <td>0.571647</td>\n",
              "      <td>0.568491</td>\n",
              "      <td>0.565811</td>\n",
              "      <td>0.562905</td>\n",
              "      <td>0.559912</td>\n",
              "      <td>0.556444</td>\n",
              "      <td>0.553277</td>\n",
              "      <td>0.550284</td>\n",
              "      <td>0.547889</td>\n",
              "      <td>0.545969</td>\n",
              "      <td>0.544347</td>\n",
              "      <td>0.543138</td>\n",
              "      <td>0.542308</td>\n",
              "      <td>0.542172</td>\n",
              "      <td>0.542332</td>\n",
              "      <td>0.543631</td>\n",
              "      <td>0.545069</td>\n",
              "      <td>0.547261</td>\n",
              "      <td>0.550155</td>\n",
              "      <td>0.553716</td>\n",
              "      <td>0.557709</td>\n",
              "      <td>0.562101</td>\n",
              "      <td>...</td>\n",
              "      <td>1.450067</td>\n",
              "      <td>1.437672</td>\n",
              "      <td>1.426027</td>\n",
              "      <td>1.414819</td>\n",
              "      <td>1.404095</td>\n",
              "      <td>1.393394</td>\n",
              "      <td>1.382096</td>\n",
              "      <td>1.371558</td>\n",
              "      <td>1.362166</td>\n",
              "      <td>1.355040</td>\n",
              "      <td>1.343697</td>\n",
              "      <td>1.335040</td>\n",
              "      <td>1.327162</td>\n",
              "      <td>1.319342</td>\n",
              "      <td>1.311546</td>\n",
              "      <td>1.304842</td>\n",
              "      <td>1.297528</td>\n",
              "      <td>1.291124</td>\n",
              "      <td>1.284462</td>\n",
              "      <td>1.277729</td>\n",
              "      <td>1.273355</td>\n",
              "      <td>1.268584</td>\n",
              "      <td>1.264794</td>\n",
              "      <td>1.262168</td>\n",
              "      <td>1.260923</td>\n",
              "      <td>1.259908</td>\n",
              "      <td>1.258522</td>\n",
              "      <td>1.254109</td>\n",
              "      <td>1.244606</td>\n",
              "      <td>1.229652</td>\n",
              "      <td>1.217083</td>\n",
              "      <td>1.205337</td>\n",
              "      <td>1.198471</td>\n",
              "      <td>1.206278</td>\n",
              "      <td>1.194331</td>\n",
              "      <td>39.030000</td>\n",
              "      <td>25.057500</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.510919</td>\n",
              "      <td>0.513115</td>\n",
              "      <td>0.517337</td>\n",
              "      <td>0.523236</td>\n",
              "      <td>0.529703</td>\n",
              "      <td>0.536550</td>\n",
              "      <td>0.542027</td>\n",
              "      <td>0.547747</td>\n",
              "      <td>0.555886</td>\n",
              "      <td>0.569253</td>\n",
              "      <td>0.582528</td>\n",
              "      <td>0.594201</td>\n",
              "      <td>0.600954</td>\n",
              "      <td>0.604466</td>\n",
              "      <td>0.607168</td>\n",
              "      <td>0.608071</td>\n",
              "      <td>0.605993</td>\n",
              "      <td>0.602274</td>\n",
              "      <td>0.598185</td>\n",
              "      <td>0.594525</td>\n",
              "      <td>0.590582</td>\n",
              "      <td>0.587258</td>\n",
              "      <td>0.583973</td>\n",
              "      <td>0.580665</td>\n",
              "      <td>0.577652</td>\n",
              "      <td>0.574572</td>\n",
              "      <td>0.571976</td>\n",
              "      <td>0.570197</td>\n",
              "      <td>0.568417</td>\n",
              "      <td>0.567208</td>\n",
              "      <td>0.566290</td>\n",
              "      <td>0.566065</td>\n",
              "      <td>0.566533</td>\n",
              "      <td>0.567509</td>\n",
              "      <td>0.569262</td>\n",
              "      <td>0.571613</td>\n",
              "      <td>0.574563</td>\n",
              "      <td>0.578694</td>\n",
              "      <td>0.582621</td>\n",
              "      <td>0.586750</td>\n",
              "      <td>...</td>\n",
              "      <td>1.494271</td>\n",
              "      <td>1.481875</td>\n",
              "      <td>1.470441</td>\n",
              "      <td>1.459452</td>\n",
              "      <td>1.447781</td>\n",
              "      <td>1.436981</td>\n",
              "      <td>1.425552</td>\n",
              "      <td>1.414226</td>\n",
              "      <td>1.403628</td>\n",
              "      <td>1.395818</td>\n",
              "      <td>1.383005</td>\n",
              "      <td>1.374082</td>\n",
              "      <td>1.365277</td>\n",
              "      <td>1.357030</td>\n",
              "      <td>1.349200</td>\n",
              "      <td>1.341693</td>\n",
              "      <td>1.334284</td>\n",
              "      <td>1.327494</td>\n",
              "      <td>1.320617</td>\n",
              "      <td>1.314573</td>\n",
              "      <td>1.309585</td>\n",
              "      <td>1.305061</td>\n",
              "      <td>1.301706</td>\n",
              "      <td>1.299526</td>\n",
              "      <td>1.298195</td>\n",
              "      <td>1.297335</td>\n",
              "      <td>1.296018</td>\n",
              "      <td>1.292631</td>\n",
              "      <td>1.285629</td>\n",
              "      <td>1.277651</td>\n",
              "      <td>1.270125</td>\n",
              "      <td>1.263422</td>\n",
              "      <td>1.260902</td>\n",
              "      <td>1.270895</td>\n",
              "      <td>1.257434</td>\n",
              "      <td>41.090000</td>\n",
              "      <td>30.075000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.528750</td>\n",
              "      <td>0.531115</td>\n",
              "      <td>0.535573</td>\n",
              "      <td>0.541501</td>\n",
              "      <td>0.547925</td>\n",
              "      <td>0.554984</td>\n",
              "      <td>0.561157</td>\n",
              "      <td>0.566591</td>\n",
              "      <td>0.575060</td>\n",
              "      <td>0.587514</td>\n",
              "      <td>0.600904</td>\n",
              "      <td>0.613614</td>\n",
              "      <td>0.620183</td>\n",
              "      <td>0.623810</td>\n",
              "      <td>0.625986</td>\n",
              "      <td>0.626439</td>\n",
              "      <td>0.625180</td>\n",
              "      <td>0.622114</td>\n",
              "      <td>0.618871</td>\n",
              "      <td>0.615843</td>\n",
              "      <td>0.612567</td>\n",
              "      <td>0.609685</td>\n",
              "      <td>0.606625</td>\n",
              "      <td>0.603047</td>\n",
              "      <td>0.599838</td>\n",
              "      <td>0.596773</td>\n",
              "      <td>0.594010</td>\n",
              "      <td>0.592153</td>\n",
              "      <td>0.590351</td>\n",
              "      <td>0.588743</td>\n",
              "      <td>0.587934</td>\n",
              "      <td>0.587687</td>\n",
              "      <td>0.588151</td>\n",
              "      <td>0.589015</td>\n",
              "      <td>0.590629</td>\n",
              "      <td>0.592974</td>\n",
              "      <td>0.595974</td>\n",
              "      <td>0.600088</td>\n",
              "      <td>0.603819</td>\n",
              "      <td>0.607969</td>\n",
              "      <td>...</td>\n",
              "      <td>1.543802</td>\n",
              "      <td>1.529735</td>\n",
              "      <td>1.516349</td>\n",
              "      <td>1.504460</td>\n",
              "      <td>1.491267</td>\n",
              "      <td>1.478949</td>\n",
              "      <td>1.467257</td>\n",
              "      <td>1.454914</td>\n",
              "      <td>1.444054</td>\n",
              "      <td>1.435333</td>\n",
              "      <td>1.421924</td>\n",
              "      <td>1.412746</td>\n",
              "      <td>1.404159</td>\n",
              "      <td>1.395637</td>\n",
              "      <td>1.387896</td>\n",
              "      <td>1.380348</td>\n",
              "      <td>1.372971</td>\n",
              "      <td>1.366158</td>\n",
              "      <td>1.359384</td>\n",
              "      <td>1.353154</td>\n",
              "      <td>1.348107</td>\n",
              "      <td>1.344020</td>\n",
              "      <td>1.340308</td>\n",
              "      <td>1.337843</td>\n",
              "      <td>1.336640</td>\n",
              "      <td>1.335376</td>\n",
              "      <td>1.333793</td>\n",
              "      <td>1.330167</td>\n",
              "      <td>1.328191</td>\n",
              "      <td>1.326197</td>\n",
              "      <td>1.324954</td>\n",
              "      <td>1.322994</td>\n",
              "      <td>1.322975</td>\n",
              "      <td>1.337093</td>\n",
              "      <td>1.325678</td>\n",
              "      <td>42.870000</td>\n",
              "      <td>36.700000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.657782</td>\n",
              "      <td>0.652291</td>\n",
              "      <td>0.659153</td>\n",
              "      <td>0.658982</td>\n",
              "      <td>0.659410</td>\n",
              "      <td>0.667296</td>\n",
              "      <td>0.672708</td>\n",
              "      <td>0.680601</td>\n",
              "      <td>0.691116</td>\n",
              "      <td>0.704255</td>\n",
              "      <td>0.719512</td>\n",
              "      <td>0.732278</td>\n",
              "      <td>0.743683</td>\n",
              "      <td>0.747690</td>\n",
              "      <td>0.749260</td>\n",
              "      <td>0.749274</td>\n",
              "      <td>0.746646</td>\n",
              "      <td>0.745059</td>\n",
              "      <td>0.743047</td>\n",
              "      <td>0.739153</td>\n",
              "      <td>0.737969</td>\n",
              "      <td>0.734946</td>\n",
              "      <td>0.731756</td>\n",
              "      <td>0.727761</td>\n",
              "      <td>0.727038</td>\n",
              "      <td>0.722906</td>\n",
              "      <td>0.719451</td>\n",
              "      <td>0.717684</td>\n",
              "      <td>0.716110</td>\n",
              "      <td>0.713546</td>\n",
              "      <td>0.714271</td>\n",
              "      <td>0.713999</td>\n",
              "      <td>0.714599</td>\n",
              "      <td>0.716650</td>\n",
              "      <td>0.716280</td>\n",
              "      <td>0.719780</td>\n",
              "      <td>0.722950</td>\n",
              "      <td>0.726953</td>\n",
              "      <td>0.731097</td>\n",
              "      <td>0.734740</td>\n",
              "      <td>...</td>\n",
              "      <td>1.740056</td>\n",
              "      <td>1.720852</td>\n",
              "      <td>1.707253</td>\n",
              "      <td>1.688177</td>\n",
              "      <td>1.674865</td>\n",
              "      <td>1.657678</td>\n",
              "      <td>1.642662</td>\n",
              "      <td>1.631072</td>\n",
              "      <td>1.618494</td>\n",
              "      <td>1.604445</td>\n",
              "      <td>1.592295</td>\n",
              "      <td>1.576147</td>\n",
              "      <td>1.559133</td>\n",
              "      <td>1.549010</td>\n",
              "      <td>1.535621</td>\n",
              "      <td>1.528946</td>\n",
              "      <td>1.521588</td>\n",
              "      <td>1.507782</td>\n",
              "      <td>1.502026</td>\n",
              "      <td>1.496057</td>\n",
              "      <td>1.483685</td>\n",
              "      <td>1.480177</td>\n",
              "      <td>1.486023</td>\n",
              "      <td>1.474680</td>\n",
              "      <td>1.483881</td>\n",
              "      <td>1.483797</td>\n",
              "      <td>1.479876</td>\n",
              "      <td>1.483365</td>\n",
              "      <td>1.491504</td>\n",
              "      <td>1.504519</td>\n",
              "      <td>1.527945</td>\n",
              "      <td>1.539556</td>\n",
              "      <td>1.584125</td>\n",
              "      <td>1.672314</td>\n",
              "      <td>1.653452</td>\n",
              "      <td>52.690000</td>\n",
              "      <td>63.070000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 175 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        absorbance0  ...  hdl_cholesterol_human_x0_ok\n",
              "count  13140.000000  ...                 13140.000000\n",
              "mean       0.507347  ...                     0.570776\n",
              "std        0.042075  ...                     0.494984\n",
              "min        0.237409  ...                     0.000000\n",
              "25%        0.485665  ...                     0.000000\n",
              "50%        0.510919  ...                     1.000000\n",
              "75%        0.528750  ...                     1.000000\n",
              "max        0.657782  ...                     1.000000\n",
              "\n",
              "[8 rows x 175 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4aODcXFHlKby"
      },
      "source": [
        ""
      ],
      "execution_count": 146,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGjsJ0Cq3yhL"
      },
      "source": [
        "minmax=MinMaxScaler()\n",
        "minmax.fit(hdl_col_df.iloc[:,1:])\n",
        "data_transform=minmax.transform(hdl_col_df.iloc[:,1:])"
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttW3fbYt56F7"
      },
      "source": [
        ""
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IU8nkTaP6OGa"
      },
      "source": [
        ""
      ],
      "execution_count": 147,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3F1iU_DK6weT"
      },
      "source": [
        "X=data_transform[:,:-3]\n",
        "y_hdl_col=hdl_col_df.iloc[:,-3:]\n",
        "y_hemo=hemo_df.iloc[:,-3:]\n",
        "y_hemo=hemo_df.iloc[:,-3:]\n",
        "y_col=col_df.iloc[:,-3:]"
      ],
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DY-BfoN-7lwC"
      },
      "source": [
        "import tensorflow as tf\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout,Embedding, Flatten,LayerNormalization\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "import keras.optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld,mse\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": 149,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "luXTUyYQ9tQU"
      },
      "source": [
        "def blood_model():\n",
        "    #clear_session()\n",
        "\n",
        "\n",
        "\n",
        "    input_len=len(X[0])\n",
        "\n",
        "    print(input_len)\n",
        "    output_size=3\n",
        "    drop_frac0=0.0 \n",
        "    drop_frac1=0.0\n",
        "\n",
        "\n",
        "\n",
        "    input1=Input(shape=(input_len,),)\n",
        "\n",
        "    #flatt=Flatten()(lstm1)\n",
        "\n",
        "    non=42\n",
        "    #initializer = tf.keras.initializers.LecunNormal()\n",
        "    #initializer=tf.keras.initializers.LecunUniform()\n",
        "    #initializer=tf.keras.initializers.HeUniform(    seed=None)\n",
        "    #initializer= tf.keras.initializers.RandomNormal(    mean=3.0, stddev=0.05, seed=None)\n",
        "\n",
        "    initializer=\"normal\"\n",
        "    d1=Dense(193,activation=\"sigmoid\",kernel_initializer=initializer,input_dim=input_len)(input1)\n",
        "    d1=Dropout(drop_frac0)(d1)\n",
        "\n",
        "\n",
        "\n",
        "    pred=Dense(output_size,activation=\"softmax\")(d1)\n",
        "\n",
        "    model = Model(inputs=input1, outputs=pred)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adamax(learning_rate=0.01)\n",
        "\n",
        "\n",
        "    lossfn = tf.keras.metrics.CategoricalCrossentropy()\n",
        "\n",
        "    model.compile(loss=\"CategoricalCrossentropy\",\n",
        "        optimizer=opt,\n",
        "        metrics=[lossfn])\n",
        "    return(model)"
      ],
      "execution_count": 150,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QRCPMXoZf3Zv"
      },
      "source": [
        "!rm *.hdf5"
      ],
      "execution_count": 151,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aOiBdSGKU1nm"
      },
      "source": [
        "model_name=\"hdl_col\"\n",
        "def scheduler(epoch, lr):\n",
        "    return 0.01\n",
        "\n",
        "callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks = [callback_LR,\n",
        "        \n",
        "        #savemodela,\n",
        "        ModelCheckpoint(filepath=model_name+\"_{val_categorical_crossentropy:.5f}__.hdf5\", monitor='val_categorical_crossentropy',\n",
        "                        verbose=2, save_best_only=True, mode='min')]"
      ],
      "execution_count": 152,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hlHiSGPt6EB4",
        "outputId": "42671f2d-914b-4482-c1b7-0a78b8deead5"
      },
      "source": [
        "hdl_model=blood_model()"
      ],
      "execution_count": 153,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiO4COmH966N",
        "outputId": "218507f5-fee5-4c75-cffc-0403d98635d5"
      },
      "source": [
        "hdl_model.fit(X,\n",
        "          y_hdl_col,\n",
        "          epochs=100, \n",
        "          batch_size=3,\n",
        "          validation_split=0.1,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          shuffle=False\n",
        "          \n",
        "          )"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.9875 - categorical_crossentropy: 0.9875\n",
            "Epoch 00001: val_categorical_crossentropy improved from inf to 0.96652, saving model to hdl_col_0.96652__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.9876 - categorical_crossentropy: 0.9876 - val_loss: 0.9665 - val_categorical_crossentropy: 0.9665 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.9352 - categorical_crossentropy: 0.9352\n",
            "Epoch 00002: val_categorical_crossentropy improved from 0.96652 to 0.92782, saving model to hdl_col_0.92782__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.9351 - categorical_crossentropy: 0.9351 - val_loss: 0.9278 - val_categorical_crossentropy: 0.9278 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.8957 - categorical_crossentropy: 0.8957\n",
            "Epoch 00003: val_categorical_crossentropy improved from 0.92782 to 0.89522, saving model to hdl_col_0.89522__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.8958 - categorical_crossentropy: 0.8958 - val_loss: 0.8952 - val_categorical_crossentropy: 0.8952 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.8610 - categorical_crossentropy: 0.8610\n",
            "Epoch 00004: val_categorical_crossentropy improved from 0.89522 to 0.86265, saving model to hdl_col_0.86265__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.8616 - categorical_crossentropy: 0.8616 - val_loss: 0.8627 - val_categorical_crossentropy: 0.8627 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.8318 - categorical_crossentropy: 0.8318\n",
            "Epoch 00005: val_categorical_crossentropy improved from 0.86265 to 0.83178, saving model to hdl_col_0.83178__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.8319 - categorical_crossentropy: 0.8319 - val_loss: 0.8318 - val_categorical_crossentropy: 0.8318 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.8059 - categorical_crossentropy: 0.8059\n",
            "Epoch 00006: val_categorical_crossentropy improved from 0.83178 to 0.80668, saving model to hdl_col_0.80668__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.8062 - categorical_crossentropy: 0.8062 - val_loss: 0.8067 - val_categorical_crossentropy: 0.8067 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.7830 - categorical_crossentropy: 0.7830\n",
            "Epoch 00007: val_categorical_crossentropy improved from 0.80668 to 0.78797, saving model to hdl_col_0.78797__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7834 - categorical_crossentropy: 0.7834 - val_loss: 0.7880 - val_categorical_crossentropy: 0.7880 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.7616 - categorical_crossentropy: 0.7616\n",
            "Epoch 00008: val_categorical_crossentropy improved from 0.78797 to 0.77162, saving model to hdl_col_0.77162__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7619 - categorical_crossentropy: 0.7619 - val_loss: 0.7716 - val_categorical_crossentropy: 0.7716 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "3911/3942 [============================>.] - ETA: 0s - loss: 0.7420 - categorical_crossentropy: 0.7420\n",
            "Epoch 00009: val_categorical_crossentropy improved from 0.77162 to 0.75613, saving model to hdl_col_0.75613__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7426 - categorical_crossentropy: 0.7426 - val_loss: 0.7561 - val_categorical_crossentropy: 0.7561 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.7240 - categorical_crossentropy: 0.7240\n",
            "Epoch 00010: val_categorical_crossentropy improved from 0.75613 to 0.73885, saving model to hdl_col_0.73885__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7242 - categorical_crossentropy: 0.7242 - val_loss: 0.7389 - val_categorical_crossentropy: 0.7389 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.7060 - categorical_crossentropy: 0.7060\n",
            "Epoch 00011: val_categorical_crossentropy improved from 0.73885 to 0.72258, saving model to hdl_col_0.72258__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7061 - categorical_crossentropy: 0.7061 - val_loss: 0.7226 - val_categorical_crossentropy: 0.7226 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.6888 - categorical_crossentropy: 0.6888\n",
            "Epoch 00012: val_categorical_crossentropy improved from 0.72258 to 0.70686, saving model to hdl_col_0.70686__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6888 - categorical_crossentropy: 0.6888 - val_loss: 0.7069 - val_categorical_crossentropy: 0.7069 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.6723 - categorical_crossentropy: 0.6723\n",
            "Epoch 00013: val_categorical_crossentropy improved from 0.70686 to 0.69143, saving model to hdl_col_0.69143__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6726 - categorical_crossentropy: 0.6726 - val_loss: 0.6914 - val_categorical_crossentropy: 0.6914 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.6568 - categorical_crossentropy: 0.6568\n",
            "Epoch 00014: val_categorical_crossentropy improved from 0.69143 to 0.67578, saving model to hdl_col_0.67578__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6569 - categorical_crossentropy: 0.6569 - val_loss: 0.6758 - val_categorical_crossentropy: 0.6758 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.6419 - categorical_crossentropy: 0.6419\n",
            "Epoch 00015: val_categorical_crossentropy improved from 0.67578 to 0.66093, saving model to hdl_col_0.66093__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6421 - categorical_crossentropy: 0.6421 - val_loss: 0.6609 - val_categorical_crossentropy: 0.6609 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.6277 - categorical_crossentropy: 0.6277\n",
            "Epoch 00016: val_categorical_crossentropy improved from 0.66093 to 0.64838, saving model to hdl_col_0.64838__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.6279 - categorical_crossentropy: 0.6279 - val_loss: 0.6484 - val_categorical_crossentropy: 0.6484 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.6141 - categorical_crossentropy: 0.6141\n",
            "Epoch 00017: val_categorical_crossentropy improved from 0.64838 to 0.63808, saving model to hdl_col_0.63808__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6141 - categorical_crossentropy: 0.6141 - val_loss: 0.6381 - val_categorical_crossentropy: 0.6381 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.6009 - categorical_crossentropy: 0.6009\n",
            "Epoch 00018: val_categorical_crossentropy improved from 0.63808 to 0.62882, saving model to hdl_col_0.62882__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6010 - categorical_crossentropy: 0.6010 - val_loss: 0.6288 - val_categorical_crossentropy: 0.6288 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.5887 - categorical_crossentropy: 0.5887\n",
            "Epoch 00019: val_categorical_crossentropy improved from 0.62882 to 0.61606, saving model to hdl_col_0.61606__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5887 - categorical_crossentropy: 0.5887 - val_loss: 0.6161 - val_categorical_crossentropy: 0.6161 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.5769 - categorical_crossentropy: 0.5769\n",
            "Epoch 00020: val_categorical_crossentropy improved from 0.61606 to 0.60427, saving model to hdl_col_0.60427__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5771 - categorical_crossentropy: 0.5771 - val_loss: 0.6043 - val_categorical_crossentropy: 0.6043 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.5658 - categorical_crossentropy: 0.5658\n",
            "Epoch 00021: val_categorical_crossentropy improved from 0.60427 to 0.59288, saving model to hdl_col_0.59288__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5660 - categorical_crossentropy: 0.5660 - val_loss: 0.5929 - val_categorical_crossentropy: 0.5929 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.5550 - categorical_crossentropy: 0.5550\n",
            "Epoch 00022: val_categorical_crossentropy improved from 0.59288 to 0.58227, saving model to hdl_col_0.58227__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5550 - categorical_crossentropy: 0.5550 - val_loss: 0.5823 - val_categorical_crossentropy: 0.5823 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.5441 - categorical_crossentropy: 0.5441\n",
            "Epoch 00023: val_categorical_crossentropy improved from 0.58227 to 0.56966, saving model to hdl_col_0.56966__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5441 - categorical_crossentropy: 0.5441 - val_loss: 0.5697 - val_categorical_crossentropy: 0.5697 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.5336 - categorical_crossentropy: 0.5336\n",
            "Epoch 00024: val_categorical_crossentropy improved from 0.56966 to 0.55661, saving model to hdl_col_0.55661__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5336 - categorical_crossentropy: 0.5336 - val_loss: 0.5566 - val_categorical_crossentropy: 0.5566 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.5232 - categorical_crossentropy: 0.5232\n",
            "Epoch 00025: val_categorical_crossentropy improved from 0.55661 to 0.54395, saving model to hdl_col_0.54395__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5232 - categorical_crossentropy: 0.5232 - val_loss: 0.5440 - val_categorical_crossentropy: 0.5440 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.5130 - categorical_crossentropy: 0.5130\n",
            "Epoch 00026: val_categorical_crossentropy improved from 0.54395 to 0.53240, saving model to hdl_col_0.53240__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5129 - categorical_crossentropy: 0.5129 - val_loss: 0.5324 - val_categorical_crossentropy: 0.5324 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.5032 - categorical_crossentropy: 0.5032\n",
            "Epoch 00027: val_categorical_crossentropy improved from 0.53240 to 0.52123, saving model to hdl_col_0.52123__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5031 - categorical_crossentropy: 0.5031 - val_loss: 0.5212 - val_categorical_crossentropy: 0.5212 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.4939 - categorical_crossentropy: 0.4939\n",
            "Epoch 00028: val_categorical_crossentropy improved from 0.52123 to 0.51171, saving model to hdl_col_0.51171__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4937 - categorical_crossentropy: 0.4937 - val_loss: 0.5117 - val_categorical_crossentropy: 0.5117 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "3916/3942 [============================>.] - ETA: 0s - loss: 0.4846 - categorical_crossentropy: 0.4846\n",
            "Epoch 00029: val_categorical_crossentropy improved from 0.51171 to 0.50275, saving model to hdl_col_0.50275__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4847 - categorical_crossentropy: 0.4847 - val_loss: 0.5027 - val_categorical_crossentropy: 0.5027 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.4760 - categorical_crossentropy: 0.4760\n",
            "Epoch 00030: val_categorical_crossentropy improved from 0.50275 to 0.49351, saving model to hdl_col_0.49351__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4759 - categorical_crossentropy: 0.4759 - val_loss: 0.4935 - val_categorical_crossentropy: 0.4935 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.4674 - categorical_crossentropy: 0.4674\n",
            "Epoch 00031: val_categorical_crossentropy improved from 0.49351 to 0.48410, saving model to hdl_col_0.48410__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4673 - categorical_crossentropy: 0.4673 - val_loss: 0.4841 - val_categorical_crossentropy: 0.4841 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.4588 - categorical_crossentropy: 0.4588\n",
            "Epoch 00032: val_categorical_crossentropy improved from 0.48410 to 0.47560, saving model to hdl_col_0.47560__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4588 - categorical_crossentropy: 0.4588 - val_loss: 0.4756 - val_categorical_crossentropy: 0.4756 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.4499 - categorical_crossentropy: 0.4499\n",
            "Epoch 00033: val_categorical_crossentropy improved from 0.47560 to 0.46876, saving model to hdl_col_0.46876__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4502 - categorical_crossentropy: 0.4502 - val_loss: 0.4688 - val_categorical_crossentropy: 0.4688 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.4413 - categorical_crossentropy: 0.4413\n",
            "Epoch 00034: val_categorical_crossentropy improved from 0.46876 to 0.46302, saving model to hdl_col_0.46302__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4415 - categorical_crossentropy: 0.4415 - val_loss: 0.4630 - val_categorical_crossentropy: 0.4630 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.4328 - categorical_crossentropy: 0.4328\n",
            "Epoch 00035: val_categorical_crossentropy improved from 0.46302 to 0.45830, saving model to hdl_col_0.45830__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4327 - categorical_crossentropy: 0.4327 - val_loss: 0.4583 - val_categorical_crossentropy: 0.4583 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.4240 - categorical_crossentropy: 0.4240\n",
            "Epoch 00036: val_categorical_crossentropy improved from 0.45830 to 0.45529, saving model to hdl_col_0.45529__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4239 - categorical_crossentropy: 0.4239 - val_loss: 0.4553 - val_categorical_crossentropy: 0.4553 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.4154 - categorical_crossentropy: 0.4154\n",
            "Epoch 00037: val_categorical_crossentropy improved from 0.45529 to 0.45344, saving model to hdl_col_0.45344__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4153 - categorical_crossentropy: 0.4153 - val_loss: 0.4534 - val_categorical_crossentropy: 0.4534 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.4069 - categorical_crossentropy: 0.4069\n",
            "Epoch 00038: val_categorical_crossentropy improved from 0.45344 to 0.45272, saving model to hdl_col_0.45272__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4070 - categorical_crossentropy: 0.4070 - val_loss: 0.4527 - val_categorical_crossentropy: 0.4527 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.3989 - categorical_crossentropy: 0.3989\n",
            "Epoch 00039: val_categorical_crossentropy improved from 0.45272 to 0.45067, saving model to hdl_col_0.45067__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3988 - categorical_crossentropy: 0.3988 - val_loss: 0.4507 - val_categorical_crossentropy: 0.4507 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.3907 - categorical_crossentropy: 0.3907\n",
            "Epoch 00040: val_categorical_crossentropy improved from 0.45067 to 0.44551, saving model to hdl_col_0.44551__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3906 - categorical_crossentropy: 0.3906 - val_loss: 0.4455 - val_categorical_crossentropy: 0.4455 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.3826 - categorical_crossentropy: 0.3826\n",
            "Epoch 00041: val_categorical_crossentropy improved from 0.44551 to 0.43835, saving model to hdl_col_0.43835__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3825 - categorical_crossentropy: 0.3825 - val_loss: 0.4384 - val_categorical_crossentropy: 0.4384 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.3744 - categorical_crossentropy: 0.3744\n",
            "Epoch 00042: val_categorical_crossentropy improved from 0.43835 to 0.43085, saving model to hdl_col_0.43085__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3744 - categorical_crossentropy: 0.3744 - val_loss: 0.4308 - val_categorical_crossentropy: 0.4308 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.3664 - categorical_crossentropy: 0.3664\n",
            "Epoch 00043: val_categorical_crossentropy improved from 0.43085 to 0.42556, saving model to hdl_col_0.42556__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3665 - categorical_crossentropy: 0.3665 - val_loss: 0.4256 - val_categorical_crossentropy: 0.4256 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.3586 - categorical_crossentropy: 0.3586\n",
            "Epoch 00044: val_categorical_crossentropy improved from 0.42556 to 0.42121, saving model to hdl_col_0.42121__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3588 - categorical_crossentropy: 0.3588 - val_loss: 0.4212 - val_categorical_crossentropy: 0.4212 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.3514 - categorical_crossentropy: 0.3514\n",
            "Epoch 00045: val_categorical_crossentropy improved from 0.42121 to 0.41756, saving model to hdl_col_0.41756__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3516 - categorical_crossentropy: 0.3516 - val_loss: 0.4176 - val_categorical_crossentropy: 0.4176 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.3448 - categorical_crossentropy: 0.3448\n",
            "Epoch 00046: val_categorical_crossentropy improved from 0.41756 to 0.41369, saving model to hdl_col_0.41369__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3447 - categorical_crossentropy: 0.3447 - val_loss: 0.4137 - val_categorical_crossentropy: 0.4137 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.3378 - categorical_crossentropy: 0.3378\n",
            "Epoch 00047: val_categorical_crossentropy improved from 0.41369 to 0.41169, saving model to hdl_col_0.41169__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3379 - categorical_crossentropy: 0.3379 - val_loss: 0.4117 - val_categorical_crossentropy: 0.4117 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.3312 - categorical_crossentropy: 0.3312\n",
            "Epoch 00048: val_categorical_crossentropy improved from 0.41169 to 0.40652, saving model to hdl_col_0.40652__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3314 - categorical_crossentropy: 0.3314 - val_loss: 0.4065 - val_categorical_crossentropy: 0.4065 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.3249 - categorical_crossentropy: 0.3249\n",
            "Epoch 00049: val_categorical_crossentropy improved from 0.40652 to 0.39728, saving model to hdl_col_0.39728__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3248 - categorical_crossentropy: 0.3248 - val_loss: 0.3973 - val_categorical_crossentropy: 0.3973 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.3186 - categorical_crossentropy: 0.3186\n",
            "Epoch 00050: val_categorical_crossentropy improved from 0.39728 to 0.39106, saving model to hdl_col_0.39106__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3185 - categorical_crossentropy: 0.3185 - val_loss: 0.3911 - val_categorical_crossentropy: 0.3911 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.3123 - categorical_crossentropy: 0.3123\n",
            "Epoch 00051: val_categorical_crossentropy improved from 0.39106 to 0.38338, saving model to hdl_col_0.38338__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3125 - categorical_crossentropy: 0.3125 - val_loss: 0.3834 - val_categorical_crossentropy: 0.3834 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.3069 - categorical_crossentropy: 0.3069\n",
            "Epoch 00052: val_categorical_crossentropy did not improve from 0.38338\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3069 - categorical_crossentropy: 0.3069 - val_loss: 0.3882 - val_categorical_crossentropy: 0.3882 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.3015 - categorical_crossentropy: 0.3015\n",
            "Epoch 00053: val_categorical_crossentropy improved from 0.38338 to 0.37652, saving model to hdl_col_0.37652__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3014 - categorical_crossentropy: 0.3014 - val_loss: 0.3765 - val_categorical_crossentropy: 0.3765 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.2962 - categorical_crossentropy: 0.2962\n",
            "Epoch 00054: val_categorical_crossentropy improved from 0.37652 to 0.36879, saving model to hdl_col_0.36879__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2963 - categorical_crossentropy: 0.2963 - val_loss: 0.3688 - val_categorical_crossentropy: 0.3688 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "3916/3942 [============================>.] - ETA: 0s - loss: 0.2910 - categorical_crossentropy: 0.2910\n",
            "Epoch 00055: val_categorical_crossentropy improved from 0.36879 to 0.36635, saving model to hdl_col_0.36635__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2913 - categorical_crossentropy: 0.2913 - val_loss: 0.3664 - val_categorical_crossentropy: 0.3664 - lr: 0.0100\n",
            "Epoch 56/100\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.2861 - categorical_crossentropy: 0.2861\n",
            "Epoch 00056: val_categorical_crossentropy improved from 0.36635 to 0.36311, saving model to hdl_col_0.36311__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2863 - categorical_crossentropy: 0.2863 - val_loss: 0.3631 - val_categorical_crossentropy: 0.3631 - lr: 0.0100\n",
            "Epoch 57/100\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.2812 - categorical_crossentropy: 0.2812\n",
            "Epoch 00057: val_categorical_crossentropy improved from 0.36311 to 0.35927, saving model to hdl_col_0.35927__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2814 - categorical_crossentropy: 0.2814 - val_loss: 0.3593 - val_categorical_crossentropy: 0.3593 - lr: 0.0100\n",
            "Epoch 58/100\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.2766 - categorical_crossentropy: 0.2766\n",
            "Epoch 00058: val_categorical_crossentropy improved from 0.35927 to 0.35411, saving model to hdl_col_0.35411__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2767 - categorical_crossentropy: 0.2767 - val_loss: 0.3541 - val_categorical_crossentropy: 0.3541 - lr: 0.0100\n",
            "Epoch 59/100\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.2721 - categorical_crossentropy: 0.2721\n",
            "Epoch 00059: val_categorical_crossentropy improved from 0.35411 to 0.34599, saving model to hdl_col_0.34599__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2723 - categorical_crossentropy: 0.2723 - val_loss: 0.3460 - val_categorical_crossentropy: 0.3460 - lr: 0.0100\n",
            "Epoch 60/100\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.2678 - categorical_crossentropy: 0.2678\n",
            "Epoch 00060: val_categorical_crossentropy improved from 0.34599 to 0.33818, saving model to hdl_col_0.33818__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2681 - categorical_crossentropy: 0.2681 - val_loss: 0.3382 - val_categorical_crossentropy: 0.3382 - lr: 0.0100\n",
            "Epoch 61/100\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.2639 - categorical_crossentropy: 0.2639\n",
            "Epoch 00061: val_categorical_crossentropy improved from 0.33818 to 0.32829, saving model to hdl_col_0.32829__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2639 - categorical_crossentropy: 0.2639 - val_loss: 0.3283 - val_categorical_crossentropy: 0.3283 - lr: 0.0100\n",
            "Epoch 62/100\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.2590 - categorical_crossentropy: 0.2590\n",
            "Epoch 00062: val_categorical_crossentropy did not improve from 0.32829\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2593 - categorical_crossentropy: 0.2593 - val_loss: 0.3341 - val_categorical_crossentropy: 0.3341 - lr: 0.0100\n",
            "Epoch 63/100\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.2555 - categorical_crossentropy: 0.2555\n",
            "Epoch 00063: val_categorical_crossentropy improved from 0.32829 to 0.31704, saving model to hdl_col_0.31704__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2560 - categorical_crossentropy: 0.2560 - val_loss: 0.3170 - val_categorical_crossentropy: 0.3170 - lr: 0.0100\n",
            "Epoch 64/100\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.2516 - categorical_crossentropy: 0.2516\n",
            "Epoch 00064: val_categorical_crossentropy did not improve from 0.31704\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2519 - categorical_crossentropy: 0.2519 - val_loss: 0.3181 - val_categorical_crossentropy: 0.3181 - lr: 0.0100\n",
            "Epoch 65/100\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.2473 - categorical_crossentropy: 0.2473\n",
            "Epoch 00065: val_categorical_crossentropy improved from 0.31704 to 0.31516, saving model to hdl_col_0.31516__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2474 - categorical_crossentropy: 0.2474 - val_loss: 0.3152 - val_categorical_crossentropy: 0.3152 - lr: 0.0100\n",
            "Epoch 66/100\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.2435 - categorical_crossentropy: 0.2435\n",
            "Epoch 00066: val_categorical_crossentropy improved from 0.31516 to 0.30806, saving model to hdl_col_0.30806__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2436 - categorical_crossentropy: 0.2436 - val_loss: 0.3081 - val_categorical_crossentropy: 0.3081 - lr: 0.0100\n",
            "Epoch 67/100\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.2396 - categorical_crossentropy: 0.2396\n",
            "Epoch 00067: val_categorical_crossentropy improved from 0.30806 to 0.29491, saving model to hdl_col_0.29491__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2399 - categorical_crossentropy: 0.2399 - val_loss: 0.2949 - val_categorical_crossentropy: 0.2949 - lr: 0.0100\n",
            "Epoch 68/100\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.2360 - categorical_crossentropy: 0.2360\n",
            "Epoch 00068: val_categorical_crossentropy improved from 0.29491 to 0.28701, saving model to hdl_col_0.28701__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2363 - categorical_crossentropy: 0.2363 - val_loss: 0.2870 - val_categorical_crossentropy: 0.2870 - lr: 0.0100\n",
            "Epoch 69/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.2322 - categorical_crossentropy: 0.2322\n",
            "Epoch 00069: val_categorical_crossentropy improved from 0.28701 to 0.27842, saving model to hdl_col_0.27842__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2324 - categorical_crossentropy: 0.2324 - val_loss: 0.2784 - val_categorical_crossentropy: 0.2784 - lr: 0.0100\n",
            "Epoch 70/100\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.2284 - categorical_crossentropy: 0.2284\n",
            "Epoch 00070: val_categorical_crossentropy improved from 0.27842 to 0.26623, saving model to hdl_col_0.26623__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2286 - categorical_crossentropy: 0.2286 - val_loss: 0.2662 - val_categorical_crossentropy: 0.2662 - lr: 0.0100\n",
            "Epoch 71/100\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.2244 - categorical_crossentropy: 0.2244\n",
            "Epoch 00071: val_categorical_crossentropy improved from 0.26623 to 0.25915, saving model to hdl_col_0.25915__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2247 - categorical_crossentropy: 0.2247 - val_loss: 0.2591 - val_categorical_crossentropy: 0.2591 - lr: 0.0100\n",
            "Epoch 72/100\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.2209 - categorical_crossentropy: 0.2209\n",
            "Epoch 00072: val_categorical_crossentropy improved from 0.25915 to 0.25293, saving model to hdl_col_0.25293__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2213 - categorical_crossentropy: 0.2213 - val_loss: 0.2529 - val_categorical_crossentropy: 0.2529 - lr: 0.0100\n",
            "Epoch 73/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.2177 - categorical_crossentropy: 0.2177\n",
            "Epoch 00073: val_categorical_crossentropy improved from 0.25293 to 0.24736, saving model to hdl_col_0.24736__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2179 - categorical_crossentropy: 0.2179 - val_loss: 0.2474 - val_categorical_crossentropy: 0.2474 - lr: 0.0100\n",
            "Epoch 74/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.2140 - categorical_crossentropy: 0.2140\n",
            "Epoch 00074: val_categorical_crossentropy improved from 0.24736 to 0.24205, saving model to hdl_col_0.24205__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2142 - categorical_crossentropy: 0.2142 - val_loss: 0.2421 - val_categorical_crossentropy: 0.2421 - lr: 0.0100\n",
            "Epoch 75/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.2109 - categorical_crossentropy: 0.2109\n",
            "Epoch 00075: val_categorical_crossentropy improved from 0.24205 to 0.23676, saving model to hdl_col_0.23676__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2111 - categorical_crossentropy: 0.2111 - val_loss: 0.2368 - val_categorical_crossentropy: 0.2368 - lr: 0.0100\n",
            "Epoch 76/100\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.2082 - categorical_crossentropy: 0.2082\n",
            "Epoch 00076: val_categorical_crossentropy improved from 0.23676 to 0.23273, saving model to hdl_col_0.23273__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2082 - categorical_crossentropy: 0.2082 - val_loss: 0.2327 - val_categorical_crossentropy: 0.2327 - lr: 0.0100\n",
            "Epoch 77/100\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.2051 - categorical_crossentropy: 0.2051\n",
            "Epoch 00077: val_categorical_crossentropy improved from 0.23273 to 0.22660, saving model to hdl_col_0.22660__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2051 - categorical_crossentropy: 0.2051 - val_loss: 0.2266 - val_categorical_crossentropy: 0.2266 - lr: 0.0100\n",
            "Epoch 78/100\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.2019 - categorical_crossentropy: 0.2019\n",
            "Epoch 00078: val_categorical_crossentropy improved from 0.22660 to 0.22158, saving model to hdl_col_0.22158__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2019 - categorical_crossentropy: 0.2019 - val_loss: 0.2216 - val_categorical_crossentropy: 0.2216 - lr: 0.0100\n",
            "Epoch 79/100\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.1988 - categorical_crossentropy: 0.1988\n",
            "Epoch 00079: val_categorical_crossentropy improved from 0.22158 to 0.21729, saving model to hdl_col_0.21729__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1987 - categorical_crossentropy: 0.1987 - val_loss: 0.2173 - val_categorical_crossentropy: 0.2173 - lr: 0.0100\n",
            "Epoch 80/100\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.1955 - categorical_crossentropy: 0.1955\n",
            "Epoch 00080: val_categorical_crossentropy improved from 0.21729 to 0.21468, saving model to hdl_col_0.21468__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1958 - categorical_crossentropy: 0.1958 - val_loss: 0.2147 - val_categorical_crossentropy: 0.2147 - lr: 0.0100\n",
            "Epoch 81/100\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.1931 - categorical_crossentropy: 0.1931\n",
            "Epoch 00081: val_categorical_crossentropy improved from 0.21468 to 0.21082, saving model to hdl_col_0.21082__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1934 - categorical_crossentropy: 0.1934 - val_loss: 0.2108 - val_categorical_crossentropy: 0.2108 - lr: 0.0100\n",
            "Epoch 82/100\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.1902 - categorical_crossentropy: 0.1902\n",
            "Epoch 00082: val_categorical_crossentropy improved from 0.21082 to 0.20806, saving model to hdl_col_0.20806__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1905 - categorical_crossentropy: 0.1905 - val_loss: 0.2081 - val_categorical_crossentropy: 0.2081 - lr: 0.0100\n",
            "Epoch 83/100\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.1875 - categorical_crossentropy: 0.1875\n",
            "Epoch 00083: val_categorical_crossentropy improved from 0.20806 to 0.20264, saving model to hdl_col_0.20264__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1878 - categorical_crossentropy: 0.1878 - val_loss: 0.2026 - val_categorical_crossentropy: 0.2026 - lr: 0.0100\n",
            "Epoch 84/100\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.1849 - categorical_crossentropy: 0.1849\n",
            "Epoch 00084: val_categorical_crossentropy improved from 0.20264 to 0.20124, saving model to hdl_col_0.20124__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1849 - categorical_crossentropy: 0.1849 - val_loss: 0.2012 - val_categorical_crossentropy: 0.2012 - lr: 0.0100\n",
            "Epoch 85/100\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.1814 - categorical_crossentropy: 0.1814\n",
            "Epoch 00085: val_categorical_crossentropy improved from 0.20124 to 0.19998, saving model to hdl_col_0.19998__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1817 - categorical_crossentropy: 0.1817 - val_loss: 0.2000 - val_categorical_crossentropy: 0.2000 - lr: 0.0100\n",
            "Epoch 86/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.1777 - categorical_crossentropy: 0.1777\n",
            "Epoch 00086: val_categorical_crossentropy improved from 0.19998 to 0.19612, saving model to hdl_col_0.19612__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1779 - categorical_crossentropy: 0.1779 - val_loss: 0.1961 - val_categorical_crossentropy: 0.1961 - lr: 0.0100\n",
            "Epoch 87/100\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.1739 - categorical_crossentropy: 0.1739\n",
            "Epoch 00087: val_categorical_crossentropy did not improve from 0.19612\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1741 - categorical_crossentropy: 0.1741 - val_loss: 0.1961 - val_categorical_crossentropy: 0.1961 - lr: 0.0100\n",
            "Epoch 88/100\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.1703 - categorical_crossentropy: 0.1703\n",
            "Epoch 00088: val_categorical_crossentropy did not improve from 0.19612\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1704 - categorical_crossentropy: 0.1704 - val_loss: 0.1972 - val_categorical_crossentropy: 0.1972 - lr: 0.0100\n",
            "Epoch 89/100\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.1670 - categorical_crossentropy: 0.1670\n",
            "Epoch 00089: val_categorical_crossentropy improved from 0.19612 to 0.19406, saving model to hdl_col_0.19406__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1672 - categorical_crossentropy: 0.1672 - val_loss: 0.1941 - val_categorical_crossentropy: 0.1941 - lr: 0.0100\n",
            "Epoch 90/100\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.1639 - categorical_crossentropy: 0.1639\n",
            "Epoch 00090: val_categorical_crossentropy improved from 0.19406 to 0.18668, saving model to hdl_col_0.18668__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1642 - categorical_crossentropy: 0.1642 - val_loss: 0.1867 - val_categorical_crossentropy: 0.1867 - lr: 0.0100\n",
            "Epoch 91/100\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.1610 - categorical_crossentropy: 0.1610\n",
            "Epoch 00091: val_categorical_crossentropy improved from 0.18668 to 0.18544, saving model to hdl_col_0.18544__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1612 - categorical_crossentropy: 0.1612 - val_loss: 0.1854 - val_categorical_crossentropy: 0.1854 - lr: 0.0100\n",
            "Epoch 92/100\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.1581 - categorical_crossentropy: 0.1581\n",
            "Epoch 00092: val_categorical_crossentropy improved from 0.18544 to 0.17892, saving model to hdl_col_0.17892__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1583 - categorical_crossentropy: 0.1583 - val_loss: 0.1789 - val_categorical_crossentropy: 0.1789 - lr: 0.0100\n",
            "Epoch 93/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.1546 - categorical_crossentropy: 0.1546\n",
            "Epoch 00093: val_categorical_crossentropy improved from 0.17892 to 0.17154, saving model to hdl_col_0.17154__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1549 - categorical_crossentropy: 0.1549 - val_loss: 0.1715 - val_categorical_crossentropy: 0.1715 - lr: 0.0100\n",
            "Epoch 94/100\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.1522 - categorical_crossentropy: 0.1522\n",
            "Epoch 00094: val_categorical_crossentropy improved from 0.17154 to 0.16939, saving model to hdl_col_0.16939__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1524 - categorical_crossentropy: 0.1524 - val_loss: 0.1694 - val_categorical_crossentropy: 0.1694 - lr: 0.0100\n",
            "Epoch 95/100\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.1493 - categorical_crossentropy: 0.1493\n",
            "Epoch 00095: val_categorical_crossentropy improved from 0.16939 to 0.16707, saving model to hdl_col_0.16707__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1495 - categorical_crossentropy: 0.1495 - val_loss: 0.1671 - val_categorical_crossentropy: 0.1671 - lr: 0.0100\n",
            "Epoch 96/100\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.1471 - categorical_crossentropy: 0.1471\n",
            "Epoch 00096: val_categorical_crossentropy improved from 0.16707 to 0.16606, saving model to hdl_col_0.16606__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1473 - categorical_crossentropy: 0.1473 - val_loss: 0.1661 - val_categorical_crossentropy: 0.1661 - lr: 0.0100\n",
            "Epoch 97/100\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.1435 - categorical_crossentropy: 0.1435\n",
            "Epoch 00097: val_categorical_crossentropy improved from 0.16606 to 0.16405, saving model to hdl_col_0.16405__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1437 - categorical_crossentropy: 0.1437 - val_loss: 0.1640 - val_categorical_crossentropy: 0.1640 - lr: 0.0100\n",
            "Epoch 98/100\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.1432 - categorical_crossentropy: 0.1432\n",
            "Epoch 00098: val_categorical_crossentropy improved from 0.16405 to 0.16225, saving model to hdl_col_0.16225__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1434 - categorical_crossentropy: 0.1434 - val_loss: 0.1622 - val_categorical_crossentropy: 0.1622 - lr: 0.0100\n",
            "Epoch 99/100\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.1392 - categorical_crossentropy: 0.1392\n",
            "Epoch 00099: val_categorical_crossentropy improved from 0.16225 to 0.16217, saving model to hdl_col_0.16217__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1393 - categorical_crossentropy: 0.1393 - val_loss: 0.1622 - val_categorical_crossentropy: 0.1622 - lr: 0.0100\n",
            "Epoch 100/100\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.1381 - categorical_crossentropy: 0.1381\n",
            "Epoch 00100: val_categorical_crossentropy improved from 0.16217 to 0.15790, saving model to hdl_col_0.15790__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1383 - categorical_crossentropy: 0.1383 - val_loss: 0.1579 - val_categorical_crossentropy: 0.1579 - lr: 0.0100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4fab1db490>"
            ]
          },
          "metadata": {},
          "execution_count": 154
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCkGjfek5N3Q"
      },
      "source": [
        ""
      ],
      "execution_count": 154,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WauRtjb05Ocw"
      },
      "source": [
        "model_name=\"col\"\n",
        "def scheduler(epoch, lr):\n",
        "    return 0.01\n",
        "\n",
        "callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks = [callback_LR,\n",
        "        \n",
        "\n",
        "        \n",
        "        #savemodela,\n",
        "        ModelCheckpoint(filepath=model_name+\"_{val_categorical_crossentropy:.5f}__.hdf5\", monitor='val_categorical_crossentropy',\n",
        "                        verbose=2, save_best_only=True, mode='min')]"
      ],
      "execution_count": 155,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10uUmguL6rT3",
        "outputId": "c8ea910e-707a-41d1-a7c8-c295e68c4c92"
      },
      "source": [
        "col_model=blood_model()"
      ],
      "execution_count": 156,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "js1cZV6n5Ocw",
        "outputId": "8b4987eb-fc88-4798-b800-588924b9b8ea"
      },
      "source": [
        "col_model.fit(X,\n",
        "          y_col,\n",
        "          epochs=150, \n",
        "          batch_size=3,\n",
        "          validation_split=0.1,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          shuffle=False\n",
        "          \n",
        "          )"
      ],
      "execution_count": 157,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.8414 - categorical_crossentropy: 0.8414\n",
            "Epoch 00001: val_categorical_crossentropy improved from inf to 0.82642, saving model to col_0.82642__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.8414 - categorical_crossentropy: 0.8414 - val_loss: 0.8264 - val_categorical_crossentropy: 0.8264 - lr: 0.0100\n",
            "Epoch 2/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.8070 - categorical_crossentropy: 0.8070\n",
            "Epoch 00002: val_categorical_crossentropy improved from 0.82642 to 0.81192, saving model to col_0.81192__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.8071 - categorical_crossentropy: 0.8071 - val_loss: 0.8119 - val_categorical_crossentropy: 0.8119 - lr: 0.0100\n",
            "Epoch 3/150\n",
            "3911/3942 [============================>.] - ETA: 0s - loss: 0.7908 - categorical_crossentropy: 0.7908\n",
            "Epoch 00003: val_categorical_crossentropy improved from 0.81192 to 0.80553, saving model to col_0.80553__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7907 - categorical_crossentropy: 0.7907 - val_loss: 0.8055 - val_categorical_crossentropy: 0.8055 - lr: 0.0100\n",
            "Epoch 4/150\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.7770 - categorical_crossentropy: 0.7770\n",
            "Epoch 00004: val_categorical_crossentropy improved from 0.80553 to 0.79582, saving model to col_0.79582__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7772 - categorical_crossentropy: 0.7772 - val_loss: 0.7958 - val_categorical_crossentropy: 0.7958 - lr: 0.0100\n",
            "Epoch 5/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.7631 - categorical_crossentropy: 0.7631\n",
            "Epoch 00005: val_categorical_crossentropy improved from 0.79582 to 0.78778, saving model to col_0.78778__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7630 - categorical_crossentropy: 0.7630 - val_loss: 0.7878 - val_categorical_crossentropy: 0.7878 - lr: 0.0100\n",
            "Epoch 6/150\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.7488 - categorical_crossentropy: 0.7488\n",
            "Epoch 00006: val_categorical_crossentropy improved from 0.78778 to 0.78147, saving model to col_0.78147__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7488 - categorical_crossentropy: 0.7488 - val_loss: 0.7815 - val_categorical_crossentropy: 0.7815 - lr: 0.0100\n",
            "Epoch 7/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.7363 - categorical_crossentropy: 0.7363\n",
            "Epoch 00007: val_categorical_crossentropy improved from 0.78147 to 0.76830, saving model to col_0.76830__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7363 - categorical_crossentropy: 0.7363 - val_loss: 0.7683 - val_categorical_crossentropy: 0.7683 - lr: 0.0100\n",
            "Epoch 8/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.7235 - categorical_crossentropy: 0.7235\n",
            "Epoch 00008: val_categorical_crossentropy improved from 0.76830 to 0.75078, saving model to col_0.75078__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7234 - categorical_crossentropy: 0.7234 - val_loss: 0.7508 - val_categorical_crossentropy: 0.7508 - lr: 0.0100\n",
            "Epoch 9/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.7088 - categorical_crossentropy: 0.7088\n",
            "Epoch 00009: val_categorical_crossentropy improved from 0.75078 to 0.72756, saving model to col_0.72756__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.7086 - categorical_crossentropy: 0.7086 - val_loss: 0.7276 - val_categorical_crossentropy: 0.7276 - lr: 0.0100\n",
            "Epoch 10/150\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.6903 - categorical_crossentropy: 0.6903\n",
            "Epoch 00010: val_categorical_crossentropy improved from 0.72756 to 0.70693, saving model to col_0.70693__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6905 - categorical_crossentropy: 0.6905 - val_loss: 0.7069 - val_categorical_crossentropy: 0.7069 - lr: 0.0100\n",
            "Epoch 11/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.6724 - categorical_crossentropy: 0.6724\n",
            "Epoch 00011: val_categorical_crossentropy improved from 0.70693 to 0.68967, saving model to col_0.68967__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6726 - categorical_crossentropy: 0.6726 - val_loss: 0.6897 - val_categorical_crossentropy: 0.6897 - lr: 0.0100\n",
            "Epoch 12/150\n",
            "3911/3942 [============================>.] - ETA: 0s - loss: 0.6565 - categorical_crossentropy: 0.6565\n",
            "Epoch 00012: val_categorical_crossentropy improved from 0.68967 to 0.67288, saving model to col_0.67288__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6566 - categorical_crossentropy: 0.6566 - val_loss: 0.6729 - val_categorical_crossentropy: 0.6729 - lr: 0.0100\n",
            "Epoch 13/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.6417 - categorical_crossentropy: 0.6417\n",
            "Epoch 00013: val_categorical_crossentropy improved from 0.67288 to 0.65632, saving model to col_0.65632__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6419 - categorical_crossentropy: 0.6419 - val_loss: 0.6563 - val_categorical_crossentropy: 0.6563 - lr: 0.0100\n",
            "Epoch 14/150\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.6266 - categorical_crossentropy: 0.6266\n",
            "Epoch 00014: val_categorical_crossentropy improved from 0.65632 to 0.63868, saving model to col_0.63868__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6269 - categorical_crossentropy: 0.6269 - val_loss: 0.6387 - val_categorical_crossentropy: 0.6387 - lr: 0.0100\n",
            "Epoch 15/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.6108 - categorical_crossentropy: 0.6108\n",
            "Epoch 00015: val_categorical_crossentropy improved from 0.63868 to 0.62213, saving model to col_0.62213__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.6111 - categorical_crossentropy: 0.6111 - val_loss: 0.6221 - val_categorical_crossentropy: 0.6221 - lr: 0.0100\n",
            "Epoch 16/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.5961 - categorical_crossentropy: 0.5961\n",
            "Epoch 00016: val_categorical_crossentropy improved from 0.62213 to 0.60728, saving model to col_0.60728__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5961 - categorical_crossentropy: 0.5961 - val_loss: 0.6073 - val_categorical_crossentropy: 0.6073 - lr: 0.0100\n",
            "Epoch 17/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.5821 - categorical_crossentropy: 0.5821\n",
            "Epoch 00017: val_categorical_crossentropy improved from 0.60728 to 0.59310, saving model to col_0.59310__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5819 - categorical_crossentropy: 0.5819 - val_loss: 0.5931 - val_categorical_crossentropy: 0.5931 - lr: 0.0100\n",
            "Epoch 18/150\n",
            "3911/3942 [============================>.] - ETA: 0s - loss: 0.5683 - categorical_crossentropy: 0.5683\n",
            "Epoch 00018: val_categorical_crossentropy improved from 0.59310 to 0.57906, saving model to col_0.57906__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5687 - categorical_crossentropy: 0.5687 - val_loss: 0.5791 - val_categorical_crossentropy: 0.5791 - lr: 0.0100\n",
            "Epoch 19/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.5567 - categorical_crossentropy: 0.5567\n",
            "Epoch 00019: val_categorical_crossentropy improved from 0.57906 to 0.56571, saving model to col_0.56571__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5567 - categorical_crossentropy: 0.5567 - val_loss: 0.5657 - val_categorical_crossentropy: 0.5657 - lr: 0.0100\n",
            "Epoch 20/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.5450 - categorical_crossentropy: 0.5450\n",
            "Epoch 00020: val_categorical_crossentropy improved from 0.56571 to 0.55410, saving model to col_0.55410__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5455 - categorical_crossentropy: 0.5455 - val_loss: 0.5541 - val_categorical_crossentropy: 0.5541 - lr: 0.0100\n",
            "Epoch 21/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.5347 - categorical_crossentropy: 0.5347\n",
            "Epoch 00021: val_categorical_crossentropy improved from 0.55410 to 0.54330, saving model to col_0.54330__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5347 - categorical_crossentropy: 0.5347 - val_loss: 0.5433 - val_categorical_crossentropy: 0.5433 - lr: 0.0100\n",
            "Epoch 22/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.5242 - categorical_crossentropy: 0.5242\n",
            "Epoch 00022: val_categorical_crossentropy improved from 0.54330 to 0.53379, saving model to col_0.53379__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5240 - categorical_crossentropy: 0.5240 - val_loss: 0.5338 - val_categorical_crossentropy: 0.5338 - lr: 0.0100\n",
            "Epoch 23/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.5134 - categorical_crossentropy: 0.5134\n",
            "Epoch 00023: val_categorical_crossentropy improved from 0.53379 to 0.52625, saving model to col_0.52625__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5134 - categorical_crossentropy: 0.5134 - val_loss: 0.5263 - val_categorical_crossentropy: 0.5263 - lr: 0.0100\n",
            "Epoch 24/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.5032 - categorical_crossentropy: 0.5032\n",
            "Epoch 00024: val_categorical_crossentropy improved from 0.52625 to 0.52015, saving model to col_0.52015__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.5030 - categorical_crossentropy: 0.5030 - val_loss: 0.5201 - val_categorical_crossentropy: 0.5201 - lr: 0.0100\n",
            "Epoch 25/150\n",
            "3916/3942 [============================>.] - ETA: 0s - loss: 0.4925 - categorical_crossentropy: 0.4925\n",
            "Epoch 00025: val_categorical_crossentropy improved from 0.52015 to 0.51355, saving model to col_0.51355__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4929 - categorical_crossentropy: 0.4929 - val_loss: 0.5135 - val_categorical_crossentropy: 0.5135 - lr: 0.0100\n",
            "Epoch 26/150\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.4833 - categorical_crossentropy: 0.4833\n",
            "Epoch 00026: val_categorical_crossentropy improved from 0.51355 to 0.50278, saving model to col_0.50278__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4831 - categorical_crossentropy: 0.4831 - val_loss: 0.5028 - val_categorical_crossentropy: 0.5028 - lr: 0.0100\n",
            "Epoch 27/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.4736 - categorical_crossentropy: 0.4736\n",
            "Epoch 00027: val_categorical_crossentropy improved from 0.50278 to 0.49155, saving model to col_0.49155__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4735 - categorical_crossentropy: 0.4735 - val_loss: 0.4916 - val_categorical_crossentropy: 0.4916 - lr: 0.0100\n",
            "Epoch 28/150\n",
            "3912/3942 [============================>.] - ETA: 0s - loss: 0.4637 - categorical_crossentropy: 0.4637\n",
            "Epoch 00028: val_categorical_crossentropy improved from 0.49155 to 0.48227, saving model to col_0.48227__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4639 - categorical_crossentropy: 0.4639 - val_loss: 0.4823 - val_categorical_crossentropy: 0.4823 - lr: 0.0100\n",
            "Epoch 29/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.4545 - categorical_crossentropy: 0.4545\n",
            "Epoch 00029: val_categorical_crossentropy improved from 0.48227 to 0.47371, saving model to col_0.47371__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4545 - categorical_crossentropy: 0.4545 - val_loss: 0.4737 - val_categorical_crossentropy: 0.4737 - lr: 0.0100\n",
            "Epoch 30/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.4451 - categorical_crossentropy: 0.4451\n",
            "Epoch 00030: val_categorical_crossentropy improved from 0.47371 to 0.46609, saving model to col_0.46609__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4453 - categorical_crossentropy: 0.4453 - val_loss: 0.4661 - val_categorical_crossentropy: 0.4661 - lr: 0.0100\n",
            "Epoch 31/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.4366 - categorical_crossentropy: 0.4366\n",
            "Epoch 00031: val_categorical_crossentropy improved from 0.46609 to 0.45919, saving model to col_0.45919__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4366 - categorical_crossentropy: 0.4366 - val_loss: 0.4592 - val_categorical_crossentropy: 0.4592 - lr: 0.0100\n",
            "Epoch 32/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.4281 - categorical_crossentropy: 0.4281\n",
            "Epoch 00032: val_categorical_crossentropy improved from 0.45919 to 0.45316, saving model to col_0.45316__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4281 - categorical_crossentropy: 0.4281 - val_loss: 0.4532 - val_categorical_crossentropy: 0.4532 - lr: 0.0100\n",
            "Epoch 33/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.4200 - categorical_crossentropy: 0.4200\n",
            "Epoch 00033: val_categorical_crossentropy improved from 0.45316 to 0.44877, saving model to col_0.44877__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4200 - categorical_crossentropy: 0.4200 - val_loss: 0.4488 - val_categorical_crossentropy: 0.4488 - lr: 0.0100\n",
            "Epoch 34/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.4123 - categorical_crossentropy: 0.4123\n",
            "Epoch 00034: val_categorical_crossentropy improved from 0.44877 to 0.44466, saving model to col_0.44466__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4122 - categorical_crossentropy: 0.4122 - val_loss: 0.4447 - val_categorical_crossentropy: 0.4447 - lr: 0.0100\n",
            "Epoch 35/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.4050 - categorical_crossentropy: 0.4050\n",
            "Epoch 00035: val_categorical_crossentropy improved from 0.44466 to 0.44059, saving model to col_0.44059__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4048 - categorical_crossentropy: 0.4048 - val_loss: 0.4406 - val_categorical_crossentropy: 0.4406 - lr: 0.0100\n",
            "Epoch 36/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.3977 - categorical_crossentropy: 0.3977\n",
            "Epoch 00036: val_categorical_crossentropy improved from 0.44059 to 0.43604, saving model to col_0.43604__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3975 - categorical_crossentropy: 0.3975 - val_loss: 0.4360 - val_categorical_crossentropy: 0.4360 - lr: 0.0100\n",
            "Epoch 37/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.3907 - categorical_crossentropy: 0.3907\n",
            "Epoch 00037: val_categorical_crossentropy improved from 0.43604 to 0.43193, saving model to col_0.43193__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3906 - categorical_crossentropy: 0.3906 - val_loss: 0.4319 - val_categorical_crossentropy: 0.4319 - lr: 0.0100\n",
            "Epoch 38/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.3840 - categorical_crossentropy: 0.3840\n",
            "Epoch 00038: val_categorical_crossentropy improved from 0.43193 to 0.42790, saving model to col_0.42790__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3840 - categorical_crossentropy: 0.3840 - val_loss: 0.4279 - val_categorical_crossentropy: 0.4279 - lr: 0.0100\n",
            "Epoch 39/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.3775 - categorical_crossentropy: 0.3775\n",
            "Epoch 00039: val_categorical_crossentropy improved from 0.42790 to 0.42403, saving model to col_0.42403__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3775 - categorical_crossentropy: 0.3775 - val_loss: 0.4240 - val_categorical_crossentropy: 0.4240 - lr: 0.0100\n",
            "Epoch 40/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.3713 - categorical_crossentropy: 0.3713\n",
            "Epoch 00040: val_categorical_crossentropy improved from 0.42403 to 0.42020, saving model to col_0.42020__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3712 - categorical_crossentropy: 0.3712 - val_loss: 0.4202 - val_categorical_crossentropy: 0.4202 - lr: 0.0100\n",
            "Epoch 41/150\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.3652 - categorical_crossentropy: 0.3652\n",
            "Epoch 00041: val_categorical_crossentropy improved from 0.42020 to 0.41546, saving model to col_0.41546__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3652 - categorical_crossentropy: 0.3652 - val_loss: 0.4155 - val_categorical_crossentropy: 0.4155 - lr: 0.0100\n",
            "Epoch 42/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.3594 - categorical_crossentropy: 0.3594\n",
            "Epoch 00042: val_categorical_crossentropy improved from 0.41546 to 0.41277, saving model to col_0.41277__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3593 - categorical_crossentropy: 0.3593 - val_loss: 0.4128 - val_categorical_crossentropy: 0.4128 - lr: 0.0100\n",
            "Epoch 43/150\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.3537 - categorical_crossentropy: 0.3537\n",
            "Epoch 00043: val_categorical_crossentropy improved from 0.41277 to 0.41041, saving model to col_0.41041__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3536 - categorical_crossentropy: 0.3536 - val_loss: 0.4104 - val_categorical_crossentropy: 0.4104 - lr: 0.0100\n",
            "Epoch 44/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.3482 - categorical_crossentropy: 0.3482\n",
            "Epoch 00044: val_categorical_crossentropy improved from 0.41041 to 0.40886, saving model to col_0.40886__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3481 - categorical_crossentropy: 0.3481 - val_loss: 0.4089 - val_categorical_crossentropy: 0.4089 - lr: 0.0100\n",
            "Epoch 45/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.3426 - categorical_crossentropy: 0.3426\n",
            "Epoch 00045: val_categorical_crossentropy improved from 0.40886 to 0.40625, saving model to col_0.40625__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3426 - categorical_crossentropy: 0.3426 - val_loss: 0.4062 - val_categorical_crossentropy: 0.4062 - lr: 0.0100\n",
            "Epoch 46/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.3371 - categorical_crossentropy: 0.3371\n",
            "Epoch 00046: val_categorical_crossentropy improved from 0.40625 to 0.40300, saving model to col_0.40300__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3371 - categorical_crossentropy: 0.3371 - val_loss: 0.4030 - val_categorical_crossentropy: 0.4030 - lr: 0.0100\n",
            "Epoch 47/150\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.3317 - categorical_crossentropy: 0.3317\n",
            "Epoch 00047: val_categorical_crossentropy improved from 0.40300 to 0.39929, saving model to col_0.39929__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3317 - categorical_crossentropy: 0.3317 - val_loss: 0.3993 - val_categorical_crossentropy: 0.3993 - lr: 0.0100\n",
            "Epoch 48/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.3264 - categorical_crossentropy: 0.3264\n",
            "Epoch 00048: val_categorical_crossentropy improved from 0.39929 to 0.39570, saving model to col_0.39570__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3264 - categorical_crossentropy: 0.3264 - val_loss: 0.3957 - val_categorical_crossentropy: 0.3957 - lr: 0.0100\n",
            "Epoch 49/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.3211 - categorical_crossentropy: 0.3211\n",
            "Epoch 00049: val_categorical_crossentropy improved from 0.39570 to 0.39218, saving model to col_0.39218__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3211 - categorical_crossentropy: 0.3211 - val_loss: 0.3922 - val_categorical_crossentropy: 0.3922 - lr: 0.0100\n",
            "Epoch 50/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.3159 - categorical_crossentropy: 0.3159\n",
            "Epoch 00050: val_categorical_crossentropy improved from 0.39218 to 0.38957, saving model to col_0.38957__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3159 - categorical_crossentropy: 0.3159 - val_loss: 0.3896 - val_categorical_crossentropy: 0.3896 - lr: 0.0100\n",
            "Epoch 51/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.3109 - categorical_crossentropy: 0.3109\n",
            "Epoch 00051: val_categorical_crossentropy improved from 0.38957 to 0.38779, saving model to col_0.38779__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3109 - categorical_crossentropy: 0.3109 - val_loss: 0.3878 - val_categorical_crossentropy: 0.3878 - lr: 0.0100\n",
            "Epoch 52/150\n",
            "3912/3942 [============================>.] - ETA: 0s - loss: 0.3061 - categorical_crossentropy: 0.3061\n",
            "Epoch 00052: val_categorical_crossentropy improved from 0.38779 to 0.38436, saving model to col_0.38436__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3060 - categorical_crossentropy: 0.3060 - val_loss: 0.3844 - val_categorical_crossentropy: 0.3844 - lr: 0.0100\n",
            "Epoch 53/150\n",
            "3914/3942 [============================>.] - ETA: 0s - loss: 0.3014 - categorical_crossentropy: 0.3014\n",
            "Epoch 00053: val_categorical_crossentropy improved from 0.38436 to 0.38045, saving model to col_0.38045__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3013 - categorical_crossentropy: 0.3013 - val_loss: 0.3805 - val_categorical_crossentropy: 0.3805 - lr: 0.0100\n",
            "Epoch 54/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.2967 - categorical_crossentropy: 0.2967\n",
            "Epoch 00054: val_categorical_crossentropy improved from 0.38045 to 0.37982, saving model to col_0.37982__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2966 - categorical_crossentropy: 0.2966 - val_loss: 0.3798 - val_categorical_crossentropy: 0.3798 - lr: 0.0100\n",
            "Epoch 55/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.2920 - categorical_crossentropy: 0.2920\n",
            "Epoch 00055: val_categorical_crossentropy improved from 0.37982 to 0.37780, saving model to col_0.37780__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2919 - categorical_crossentropy: 0.2919 - val_loss: 0.3778 - val_categorical_crossentropy: 0.3778 - lr: 0.0100\n",
            "Epoch 56/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.2873 - categorical_crossentropy: 0.2873\n",
            "Epoch 00056: val_categorical_crossentropy improved from 0.37780 to 0.37501, saving model to col_0.37501__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2874 - categorical_crossentropy: 0.2874 - val_loss: 0.3750 - val_categorical_crossentropy: 0.3750 - lr: 0.0100\n",
            "Epoch 57/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.2828 - categorical_crossentropy: 0.2828\n",
            "Epoch 00057: val_categorical_crossentropy improved from 0.37501 to 0.37244, saving model to col_0.37244__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2828 - categorical_crossentropy: 0.2828 - val_loss: 0.3724 - val_categorical_crossentropy: 0.3724 - lr: 0.0100\n",
            "Epoch 58/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.2782 - categorical_crossentropy: 0.2782\n",
            "Epoch 00058: val_categorical_crossentropy improved from 0.37244 to 0.37024, saving model to col_0.37024__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2782 - categorical_crossentropy: 0.2782 - val_loss: 0.3702 - val_categorical_crossentropy: 0.3702 - lr: 0.0100\n",
            "Epoch 59/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.2737 - categorical_crossentropy: 0.2737\n",
            "Epoch 00059: val_categorical_crossentropy improved from 0.37024 to 0.36629, saving model to col_0.36629__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2737 - categorical_crossentropy: 0.2737 - val_loss: 0.3663 - val_categorical_crossentropy: 0.3663 - lr: 0.0100\n",
            "Epoch 60/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.2695 - categorical_crossentropy: 0.2695\n",
            "Epoch 00060: val_categorical_crossentropy improved from 0.36629 to 0.36439, saving model to col_0.36439__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2695 - categorical_crossentropy: 0.2695 - val_loss: 0.3644 - val_categorical_crossentropy: 0.3644 - lr: 0.0100\n",
            "Epoch 61/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.2649 - categorical_crossentropy: 0.2649\n",
            "Epoch 00061: val_categorical_crossentropy improved from 0.36439 to 0.36142, saving model to col_0.36142__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2650 - categorical_crossentropy: 0.2650 - val_loss: 0.3614 - val_categorical_crossentropy: 0.3614 - lr: 0.0100\n",
            "Epoch 62/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.2610 - categorical_crossentropy: 0.2610\n",
            "Epoch 00062: val_categorical_crossentropy improved from 0.36142 to 0.36047, saving model to col_0.36047__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2610 - categorical_crossentropy: 0.2610 - val_loss: 0.3605 - val_categorical_crossentropy: 0.3605 - lr: 0.0100\n",
            "Epoch 63/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.2568 - categorical_crossentropy: 0.2568\n",
            "Epoch 00063: val_categorical_crossentropy improved from 0.36047 to 0.35943, saving model to col_0.35943__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2569 - categorical_crossentropy: 0.2569 - val_loss: 0.3594 - val_categorical_crossentropy: 0.3594 - lr: 0.0100\n",
            "Epoch 64/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.2529 - categorical_crossentropy: 0.2529\n",
            "Epoch 00064: val_categorical_crossentropy improved from 0.35943 to 0.35854, saving model to col_0.35854__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2529 - categorical_crossentropy: 0.2529 - val_loss: 0.3585 - val_categorical_crossentropy: 0.3585 - lr: 0.0100\n",
            "Epoch 65/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.2489 - categorical_crossentropy: 0.2489\n",
            "Epoch 00065: val_categorical_crossentropy improved from 0.35854 to 0.35721, saving model to col_0.35721__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2490 - categorical_crossentropy: 0.2490 - val_loss: 0.3572 - val_categorical_crossentropy: 0.3572 - lr: 0.0100\n",
            "Epoch 66/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.2450 - categorical_crossentropy: 0.2450\n",
            "Epoch 00066: val_categorical_crossentropy improved from 0.35721 to 0.35429, saving model to col_0.35429__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2452 - categorical_crossentropy: 0.2452 - val_loss: 0.3543 - val_categorical_crossentropy: 0.3543 - lr: 0.0100\n",
            "Epoch 67/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.2415 - categorical_crossentropy: 0.2415\n",
            "Epoch 00067: val_categorical_crossentropy improved from 0.35429 to 0.35024, saving model to col_0.35024__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2415 - categorical_crossentropy: 0.2415 - val_loss: 0.3502 - val_categorical_crossentropy: 0.3502 - lr: 0.0100\n",
            "Epoch 68/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.2378 - categorical_crossentropy: 0.2378\n",
            "Epoch 00068: val_categorical_crossentropy improved from 0.35024 to 0.34671, saving model to col_0.34671__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2378 - categorical_crossentropy: 0.2378 - val_loss: 0.3467 - val_categorical_crossentropy: 0.3467 - lr: 0.0100\n",
            "Epoch 69/150\n",
            "3911/3942 [============================>.] - ETA: 0s - loss: 0.2340 - categorical_crossentropy: 0.2340\n",
            "Epoch 00069: val_categorical_crossentropy improved from 0.34671 to 0.34366, saving model to col_0.34366__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2342 - categorical_crossentropy: 0.2342 - val_loss: 0.3437 - val_categorical_crossentropy: 0.3437 - lr: 0.0100\n",
            "Epoch 70/150\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.2305 - categorical_crossentropy: 0.2305\n",
            "Epoch 00070: val_categorical_crossentropy improved from 0.34366 to 0.34048, saving model to col_0.34048__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2306 - categorical_crossentropy: 0.2306 - val_loss: 0.3405 - val_categorical_crossentropy: 0.3405 - lr: 0.0100\n",
            "Epoch 71/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.2271 - categorical_crossentropy: 0.2271\n",
            "Epoch 00071: val_categorical_crossentropy improved from 0.34048 to 0.33726, saving model to col_0.33726__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2271 - categorical_crossentropy: 0.2271 - val_loss: 0.3373 - val_categorical_crossentropy: 0.3373 - lr: 0.0100\n",
            "Epoch 72/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.2234 - categorical_crossentropy: 0.2234\n",
            "Epoch 00072: val_categorical_crossentropy improved from 0.33726 to 0.33373, saving model to col_0.33373__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2235 - categorical_crossentropy: 0.2235 - val_loss: 0.3337 - val_categorical_crossentropy: 0.3337 - lr: 0.0100\n",
            "Epoch 73/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.2200 - categorical_crossentropy: 0.2200\n",
            "Epoch 00073: val_categorical_crossentropy improved from 0.33373 to 0.32924, saving model to col_0.32924__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2201 - categorical_crossentropy: 0.2201 - val_loss: 0.3292 - val_categorical_crossentropy: 0.3292 - lr: 0.0100\n",
            "Epoch 74/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.2165 - categorical_crossentropy: 0.2165\n",
            "Epoch 00074: val_categorical_crossentropy improved from 0.32924 to 0.32473, saving model to col_0.32473__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2167 - categorical_crossentropy: 0.2167 - val_loss: 0.3247 - val_categorical_crossentropy: 0.3247 - lr: 0.0100\n",
            "Epoch 75/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.2133 - categorical_crossentropy: 0.2133\n",
            "Epoch 00075: val_categorical_crossentropy improved from 0.32473 to 0.31999, saving model to col_0.31999__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2132 - categorical_crossentropy: 0.2132 - val_loss: 0.3200 - val_categorical_crossentropy: 0.3200 - lr: 0.0100\n",
            "Epoch 76/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.2100 - categorical_crossentropy: 0.2100\n",
            "Epoch 00076: val_categorical_crossentropy improved from 0.31999 to 0.31530, saving model to col_0.31530__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2099 - categorical_crossentropy: 0.2099 - val_loss: 0.3153 - val_categorical_crossentropy: 0.3153 - lr: 0.0100\n",
            "Epoch 77/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.2062 - categorical_crossentropy: 0.2062\n",
            "Epoch 00077: val_categorical_crossentropy improved from 0.31530 to 0.30957, saving model to col_0.30957__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2063 - categorical_crossentropy: 0.2063 - val_loss: 0.3096 - val_categorical_crossentropy: 0.3096 - lr: 0.0100\n",
            "Epoch 78/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.2030 - categorical_crossentropy: 0.2030\n",
            "Epoch 00078: val_categorical_crossentropy improved from 0.30957 to 0.30645, saving model to col_0.30645__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.2031 - categorical_crossentropy: 0.2031 - val_loss: 0.3064 - val_categorical_crossentropy: 0.3064 - lr: 0.0100\n",
            "Epoch 79/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.1997 - categorical_crossentropy: 0.1997\n",
            "Epoch 00079: val_categorical_crossentropy improved from 0.30645 to 0.30077, saving model to col_0.30077__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1997 - categorical_crossentropy: 0.1997 - val_loss: 0.3008 - val_categorical_crossentropy: 0.3008 - lr: 0.0100\n",
            "Epoch 80/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.1962 - categorical_crossentropy: 0.1962\n",
            "Epoch 00080: val_categorical_crossentropy improved from 0.30077 to 0.29433, saving model to col_0.29433__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1963 - categorical_crossentropy: 0.1963 - val_loss: 0.2943 - val_categorical_crossentropy: 0.2943 - lr: 0.0100\n",
            "Epoch 81/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.1930 - categorical_crossentropy: 0.1930\n",
            "Epoch 00081: val_categorical_crossentropy improved from 0.29433 to 0.28901, saving model to col_0.28901__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1931 - categorical_crossentropy: 0.1931 - val_loss: 0.2890 - val_categorical_crossentropy: 0.2890 - lr: 0.0100\n",
            "Epoch 82/150\n",
            "3908/3942 [============================>.] - ETA: 0s - loss: 0.1899 - categorical_crossentropy: 0.1899\n",
            "Epoch 00082: val_categorical_crossentropy improved from 0.28901 to 0.28416, saving model to col_0.28416__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1900 - categorical_crossentropy: 0.1900 - val_loss: 0.2842 - val_categorical_crossentropy: 0.2842 - lr: 0.0100\n",
            "Epoch 83/150\n",
            "3912/3942 [============================>.] - ETA: 0s - loss: 0.1868 - categorical_crossentropy: 0.1868\n",
            "Epoch 00083: val_categorical_crossentropy improved from 0.28416 to 0.27846, saving model to col_0.27846__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1869 - categorical_crossentropy: 0.1869 - val_loss: 0.2785 - val_categorical_crossentropy: 0.2785 - lr: 0.0100\n",
            "Epoch 84/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.1838 - categorical_crossentropy: 0.1838\n",
            "Epoch 00084: val_categorical_crossentropy improved from 0.27846 to 0.27173, saving model to col_0.27173__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1839 - categorical_crossentropy: 0.1839 - val_loss: 0.2717 - val_categorical_crossentropy: 0.2717 - lr: 0.0100\n",
            "Epoch 85/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.1809 - categorical_crossentropy: 0.1809\n",
            "Epoch 00085: val_categorical_crossentropy improved from 0.27173 to 0.26304, saving model to col_0.26304__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1809 - categorical_crossentropy: 0.1809 - val_loss: 0.2630 - val_categorical_crossentropy: 0.2630 - lr: 0.0100\n",
            "Epoch 86/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.1776 - categorical_crossentropy: 0.1776\n",
            "Epoch 00086: val_categorical_crossentropy improved from 0.26304 to 0.25444, saving model to col_0.25444__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1777 - categorical_crossentropy: 0.1777 - val_loss: 0.2544 - val_categorical_crossentropy: 0.2544 - lr: 0.0100\n",
            "Epoch 87/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.1742 - categorical_crossentropy: 0.1742\n",
            "Epoch 00087: val_categorical_crossentropy improved from 0.25444 to 0.24621, saving model to col_0.24621__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1744 - categorical_crossentropy: 0.1744 - val_loss: 0.2462 - val_categorical_crossentropy: 0.2462 - lr: 0.0100\n",
            "Epoch 88/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.1710 - categorical_crossentropy: 0.1710\n",
            "Epoch 00088: val_categorical_crossentropy improved from 0.24621 to 0.23734, saving model to col_0.23734__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1711 - categorical_crossentropy: 0.1711 - val_loss: 0.2373 - val_categorical_crossentropy: 0.2373 - lr: 0.0100\n",
            "Epoch 89/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.1678 - categorical_crossentropy: 0.1678\n",
            "Epoch 00089: val_categorical_crossentropy improved from 0.23734 to 0.22943, saving model to col_0.22943__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1679 - categorical_crossentropy: 0.1679 - val_loss: 0.2294 - val_categorical_crossentropy: 0.2294 - lr: 0.0100\n",
            "Epoch 90/150\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.1646 - categorical_crossentropy: 0.1646\n",
            "Epoch 00090: val_categorical_crossentropy improved from 0.22943 to 0.22385, saving model to col_0.22385__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1646 - categorical_crossentropy: 0.1646 - val_loss: 0.2239 - val_categorical_crossentropy: 0.2239 - lr: 0.0100\n",
            "Epoch 91/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.1612 - categorical_crossentropy: 0.1612\n",
            "Epoch 00091: val_categorical_crossentropy improved from 0.22385 to 0.21800, saving model to col_0.21800__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1613 - categorical_crossentropy: 0.1613 - val_loss: 0.2180 - val_categorical_crossentropy: 0.2180 - lr: 0.0100\n",
            "Epoch 92/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.1578 - categorical_crossentropy: 0.1578\n",
            "Epoch 00092: val_categorical_crossentropy improved from 0.21800 to 0.21189, saving model to col_0.21189__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1579 - categorical_crossentropy: 0.1579 - val_loss: 0.2119 - val_categorical_crossentropy: 0.2119 - lr: 0.0100\n",
            "Epoch 93/150\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.1546 - categorical_crossentropy: 0.1546\n",
            "Epoch 00093: val_categorical_crossentropy improved from 0.21189 to 0.20614, saving model to col_0.20614__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1548 - categorical_crossentropy: 0.1548 - val_loss: 0.2061 - val_categorical_crossentropy: 0.2061 - lr: 0.0100\n",
            "Epoch 94/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.1517 - categorical_crossentropy: 0.1517\n",
            "Epoch 00094: val_categorical_crossentropy improved from 0.20614 to 0.20162, saving model to col_0.20162__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1517 - categorical_crossentropy: 0.1517 - val_loss: 0.2016 - val_categorical_crossentropy: 0.2016 - lr: 0.0100\n",
            "Epoch 95/150\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.1486 - categorical_crossentropy: 0.1486\n",
            "Epoch 00095: val_categorical_crossentropy improved from 0.20162 to 0.19968, saving model to col_0.19968__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1487 - categorical_crossentropy: 0.1487 - val_loss: 0.1997 - val_categorical_crossentropy: 0.1997 - lr: 0.0100\n",
            "Epoch 96/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.1458 - categorical_crossentropy: 0.1458\n",
            "Epoch 00096: val_categorical_crossentropy improved from 0.19968 to 0.19926, saving model to col_0.19926__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1459 - categorical_crossentropy: 0.1459 - val_loss: 0.1993 - val_categorical_crossentropy: 0.1993 - lr: 0.0100\n",
            "Epoch 97/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.1427 - categorical_crossentropy: 0.1427\n",
            "Epoch 00097: val_categorical_crossentropy improved from 0.19926 to 0.19400, saving model to col_0.19400__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1428 - categorical_crossentropy: 0.1428 - val_loss: 0.1940 - val_categorical_crossentropy: 0.1940 - lr: 0.0100\n",
            "Epoch 98/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.1396 - categorical_crossentropy: 0.1396\n",
            "Epoch 00098: val_categorical_crossentropy improved from 0.19400 to 0.19292, saving model to col_0.19292__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1397 - categorical_crossentropy: 0.1397 - val_loss: 0.1929 - val_categorical_crossentropy: 0.1929 - lr: 0.0100\n",
            "Epoch 99/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.1370 - categorical_crossentropy: 0.1370\n",
            "Epoch 00099: val_categorical_crossentropy improved from 0.19292 to 0.18831, saving model to col_0.18831__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1370 - categorical_crossentropy: 0.1370 - val_loss: 0.1883 - val_categorical_crossentropy: 0.1883 - lr: 0.0100\n",
            "Epoch 100/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.1344 - categorical_crossentropy: 0.1344\n",
            "Epoch 00100: val_categorical_crossentropy improved from 0.18831 to 0.18556, saving model to col_0.18556__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1344 - categorical_crossentropy: 0.1344 - val_loss: 0.1856 - val_categorical_crossentropy: 0.1856 - lr: 0.0100\n",
            "Epoch 101/150\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.1313 - categorical_crossentropy: 0.1313\n",
            "Epoch 00101: val_categorical_crossentropy did not improve from 0.18556\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1314 - categorical_crossentropy: 0.1314 - val_loss: 0.1860 - val_categorical_crossentropy: 0.1860 - lr: 0.0100\n",
            "Epoch 102/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.1280 - categorical_crossentropy: 0.1280\n",
            "Epoch 00102: val_categorical_crossentropy improved from 0.18556 to 0.17925, saving model to col_0.17925__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1282 - categorical_crossentropy: 0.1282 - val_loss: 0.1793 - val_categorical_crossentropy: 0.1793 - lr: 0.0100\n",
            "Epoch 103/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.1250 - categorical_crossentropy: 0.1250\n",
            "Epoch 00103: val_categorical_crossentropy did not improve from 0.17925\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1253 - categorical_crossentropy: 0.1253 - val_loss: 0.1824 - val_categorical_crossentropy: 0.1824 - lr: 0.0100\n",
            "Epoch 104/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.1224 - categorical_crossentropy: 0.1224\n",
            "Epoch 00104: val_categorical_crossentropy did not improve from 0.17925\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1224 - categorical_crossentropy: 0.1224 - val_loss: 0.1875 - val_categorical_crossentropy: 0.1875 - lr: 0.0100\n",
            "Epoch 105/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.1199 - categorical_crossentropy: 0.1199\n",
            "Epoch 00105: val_categorical_crossentropy did not improve from 0.17925\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1200 - categorical_crossentropy: 0.1200 - val_loss: 0.1904 - val_categorical_crossentropy: 0.1904 - lr: 0.0100\n",
            "Epoch 106/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.1181 - categorical_crossentropy: 0.1181\n",
            "Epoch 00106: val_categorical_crossentropy did not improve from 0.17925\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1182 - categorical_crossentropy: 0.1182 - val_loss: 0.1861 - val_categorical_crossentropy: 0.1861 - lr: 0.0100\n",
            "Epoch 107/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.1160 - categorical_crossentropy: 0.1160\n",
            "Epoch 00107: val_categorical_crossentropy did not improve from 0.17925\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1161 - categorical_crossentropy: 0.1161 - val_loss: 0.1799 - val_categorical_crossentropy: 0.1799 - lr: 0.0100\n",
            "Epoch 108/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.1139 - categorical_crossentropy: 0.1139\n",
            "Epoch 00108: val_categorical_crossentropy improved from 0.17925 to 0.16964, saving model to col_0.16964__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1141 - categorical_crossentropy: 0.1141 - val_loss: 0.1696 - val_categorical_crossentropy: 0.1696 - lr: 0.0100\n",
            "Epoch 109/150\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.1114 - categorical_crossentropy: 0.1114\n",
            "Epoch 00109: val_categorical_crossentropy did not improve from 0.16964\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1116 - categorical_crossentropy: 0.1116 - val_loss: 0.1776 - val_categorical_crossentropy: 0.1776 - lr: 0.0100\n",
            "Epoch 110/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.1087 - categorical_crossentropy: 0.1087\n",
            "Epoch 00110: val_categorical_crossentropy did not improve from 0.16964\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1089 - categorical_crossentropy: 0.1089 - val_loss: 0.1792 - val_categorical_crossentropy: 0.1792 - lr: 0.0100\n",
            "Epoch 111/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.1069 - categorical_crossentropy: 0.1069\n",
            "Epoch 00111: val_categorical_crossentropy did not improve from 0.16964\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1071 - categorical_crossentropy: 0.1071 - val_loss: 0.1741 - val_categorical_crossentropy: 0.1741 - lr: 0.0100\n",
            "Epoch 112/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.1051 - categorical_crossentropy: 0.1051\n",
            "Epoch 00112: val_categorical_crossentropy did not improve from 0.16964\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1053 - categorical_crossentropy: 0.1053 - val_loss: 0.1820 - val_categorical_crossentropy: 0.1820 - lr: 0.0100\n",
            "Epoch 113/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.1032 - categorical_crossentropy: 0.1032\n",
            "Epoch 00113: val_categorical_crossentropy did not improve from 0.16964\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1035 - categorical_crossentropy: 0.1035 - val_loss: 0.1746 - val_categorical_crossentropy: 0.1746 - lr: 0.0100\n",
            "Epoch 114/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.1016 - categorical_crossentropy: 0.1016\n",
            "Epoch 00114: val_categorical_crossentropy improved from 0.16964 to 0.16578, saving model to col_0.16578__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1018 - categorical_crossentropy: 0.1018 - val_loss: 0.1658 - val_categorical_crossentropy: 0.1658 - lr: 0.0100\n",
            "Epoch 115/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.0995 - categorical_crossentropy: 0.0995\n",
            "Epoch 00115: val_categorical_crossentropy improved from 0.16578 to 0.15689, saving model to col_0.15689__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0996 - categorical_crossentropy: 0.0996 - val_loss: 0.1569 - val_categorical_crossentropy: 0.1569 - lr: 0.0100\n",
            "Epoch 116/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.0974 - categorical_crossentropy: 0.0974\n",
            "Epoch 00116: val_categorical_crossentropy improved from 0.15689 to 0.15147, saving model to col_0.15147__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0977 - categorical_crossentropy: 0.0977 - val_loss: 0.1515 - val_categorical_crossentropy: 0.1515 - lr: 0.0100\n",
            "Epoch 117/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.0957 - categorical_crossentropy: 0.0957\n",
            "Epoch 00117: val_categorical_crossentropy improved from 0.15147 to 0.14955, saving model to col_0.14955__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0959 - categorical_crossentropy: 0.0959 - val_loss: 0.1495 - val_categorical_crossentropy: 0.1495 - lr: 0.0100\n",
            "Epoch 118/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.0941 - categorical_crossentropy: 0.0941\n",
            "Epoch 00118: val_categorical_crossentropy improved from 0.14955 to 0.14550, saving model to col_0.14550__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0943 - categorical_crossentropy: 0.0943 - val_loss: 0.1455 - val_categorical_crossentropy: 0.1455 - lr: 0.0100\n",
            "Epoch 119/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0930 - categorical_crossentropy: 0.0930\n",
            "Epoch 00119: val_categorical_crossentropy improved from 0.14550 to 0.14241, saving model to col_0.14241__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0932 - categorical_crossentropy: 0.0932 - val_loss: 0.1424 - val_categorical_crossentropy: 0.1424 - lr: 0.0100\n",
            "Epoch 120/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.0913 - categorical_crossentropy: 0.0913\n",
            "Epoch 00120: val_categorical_crossentropy did not improve from 0.14241\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0914 - categorical_crossentropy: 0.0914 - val_loss: 0.1506 - val_categorical_crossentropy: 0.1506 - lr: 0.0100\n",
            "Epoch 121/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0893 - categorical_crossentropy: 0.0893\n",
            "Epoch 00121: val_categorical_crossentropy did not improve from 0.14241\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0896 - categorical_crossentropy: 0.0896 - val_loss: 0.1459 - val_categorical_crossentropy: 0.1459 - lr: 0.0100\n",
            "Epoch 122/150\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.0885 - categorical_crossentropy: 0.0885\n",
            "Epoch 00122: val_categorical_crossentropy improved from 0.14241 to 0.13770, saving model to col_0.13770__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0887 - categorical_crossentropy: 0.0887 - val_loss: 0.1377 - val_categorical_crossentropy: 0.1377 - lr: 0.0100\n",
            "Epoch 123/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0868 - categorical_crossentropy: 0.0868\n",
            "Epoch 00123: val_categorical_crossentropy improved from 0.13770 to 0.13009, saving model to col_0.13009__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0870 - categorical_crossentropy: 0.0870 - val_loss: 0.1301 - val_categorical_crossentropy: 0.1301 - lr: 0.0100\n",
            "Epoch 124/150\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.0858 - categorical_crossentropy: 0.0858\n",
            "Epoch 00124: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0862 - categorical_crossentropy: 0.0862 - val_loss: 0.1327 - val_categorical_crossentropy: 0.1327 - lr: 0.0100\n",
            "Epoch 125/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0855 - categorical_crossentropy: 0.0855\n",
            "Epoch 00125: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0857 - categorical_crossentropy: 0.0857 - val_loss: 0.1390 - val_categorical_crossentropy: 0.1390 - lr: 0.0100\n",
            "Epoch 126/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.0831 - categorical_crossentropy: 0.0831\n",
            "Epoch 00126: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 9s 2ms/step - loss: 0.0830 - categorical_crossentropy: 0.0830 - val_loss: 0.1363 - val_categorical_crossentropy: 0.1363 - lr: 0.0100\n",
            "Epoch 127/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0818 - categorical_crossentropy: 0.0818\n",
            "Epoch 00127: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0820 - categorical_crossentropy: 0.0820 - val_loss: 0.1398 - val_categorical_crossentropy: 0.1398 - lr: 0.0100\n",
            "Epoch 128/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.0810 - categorical_crossentropy: 0.0810\n",
            "Epoch 00128: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0810 - categorical_crossentropy: 0.0810 - val_loss: 0.1345 - val_categorical_crossentropy: 0.1345 - lr: 0.0100\n",
            "Epoch 129/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0785 - categorical_crossentropy: 0.0785\n",
            "Epoch 00129: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0787 - categorical_crossentropy: 0.0787 - val_loss: 0.1328 - val_categorical_crossentropy: 0.1328 - lr: 0.0100\n",
            "Epoch 130/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0782 - categorical_crossentropy: 0.0782\n",
            "Epoch 00130: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0785 - categorical_crossentropy: 0.0785 - val_loss: 0.1387 - val_categorical_crossentropy: 0.1387 - lr: 0.0100\n",
            "Epoch 131/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.0769 - categorical_crossentropy: 0.0769\n",
            "Epoch 00131: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0772 - categorical_crossentropy: 0.0772 - val_loss: 0.1349 - val_categorical_crossentropy: 0.1349 - lr: 0.0100\n",
            "Epoch 132/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0755 - categorical_crossentropy: 0.0755\n",
            "Epoch 00132: val_categorical_crossentropy did not improve from 0.13009\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0757 - categorical_crossentropy: 0.0757 - val_loss: 0.1322 - val_categorical_crossentropy: 0.1322 - lr: 0.0100\n",
            "Epoch 133/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.0750 - categorical_crossentropy: 0.0750\n",
            "Epoch 00133: val_categorical_crossentropy improved from 0.13009 to 0.12290, saving model to col_0.12290__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0752 - categorical_crossentropy: 0.0752 - val_loss: 0.1229 - val_categorical_crossentropy: 0.1229 - lr: 0.0100\n",
            "Epoch 134/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.0736 - categorical_crossentropy: 0.0736\n",
            "Epoch 00134: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0739 - categorical_crossentropy: 0.0739 - val_loss: 0.1322 - val_categorical_crossentropy: 0.1322 - lr: 0.0100\n",
            "Epoch 135/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.0712 - categorical_crossentropy: 0.0712\n",
            "Epoch 00135: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 9s 2ms/step - loss: 0.0715 - categorical_crossentropy: 0.0715 - val_loss: 0.1287 - val_categorical_crossentropy: 0.1287 - lr: 0.0100\n",
            "Epoch 136/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0725 - categorical_crossentropy: 0.0725\n",
            "Epoch 00136: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0728 - categorical_crossentropy: 0.0728 - val_loss: 0.1256 - val_categorical_crossentropy: 0.1256 - lr: 0.0100\n",
            "Epoch 137/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.0712 - categorical_crossentropy: 0.0712\n",
            "Epoch 00137: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0712 - categorical_crossentropy: 0.0712 - val_loss: 0.1353 - val_categorical_crossentropy: 0.1353 - lr: 0.0100\n",
            "Epoch 138/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0694 - categorical_crossentropy: 0.0694\n",
            "Epoch 00138: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0698 - categorical_crossentropy: 0.0698 - val_loss: 0.1253 - val_categorical_crossentropy: 0.1253 - lr: 0.0100\n",
            "Epoch 139/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0693 - categorical_crossentropy: 0.0693\n",
            "Epoch 00139: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0696 - categorical_crossentropy: 0.0696 - val_loss: 0.1450 - val_categorical_crossentropy: 0.1450 - lr: 0.0100\n",
            "Epoch 140/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0679 - categorical_crossentropy: 0.0679\n",
            "Epoch 00140: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0681 - categorical_crossentropy: 0.0681 - val_loss: 0.1330 - val_categorical_crossentropy: 0.1330 - lr: 0.0100\n",
            "Epoch 141/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.0672 - categorical_crossentropy: 0.0672\n",
            "Epoch 00141: val_categorical_crossentropy did not improve from 0.12290\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0674 - categorical_crossentropy: 0.0674 - val_loss: 0.1268 - val_categorical_crossentropy: 0.1268 - lr: 0.0100\n",
            "Epoch 142/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0655 - categorical_crossentropy: 0.0655\n",
            "Epoch 00142: val_categorical_crossentropy improved from 0.12290 to 0.11989, saving model to col_0.11989__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0657 - categorical_crossentropy: 0.0657 - val_loss: 0.1199 - val_categorical_crossentropy: 0.1199 - lr: 0.0100\n",
            "Epoch 143/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0655 - categorical_crossentropy: 0.0655\n",
            "Epoch 00143: val_categorical_crossentropy did not improve from 0.11989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0658 - categorical_crossentropy: 0.0658 - val_loss: 0.1343 - val_categorical_crossentropy: 0.1343 - lr: 0.0100\n",
            "Epoch 144/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.0630 - categorical_crossentropy: 0.0630\n",
            "Epoch 00144: val_categorical_crossentropy did not improve from 0.11989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0633 - categorical_crossentropy: 0.0633 - val_loss: 0.1528 - val_categorical_crossentropy: 0.1528 - lr: 0.0100\n",
            "Epoch 145/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.0640 - categorical_crossentropy: 0.0640\n",
            "Epoch 00145: val_categorical_crossentropy improved from 0.11989 to 0.10246, saving model to col_0.10246__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0642 - categorical_crossentropy: 0.0642 - val_loss: 0.1025 - val_categorical_crossentropy: 0.1025 - lr: 0.0100\n",
            "Epoch 146/150\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.0618 - categorical_crossentropy: 0.0618\n",
            "Epoch 00146: val_categorical_crossentropy did not improve from 0.10246\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0621 - categorical_crossentropy: 0.0621 - val_loss: 0.1298 - val_categorical_crossentropy: 0.1298 - lr: 0.0100\n",
            "Epoch 147/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.0597 - categorical_crossentropy: 0.0597\n",
            "Epoch 00147: val_categorical_crossentropy did not improve from 0.10246\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0600 - categorical_crossentropy: 0.0600 - val_loss: 0.1380 - val_categorical_crossentropy: 0.1380 - lr: 0.0100\n",
            "Epoch 148/150\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.0610 - categorical_crossentropy: 0.0610\n",
            "Epoch 00148: val_categorical_crossentropy did not improve from 0.10246\n",
            "3942/3942 [==============================] - 9s 2ms/step - loss: 0.0612 - categorical_crossentropy: 0.0612 - val_loss: 0.1182 - val_categorical_crossentropy: 0.1182 - lr: 0.0100\n",
            "Epoch 149/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0580 - categorical_crossentropy: 0.0580\n",
            "Epoch 00149: val_categorical_crossentropy did not improve from 0.10246\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0582 - categorical_crossentropy: 0.0582 - val_loss: 0.1155 - val_categorical_crossentropy: 0.1155 - lr: 0.0100\n",
            "Epoch 150/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0576 - categorical_crossentropy: 0.0576\n",
            "Epoch 00150: val_categorical_crossentropy did not improve from 0.10246\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0579 - categorical_crossentropy: 0.0579 - val_loss: 0.1395 - val_categorical_crossentropy: 0.1395 - lr: 0.0100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4fad211e50>"
            ]
          },
          "metadata": {},
          "execution_count": 157
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQzTylLI8x1b"
      },
      "source": [
        ""
      ],
      "execution_count": 157,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "clK4Ff7Q8yNi"
      },
      "source": [
        "model_name=\"hemo\"\n",
        "def scheduler(epoch, lr):\n",
        "    return 0.01\n",
        "\n",
        "callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "callbacks = [callback_LR,\n",
        "        \n",
        "        #savemodela,\n",
        "        ModelCheckpoint(filepath=model_name+\"_{val_categorical_crossentropy:.5f}__.hdf5\", monitor='val_categorical_crossentropy',\n",
        "                        verbose=2, save_best_only=True, mode='min')]"
      ],
      "execution_count": 158,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEj94ZvU8yNi",
        "outputId": "aea0ebdf-3535-4144-dc31-f15fddad3258"
      },
      "source": [
        "hemo_model=blood_model()"
      ],
      "execution_count": 159,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ISspsn5U8yNj",
        "outputId": "bc6ad32a-e16e-4d28-fa05-7df4e1e275a6"
      },
      "source": [
        "hemo_model.fit(X,\n",
        "          y_hemo,\n",
        "          epochs=150, \n",
        "          batch_size=3,\n",
        "          validation_split=0.1,\n",
        "          verbose=1,\n",
        "          callbacks=callbacks,\n",
        "          shuffle=False\n",
        "          \n",
        "          )"
      ],
      "execution_count": 160,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.4260 - categorical_crossentropy: 0.4260\n",
            "Epoch 00001: val_categorical_crossentropy improved from inf to 0.43003, saving model to hemo_0.43003__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.4254 - categorical_crossentropy: 0.4254 - val_loss: 0.4300 - val_categorical_crossentropy: 0.4300 - lr: 0.0100\n",
            "Epoch 2/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.4129 - categorical_crossentropy: 0.4129\n",
            "Epoch 00002: val_categorical_crossentropy improved from 0.43003 to 0.42237, saving model to hemo_0.42237__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4127 - categorical_crossentropy: 0.4127 - val_loss: 0.4224 - val_categorical_crossentropy: 0.4224 - lr: 0.0100\n",
            "Epoch 3/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.4057 - categorical_crossentropy: 0.4057\n",
            "Epoch 00003: val_categorical_crossentropy improved from 0.42237 to 0.41116, saving model to hemo_0.41116__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.4051 - categorical_crossentropy: 0.4051 - val_loss: 0.4112 - val_categorical_crossentropy: 0.4112 - lr: 0.0100\n",
            "Epoch 4/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.3960 - categorical_crossentropy: 0.3960\n",
            "Epoch 00004: val_categorical_crossentropy improved from 0.41116 to 0.39707, saving model to hemo_0.39707__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3956 - categorical_crossentropy: 0.3956 - val_loss: 0.3971 - val_categorical_crossentropy: 0.3971 - lr: 0.0100\n",
            "Epoch 5/150\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.3816 - categorical_crossentropy: 0.3816\n",
            "Epoch 00005: val_categorical_crossentropy improved from 0.39707 to 0.38315, saving model to hemo_0.38315__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3811 - categorical_crossentropy: 0.3811 - val_loss: 0.3832 - val_categorical_crossentropy: 0.3832 - lr: 0.0100\n",
            "Epoch 6/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.3677 - categorical_crossentropy: 0.3677\n",
            "Epoch 00006: val_categorical_crossentropy improved from 0.38315 to 0.37409, saving model to hemo_0.37409__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3677 - categorical_crossentropy: 0.3677 - val_loss: 0.3741 - val_categorical_crossentropy: 0.3741 - lr: 0.0100\n",
            "Epoch 7/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.3579 - categorical_crossentropy: 0.3579\n",
            "Epoch 00007: val_categorical_crossentropy improved from 0.37409 to 0.36677, saving model to hemo_0.36677__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3578 - categorical_crossentropy: 0.3578 - val_loss: 0.3668 - val_categorical_crossentropy: 0.3668 - lr: 0.0100\n",
            "Epoch 8/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.3492 - categorical_crossentropy: 0.3492\n",
            "Epoch 00008: val_categorical_crossentropy improved from 0.36677 to 0.35973, saving model to hemo_0.35973__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3490 - categorical_crossentropy: 0.3490 - val_loss: 0.3597 - val_categorical_crossentropy: 0.3597 - lr: 0.0100\n",
            "Epoch 9/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.3407 - categorical_crossentropy: 0.3407\n",
            "Epoch 00009: val_categorical_crossentropy improved from 0.35973 to 0.35438, saving model to hemo_0.35438__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3404 - categorical_crossentropy: 0.3404 - val_loss: 0.3544 - val_categorical_crossentropy: 0.3544 - lr: 0.0100\n",
            "Epoch 10/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.3309 - categorical_crossentropy: 0.3309\n",
            "Epoch 00010: val_categorical_crossentropy improved from 0.35438 to 0.34756, saving model to hemo_0.34756__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.3309 - categorical_crossentropy: 0.3309 - val_loss: 0.3476 - val_categorical_crossentropy: 0.3476 - lr: 0.0100\n",
            "Epoch 11/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.3213 - categorical_crossentropy: 0.3213\n",
            "Epoch 00011: val_categorical_crossentropy improved from 0.34756 to 0.33730, saving model to hemo_0.33730__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3212 - categorical_crossentropy: 0.3212 - val_loss: 0.3373 - val_categorical_crossentropy: 0.3373 - lr: 0.0100\n",
            "Epoch 12/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.3117 - categorical_crossentropy: 0.3117\n",
            "Epoch 00012: val_categorical_crossentropy improved from 0.33730 to 0.32667, saving model to hemo_0.32667__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3114 - categorical_crossentropy: 0.3114 - val_loss: 0.3267 - val_categorical_crossentropy: 0.3267 - lr: 0.0100\n",
            "Epoch 13/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.3023 - categorical_crossentropy: 0.3023\n",
            "Epoch 00013: val_categorical_crossentropy improved from 0.32667 to 0.31697, saving model to hemo_0.31697__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.3021 - categorical_crossentropy: 0.3021 - val_loss: 0.3170 - val_categorical_crossentropy: 0.3170 - lr: 0.0100\n",
            "Epoch 14/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.2934 - categorical_crossentropy: 0.2934\n",
            "Epoch 00014: val_categorical_crossentropy improved from 0.31697 to 0.30830, saving model to hemo_0.30830__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2935 - categorical_crossentropy: 0.2935 - val_loss: 0.3083 - val_categorical_crossentropy: 0.3083 - lr: 0.0100\n",
            "Epoch 15/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.2854 - categorical_crossentropy: 0.2854\n",
            "Epoch 00015: val_categorical_crossentropy improved from 0.30830 to 0.29973, saving model to hemo_0.29973__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2854 - categorical_crossentropy: 0.2854 - val_loss: 0.2997 - val_categorical_crossentropy: 0.2997 - lr: 0.0100\n",
            "Epoch 16/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.2780 - categorical_crossentropy: 0.2780\n",
            "Epoch 00016: val_categorical_crossentropy improved from 0.29973 to 0.29115, saving model to hemo_0.29115__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2780 - categorical_crossentropy: 0.2780 - val_loss: 0.2912 - val_categorical_crossentropy: 0.2912 - lr: 0.0100\n",
            "Epoch 17/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.2709 - categorical_crossentropy: 0.2709\n",
            "Epoch 00017: val_categorical_crossentropy improved from 0.29115 to 0.28308, saving model to hemo_0.28308__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2711 - categorical_crossentropy: 0.2711 - val_loss: 0.2831 - val_categorical_crossentropy: 0.2831 - lr: 0.0100\n",
            "Epoch 18/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.2645 - categorical_crossentropy: 0.2645\n",
            "Epoch 00018: val_categorical_crossentropy improved from 0.28308 to 0.27582, saving model to hemo_0.27582__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2646 - categorical_crossentropy: 0.2646 - val_loss: 0.2758 - val_categorical_crossentropy: 0.2758 - lr: 0.0100\n",
            "Epoch 19/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.2585 - categorical_crossentropy: 0.2585\n",
            "Epoch 00019: val_categorical_crossentropy improved from 0.27582 to 0.26928, saving model to hemo_0.26928__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2585 - categorical_crossentropy: 0.2585 - val_loss: 0.2693 - val_categorical_crossentropy: 0.2693 - lr: 0.0100\n",
            "Epoch 20/150\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.2526 - categorical_crossentropy: 0.2526\n",
            "Epoch 00020: val_categorical_crossentropy improved from 0.26928 to 0.26333, saving model to hemo_0.26333__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2527 - categorical_crossentropy: 0.2527 - val_loss: 0.2633 - val_categorical_crossentropy: 0.2633 - lr: 0.0100\n",
            "Epoch 21/150\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.2473 - categorical_crossentropy: 0.2473\n",
            "Epoch 00021: val_categorical_crossentropy improved from 0.26333 to 0.25755, saving model to hemo_0.25755__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2473 - categorical_crossentropy: 0.2473 - val_loss: 0.2576 - val_categorical_crossentropy: 0.2576 - lr: 0.0100\n",
            "Epoch 22/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.2422 - categorical_crossentropy: 0.2422\n",
            "Epoch 00022: val_categorical_crossentropy improved from 0.25755 to 0.25189, saving model to hemo_0.25189__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2423 - categorical_crossentropy: 0.2423 - val_loss: 0.2519 - val_categorical_crossentropy: 0.2519 - lr: 0.0100\n",
            "Epoch 23/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.2375 - categorical_crossentropy: 0.2375\n",
            "Epoch 00023: val_categorical_crossentropy improved from 0.25189 to 0.24642, saving model to hemo_0.24642__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2374 - categorical_crossentropy: 0.2374 - val_loss: 0.2464 - val_categorical_crossentropy: 0.2464 - lr: 0.0100\n",
            "Epoch 24/150\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.2325 - categorical_crossentropy: 0.2325\n",
            "Epoch 00024: val_categorical_crossentropy improved from 0.24642 to 0.24136, saving model to hemo_0.24136__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2327 - categorical_crossentropy: 0.2327 - val_loss: 0.2414 - val_categorical_crossentropy: 0.2414 - lr: 0.0100\n",
            "Epoch 25/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.2279 - categorical_crossentropy: 0.2279\n",
            "Epoch 00025: val_categorical_crossentropy improved from 0.24136 to 0.23676, saving model to hemo_0.23676__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2282 - categorical_crossentropy: 0.2282 - val_loss: 0.2368 - val_categorical_crossentropy: 0.2368 - lr: 0.0100\n",
            "Epoch 26/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.2239 - categorical_crossentropy: 0.2239\n",
            "Epoch 00026: val_categorical_crossentropy improved from 0.23676 to 0.23274, saving model to hemo_0.23274__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2239 - categorical_crossentropy: 0.2239 - val_loss: 0.2327 - val_categorical_crossentropy: 0.2327 - lr: 0.0100\n",
            "Epoch 27/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.2198 - categorical_crossentropy: 0.2198\n",
            "Epoch 00027: val_categorical_crossentropy improved from 0.23274 to 0.22931, saving model to hemo_0.22931__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2198 - categorical_crossentropy: 0.2198 - val_loss: 0.2293 - val_categorical_crossentropy: 0.2293 - lr: 0.0100\n",
            "Epoch 28/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.2157 - categorical_crossentropy: 0.2157\n",
            "Epoch 00028: val_categorical_crossentropy improved from 0.22931 to 0.22622, saving model to hemo_0.22622__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2158 - categorical_crossentropy: 0.2158 - val_loss: 0.2262 - val_categorical_crossentropy: 0.2262 - lr: 0.0100\n",
            "Epoch 29/150\n",
            "3916/3942 [============================>.] - ETA: 0s - loss: 0.2117 - categorical_crossentropy: 0.2117\n",
            "Epoch 00029: val_categorical_crossentropy improved from 0.22622 to 0.22346, saving model to hemo_0.22346__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2118 - categorical_crossentropy: 0.2118 - val_loss: 0.2235 - val_categorical_crossentropy: 0.2235 - lr: 0.0100\n",
            "Epoch 30/150\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.2078 - categorical_crossentropy: 0.2078\n",
            "Epoch 00030: val_categorical_crossentropy improved from 0.22346 to 0.22263, saving model to hemo_0.22263__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2080 - categorical_crossentropy: 0.2080 - val_loss: 0.2226 - val_categorical_crossentropy: 0.2226 - lr: 0.0100\n",
            "Epoch 31/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.2042 - categorical_crossentropy: 0.2042\n",
            "Epoch 00031: val_categorical_crossentropy improved from 0.22263 to 0.22057, saving model to hemo_0.22057__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2042 - categorical_crossentropy: 0.2042 - val_loss: 0.2206 - val_categorical_crossentropy: 0.2206 - lr: 0.0100\n",
            "Epoch 32/150\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.2006 - categorical_crossentropy: 0.2006\n",
            "Epoch 00032: val_categorical_crossentropy did not improve from 0.22057\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.2005 - categorical_crossentropy: 0.2005 - val_loss: 0.2235 - val_categorical_crossentropy: 0.2235 - lr: 0.0100\n",
            "Epoch 33/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.1971 - categorical_crossentropy: 0.1971\n",
            "Epoch 00033: val_categorical_crossentropy did not improve from 0.22057\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1970 - categorical_crossentropy: 0.1970 - val_loss: 0.2218 - val_categorical_crossentropy: 0.2218 - lr: 0.0100\n",
            "Epoch 34/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.1934 - categorical_crossentropy: 0.1934\n",
            "Epoch 00034: val_categorical_crossentropy improved from 0.22057 to 0.21822, saving model to hemo_0.21822__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1935 - categorical_crossentropy: 0.1935 - val_loss: 0.2182 - val_categorical_crossentropy: 0.2182 - lr: 0.0100\n",
            "Epoch 35/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.1901 - categorical_crossentropy: 0.1901\n",
            "Epoch 00035: val_categorical_crossentropy improved from 0.21822 to 0.21424, saving model to hemo_0.21424__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1902 - categorical_crossentropy: 0.1902 - val_loss: 0.2142 - val_categorical_crossentropy: 0.2142 - lr: 0.0100\n",
            "Epoch 36/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.1869 - categorical_crossentropy: 0.1869\n",
            "Epoch 00036: val_categorical_crossentropy improved from 0.21424 to 0.21060, saving model to hemo_0.21060__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1868 - categorical_crossentropy: 0.1868 - val_loss: 0.2106 - val_categorical_crossentropy: 0.2106 - lr: 0.0100\n",
            "Epoch 37/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.1834 - categorical_crossentropy: 0.1834\n",
            "Epoch 00037: val_categorical_crossentropy improved from 0.21060 to 0.20827, saving model to hemo_0.20827__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1833 - categorical_crossentropy: 0.1833 - val_loss: 0.2083 - val_categorical_crossentropy: 0.2083 - lr: 0.0100\n",
            "Epoch 38/150\n",
            "3910/3942 [============================>.] - ETA: 0s - loss: 0.1798 - categorical_crossentropy: 0.1798\n",
            "Epoch 00038: val_categorical_crossentropy improved from 0.20827 to 0.20508, saving model to hemo_0.20508__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1800 - categorical_crossentropy: 0.1800 - val_loss: 0.2051 - val_categorical_crossentropy: 0.2051 - lr: 0.0100\n",
            "Epoch 39/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.1763 - categorical_crossentropy: 0.1763\n",
            "Epoch 00039: val_categorical_crossentropy improved from 0.20508 to 0.20155, saving model to hemo_0.20155__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1765 - categorical_crossentropy: 0.1765 - val_loss: 0.2015 - val_categorical_crossentropy: 0.2015 - lr: 0.0100\n",
            "Epoch 40/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.1730 - categorical_crossentropy: 0.1730\n",
            "Epoch 00040: val_categorical_crossentropy improved from 0.20155 to 0.19778, saving model to hemo_0.19778__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1732 - categorical_crossentropy: 0.1732 - val_loss: 0.1978 - val_categorical_crossentropy: 0.1978 - lr: 0.0100\n",
            "Epoch 41/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.1696 - categorical_crossentropy: 0.1696\n",
            "Epoch 00041: val_categorical_crossentropy improved from 0.19778 to 0.19457, saving model to hemo_0.19457__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1699 - categorical_crossentropy: 0.1699 - val_loss: 0.1946 - val_categorical_crossentropy: 0.1946 - lr: 0.0100\n",
            "Epoch 42/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.1665 - categorical_crossentropy: 0.1665\n",
            "Epoch 00042: val_categorical_crossentropy improved from 0.19457 to 0.19200, saving model to hemo_0.19200__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1666 - categorical_crossentropy: 0.1666 - val_loss: 0.1920 - val_categorical_crossentropy: 0.1920 - lr: 0.0100\n",
            "Epoch 43/150\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.1632 - categorical_crossentropy: 0.1632\n",
            "Epoch 00043: val_categorical_crossentropy improved from 0.19200 to 0.18862, saving model to hemo_0.18862__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1633 - categorical_crossentropy: 0.1633 - val_loss: 0.1886 - val_categorical_crossentropy: 0.1886 - lr: 0.0100\n",
            "Epoch 44/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.1598 - categorical_crossentropy: 0.1598\n",
            "Epoch 00044: val_categorical_crossentropy improved from 0.18862 to 0.18449, saving model to hemo_0.18449__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1599 - categorical_crossentropy: 0.1599 - val_loss: 0.1845 - val_categorical_crossentropy: 0.1845 - lr: 0.0100\n",
            "Epoch 45/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.1563 - categorical_crossentropy: 0.1563\n",
            "Epoch 00045: val_categorical_crossentropy improved from 0.18449 to 0.18279, saving model to hemo_0.18279__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1565 - categorical_crossentropy: 0.1565 - val_loss: 0.1828 - val_categorical_crossentropy: 0.1828 - lr: 0.0100\n",
            "Epoch 46/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.1529 - categorical_crossentropy: 0.1529\n",
            "Epoch 00046: val_categorical_crossentropy improved from 0.18279 to 0.18204, saving model to hemo_0.18204__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1531 - categorical_crossentropy: 0.1531 - val_loss: 0.1820 - val_categorical_crossentropy: 0.1820 - lr: 0.0100\n",
            "Epoch 47/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.1499 - categorical_crossentropy: 0.1499\n",
            "Epoch 00047: val_categorical_crossentropy improved from 0.18204 to 0.17901, saving model to hemo_0.17901__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1497 - categorical_crossentropy: 0.1497 - val_loss: 0.1790 - val_categorical_crossentropy: 0.1790 - lr: 0.0100\n",
            "Epoch 48/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.1464 - categorical_crossentropy: 0.1464\n",
            "Epoch 00048: val_categorical_crossentropy improved from 0.17901 to 0.17568, saving model to hemo_0.17568__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1462 - categorical_crossentropy: 0.1462 - val_loss: 0.1757 - val_categorical_crossentropy: 0.1757 - lr: 0.0100\n",
            "Epoch 49/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.1427 - categorical_crossentropy: 0.1427\n",
            "Epoch 00049: val_categorical_crossentropy improved from 0.17568 to 0.17175, saving model to hemo_0.17175__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1428 - categorical_crossentropy: 0.1428 - val_loss: 0.1718 - val_categorical_crossentropy: 0.1718 - lr: 0.0100\n",
            "Epoch 50/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.1392 - categorical_crossentropy: 0.1392\n",
            "Epoch 00050: val_categorical_crossentropy improved from 0.17175 to 0.16738, saving model to hemo_0.16738__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1394 - categorical_crossentropy: 0.1394 - val_loss: 0.1674 - val_categorical_crossentropy: 0.1674 - lr: 0.0100\n",
            "Epoch 51/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.1359 - categorical_crossentropy: 0.1359\n",
            "Epoch 00051: val_categorical_crossentropy improved from 0.16738 to 0.16308, saving model to hemo_0.16308__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1361 - categorical_crossentropy: 0.1361 - val_loss: 0.1631 - val_categorical_crossentropy: 0.1631 - lr: 0.0100\n",
            "Epoch 52/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.1329 - categorical_crossentropy: 0.1329\n",
            "Epoch 00052: val_categorical_crossentropy improved from 0.16308 to 0.15897, saving model to hemo_0.15897__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1329 - categorical_crossentropy: 0.1329 - val_loss: 0.1590 - val_categorical_crossentropy: 0.1590 - lr: 0.0100\n",
            "Epoch 53/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.1298 - categorical_crossentropy: 0.1298\n",
            "Epoch 00053: val_categorical_crossentropy improved from 0.15897 to 0.15459, saving model to hemo_0.15459__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1298 - categorical_crossentropy: 0.1298 - val_loss: 0.1546 - val_categorical_crossentropy: 0.1546 - lr: 0.0100\n",
            "Epoch 54/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.1268 - categorical_crossentropy: 0.1268\n",
            "Epoch 00054: val_categorical_crossentropy improved from 0.15459 to 0.15005, saving model to hemo_0.15005__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1268 - categorical_crossentropy: 0.1268 - val_loss: 0.1500 - val_categorical_crossentropy: 0.1500 - lr: 0.0100\n",
            "Epoch 55/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.1238 - categorical_crossentropy: 0.1238\n",
            "Epoch 00055: val_categorical_crossentropy improved from 0.15005 to 0.14548, saving model to hemo_0.14548__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1238 - categorical_crossentropy: 0.1238 - val_loss: 0.1455 - val_categorical_crossentropy: 0.1455 - lr: 0.0100\n",
            "Epoch 56/150\n",
            "3913/3942 [============================>.] - ETA: 0s - loss: 0.1209 - categorical_crossentropy: 0.1209\n",
            "Epoch 00056: val_categorical_crossentropy improved from 0.14548 to 0.14155, saving model to hemo_0.14155__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1209 - categorical_crossentropy: 0.1209 - val_loss: 0.1416 - val_categorical_crossentropy: 0.1416 - lr: 0.0100\n",
            "Epoch 57/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.1179 - categorical_crossentropy: 0.1179\n",
            "Epoch 00057: val_categorical_crossentropy improved from 0.14155 to 0.13837, saving model to hemo_0.13837__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1180 - categorical_crossentropy: 0.1180 - val_loss: 0.1384 - val_categorical_crossentropy: 0.1384 - lr: 0.0100\n",
            "Epoch 58/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.1153 - categorical_crossentropy: 0.1153\n",
            "Epoch 00058: val_categorical_crossentropy improved from 0.13837 to 0.13506, saving model to hemo_0.13506__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1153 - categorical_crossentropy: 0.1153 - val_loss: 0.1351 - val_categorical_crossentropy: 0.1351 - lr: 0.0100\n",
            "Epoch 59/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.1124 - categorical_crossentropy: 0.1124\n",
            "Epoch 00059: val_categorical_crossentropy improved from 0.13506 to 0.13093, saving model to hemo_0.13093__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1126 - categorical_crossentropy: 0.1126 - val_loss: 0.1309 - val_categorical_crossentropy: 0.1309 - lr: 0.0100\n",
            "Epoch 60/150\n",
            "3917/3942 [============================>.] - ETA: 0s - loss: 0.1099 - categorical_crossentropy: 0.1099\n",
            "Epoch 00060: val_categorical_crossentropy improved from 0.13093 to 0.12473, saving model to hemo_0.12473__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1099 - categorical_crossentropy: 0.1099 - val_loss: 0.1247 - val_categorical_crossentropy: 0.1247 - lr: 0.0100\n",
            "Epoch 61/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.1072 - categorical_crossentropy: 0.1072\n",
            "Epoch 00061: val_categorical_crossentropy improved from 0.12473 to 0.11760, saving model to hemo_0.11760__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1072 - categorical_crossentropy: 0.1072 - val_loss: 0.1176 - val_categorical_crossentropy: 0.1176 - lr: 0.0100\n",
            "Epoch 62/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.1047 - categorical_crossentropy: 0.1047\n",
            "Epoch 00062: val_categorical_crossentropy improved from 0.11760 to 0.10968, saving model to hemo_0.10968__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.1046 - categorical_crossentropy: 0.1046 - val_loss: 0.1097 - val_categorical_crossentropy: 0.1097 - lr: 0.0100\n",
            "Epoch 63/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.1021 - categorical_crossentropy: 0.1021\n",
            "Epoch 00063: val_categorical_crossentropy improved from 0.10968 to 0.10261, saving model to hemo_0.10261__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.1021 - categorical_crossentropy: 0.1021 - val_loss: 0.1026 - val_categorical_crossentropy: 0.1026 - lr: 0.0100\n",
            "Epoch 64/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.0995 - categorical_crossentropy: 0.0995\n",
            "Epoch 00064: val_categorical_crossentropy improved from 0.10261 to 0.09589, saving model to hemo_0.09589__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0997 - categorical_crossentropy: 0.0997 - val_loss: 0.0959 - val_categorical_crossentropy: 0.0959 - lr: 0.0100\n",
            "Epoch 65/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0972 - categorical_crossentropy: 0.0972\n",
            "Epoch 00065: val_categorical_crossentropy improved from 0.09589 to 0.09045, saving model to hemo_0.09045__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0973 - categorical_crossentropy: 0.0973 - val_loss: 0.0905 - val_categorical_crossentropy: 0.0905 - lr: 0.0100\n",
            "Epoch 66/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.0949 - categorical_crossentropy: 0.0949\n",
            "Epoch 00066: val_categorical_crossentropy improved from 0.09045 to 0.08709, saving model to hemo_0.08709__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0951 - categorical_crossentropy: 0.0951 - val_loss: 0.0871 - val_categorical_crossentropy: 0.0871 - lr: 0.0100\n",
            "Epoch 67/150\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.0922 - categorical_crossentropy: 0.0922\n",
            "Epoch 00067: val_categorical_crossentropy improved from 0.08709 to 0.08442, saving model to hemo_0.08442__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0923 - categorical_crossentropy: 0.0923 - val_loss: 0.0844 - val_categorical_crossentropy: 0.0844 - lr: 0.0100\n",
            "Epoch 68/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.0894 - categorical_crossentropy: 0.0894\n",
            "Epoch 00068: val_categorical_crossentropy improved from 0.08442 to 0.08175, saving model to hemo_0.08175__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.0895 - categorical_crossentropy: 0.0895 - val_loss: 0.0818 - val_categorical_crossentropy: 0.0818 - lr: 0.0100\n",
            "Epoch 69/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.0870 - categorical_crossentropy: 0.0870\n",
            "Epoch 00069: val_categorical_crossentropy improved from 0.08175 to 0.07925, saving model to hemo_0.07925__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0870 - categorical_crossentropy: 0.0870 - val_loss: 0.0793 - val_categorical_crossentropy: 0.0793 - lr: 0.0100\n",
            "Epoch 70/150\n",
            "3931/3942 [============================>.] - ETA: 0s - loss: 0.0846 - categorical_crossentropy: 0.0846\n",
            "Epoch 00070: val_categorical_crossentropy improved from 0.07925 to 0.07695, saving model to hemo_0.07695__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.0847 - categorical_crossentropy: 0.0847 - val_loss: 0.0769 - val_categorical_crossentropy: 0.0769 - lr: 0.0100\n",
            "Epoch 71/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.0823 - categorical_crossentropy: 0.0823\n",
            "Epoch 00071: val_categorical_crossentropy improved from 0.07695 to 0.07471, saving model to hemo_0.07471__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0824 - categorical_crossentropy: 0.0824 - val_loss: 0.0747 - val_categorical_crossentropy: 0.0747 - lr: 0.0100\n",
            "Epoch 72/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0798 - categorical_crossentropy: 0.0798\n",
            "Epoch 00072: val_categorical_crossentropy improved from 0.07471 to 0.07265, saving model to hemo_0.07265__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0799 - categorical_crossentropy: 0.0799 - val_loss: 0.0727 - val_categorical_crossentropy: 0.0727 - lr: 0.0100\n",
            "Epoch 73/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.0776 - categorical_crossentropy: 0.0776\n",
            "Epoch 00073: val_categorical_crossentropy improved from 0.07265 to 0.07107, saving model to hemo_0.07107__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0777 - categorical_crossentropy: 0.0777 - val_loss: 0.0711 - val_categorical_crossentropy: 0.0711 - lr: 0.0100\n",
            "Epoch 74/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.0759 - categorical_crossentropy: 0.0759\n",
            "Epoch 00074: val_categorical_crossentropy improved from 0.07107 to 0.06910, saving model to hemo_0.06910__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0759 - categorical_crossentropy: 0.0759 - val_loss: 0.0691 - val_categorical_crossentropy: 0.0691 - lr: 0.0100\n",
            "Epoch 75/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.0737 - categorical_crossentropy: 0.0737\n",
            "Epoch 00075: val_categorical_crossentropy did not improve from 0.06910\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.0739 - categorical_crossentropy: 0.0739 - val_loss: 0.0824 - val_categorical_crossentropy: 0.0824 - lr: 0.0100\n",
            "Epoch 76/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.0725 - categorical_crossentropy: 0.0725\n",
            "Epoch 00076: val_categorical_crossentropy improved from 0.06910 to 0.06756, saving model to hemo_0.06756__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0725 - categorical_crossentropy: 0.0725 - val_loss: 0.0676 - val_categorical_crossentropy: 0.0676 - lr: 0.0100\n",
            "Epoch 77/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0696 - categorical_crossentropy: 0.0696\n",
            "Epoch 00077: val_categorical_crossentropy did not improve from 0.06756\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.0697 - categorical_crossentropy: 0.0697 - val_loss: 0.0782 - val_categorical_crossentropy: 0.0782 - lr: 0.0100\n",
            "Epoch 78/150\n",
            "3939/3942 [============================>.] - ETA: 0s - loss: 0.0687 - categorical_crossentropy: 0.0687\n",
            "Epoch 00078: val_categorical_crossentropy did not improve from 0.06756\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0690 - categorical_crossentropy: 0.0690 - val_loss: 0.0969 - val_categorical_crossentropy: 0.0969 - lr: 0.0100\n",
            "Epoch 79/150\n",
            "3912/3942 [============================>.] - ETA: 0s - loss: 0.0663 - categorical_crossentropy: 0.0663\n",
            "Epoch 00079: val_categorical_crossentropy did not improve from 0.06756\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0665 - categorical_crossentropy: 0.0665 - val_loss: 0.0783 - val_categorical_crossentropy: 0.0783 - lr: 0.0100\n",
            "Epoch 80/150\n",
            "3916/3942 [============================>.] - ETA: 0s - loss: 0.0650 - categorical_crossentropy: 0.0650\n",
            "Epoch 00080: val_categorical_crossentropy improved from 0.06756 to 0.06671, saving model to hemo_0.06671__.hdf5\n",
            "3942/3942 [==============================] - 7s 2ms/step - loss: 0.0651 - categorical_crossentropy: 0.0651 - val_loss: 0.0667 - val_categorical_crossentropy: 0.0667 - lr: 0.0100\n",
            "Epoch 81/150\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.0633 - categorical_crossentropy: 0.0633\n",
            "Epoch 00081: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0635 - categorical_crossentropy: 0.0635 - val_loss: 0.0704 - val_categorical_crossentropy: 0.0704 - lr: 0.0100\n",
            "Epoch 82/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0618 - categorical_crossentropy: 0.0618\n",
            "Epoch 00082: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0619 - categorical_crossentropy: 0.0619 - val_loss: 0.0716 - val_categorical_crossentropy: 0.0716 - lr: 0.0100\n",
            "Epoch 83/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.0600 - categorical_crossentropy: 0.0600\n",
            "Epoch 00083: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0603 - categorical_crossentropy: 0.0603 - val_loss: 0.0677 - val_categorical_crossentropy: 0.0677 - lr: 0.0100\n",
            "Epoch 84/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0592 - categorical_crossentropy: 0.0592\n",
            "Epoch 00084: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0594 - categorical_crossentropy: 0.0594 - val_loss: 0.0988 - val_categorical_crossentropy: 0.0988 - lr: 0.0100\n",
            "Epoch 85/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0571 - categorical_crossentropy: 0.0571\n",
            "Epoch 00085: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0573 - categorical_crossentropy: 0.0573 - val_loss: 0.0915 - val_categorical_crossentropy: 0.0915 - lr: 0.0100\n",
            "Epoch 86/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0574 - categorical_crossentropy: 0.0574\n",
            "Epoch 00086: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0580 - categorical_crossentropy: 0.0580 - val_loss: 0.0947 - val_categorical_crossentropy: 0.0947 - lr: 0.0100\n",
            "Epoch 87/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0553 - categorical_crossentropy: 0.0553\n",
            "Epoch 00087: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0558 - categorical_crossentropy: 0.0558 - val_loss: 0.0949 - val_categorical_crossentropy: 0.0949 - lr: 0.0100\n",
            "Epoch 88/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0536 - categorical_crossentropy: 0.0536\n",
            "Epoch 00088: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0539 - categorical_crossentropy: 0.0539 - val_loss: 0.0912 - val_categorical_crossentropy: 0.0912 - lr: 0.0100\n",
            "Epoch 89/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.0530 - categorical_crossentropy: 0.0530\n",
            "Epoch 00089: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0535 - categorical_crossentropy: 0.0535 - val_loss: 0.0905 - val_categorical_crossentropy: 0.0905 - lr: 0.0100\n",
            "Epoch 90/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.0512 - categorical_crossentropy: 0.0512\n",
            "Epoch 00090: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 9s 2ms/step - loss: 0.0514 - categorical_crossentropy: 0.0514 - val_loss: 0.0761 - val_categorical_crossentropy: 0.0761 - lr: 0.0100\n",
            "Epoch 91/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.0506 - categorical_crossentropy: 0.0506\n",
            "Epoch 00091: val_categorical_crossentropy did not improve from 0.06671\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0508 - categorical_crossentropy: 0.0508 - val_loss: 0.0703 - val_categorical_crossentropy: 0.0703 - lr: 0.0100\n",
            "Epoch 92/150\n",
            "3930/3942 [============================>.] - ETA: 0s - loss: 0.0498 - categorical_crossentropy: 0.0498\n",
            "Epoch 00092: val_categorical_crossentropy improved from 0.06671 to 0.04985, saving model to hemo_0.04985__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0499 - categorical_crossentropy: 0.0499 - val_loss: 0.0498 - val_categorical_crossentropy: 0.0498 - lr: 0.0100\n",
            "Epoch 93/150\n",
            "3916/3942 [============================>.] - ETA: 0s - loss: 0.0489 - categorical_crossentropy: 0.0489\n",
            "Epoch 00093: val_categorical_crossentropy did not improve from 0.04985\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0491 - categorical_crossentropy: 0.0491 - val_loss: 0.0542 - val_categorical_crossentropy: 0.0542 - lr: 0.0100\n",
            "Epoch 94/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.0481 - categorical_crossentropy: 0.0481\n",
            "Epoch 00094: val_categorical_crossentropy did not improve from 0.04985\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0483 - categorical_crossentropy: 0.0483 - val_loss: 0.0533 - val_categorical_crossentropy: 0.0533 - lr: 0.0100\n",
            "Epoch 95/150\n",
            "3934/3942 [============================>.] - ETA: 0s - loss: 0.0473 - categorical_crossentropy: 0.0473\n",
            "Epoch 00095: val_categorical_crossentropy improved from 0.04985 to 0.04544, saving model to hemo_0.04544__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0475 - categorical_crossentropy: 0.0475 - val_loss: 0.0454 - val_categorical_crossentropy: 0.0454 - lr: 0.0100\n",
            "Epoch 96/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0463 - categorical_crossentropy: 0.0463\n",
            "Epoch 00096: val_categorical_crossentropy did not improve from 0.04544\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0464 - categorical_crossentropy: 0.0464 - val_loss: 0.0508 - val_categorical_crossentropy: 0.0508 - lr: 0.0100\n",
            "Epoch 97/150\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.0459 - categorical_crossentropy: 0.0459\n",
            "Epoch 00097: val_categorical_crossentropy did not improve from 0.04544\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0459 - categorical_crossentropy: 0.0459 - val_loss: 0.0564 - val_categorical_crossentropy: 0.0564 - lr: 0.0100\n",
            "Epoch 98/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0447 - categorical_crossentropy: 0.0447\n",
            "Epoch 00098: val_categorical_crossentropy did not improve from 0.04544\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0448 - categorical_crossentropy: 0.0448 - val_loss: 0.0462 - val_categorical_crossentropy: 0.0462 - lr: 0.0100\n",
            "Epoch 99/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0437 - categorical_crossentropy: 0.0437\n",
            "Epoch 00099: val_categorical_crossentropy did not improve from 0.04544\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0439 - categorical_crossentropy: 0.0439 - val_loss: 0.0500 - val_categorical_crossentropy: 0.0500 - lr: 0.0100\n",
            "Epoch 100/150\n",
            "3918/3942 [============================>.] - ETA: 0s - loss: 0.0427 - categorical_crossentropy: 0.0427\n",
            "Epoch 00100: val_categorical_crossentropy improved from 0.04544 to 0.04385, saving model to hemo_0.04385__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0427 - categorical_crossentropy: 0.0427 - val_loss: 0.0438 - val_categorical_crossentropy: 0.0438 - lr: 0.0100\n",
            "Epoch 101/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.0421 - categorical_crossentropy: 0.0421\n",
            "Epoch 00101: val_categorical_crossentropy improved from 0.04385 to 0.04276, saving model to hemo_0.04276__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0422 - categorical_crossentropy: 0.0422 - val_loss: 0.0428 - val_categorical_crossentropy: 0.0428 - lr: 0.0100\n",
            "Epoch 102/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.0415 - categorical_crossentropy: 0.0415\n",
            "Epoch 00102: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0417 - categorical_crossentropy: 0.0417 - val_loss: 0.0471 - val_categorical_crossentropy: 0.0471 - lr: 0.0100\n",
            "Epoch 103/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0409 - categorical_crossentropy: 0.0409\n",
            "Epoch 00103: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0410 - categorical_crossentropy: 0.0410 - val_loss: 0.0551 - val_categorical_crossentropy: 0.0551 - lr: 0.0100\n",
            "Epoch 104/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.0406 - categorical_crossentropy: 0.0406\n",
            "Epoch 00104: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0407 - categorical_crossentropy: 0.0407 - val_loss: 0.0539 - val_categorical_crossentropy: 0.0539 - lr: 0.0100\n",
            "Epoch 105/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.0399 - categorical_crossentropy: 0.0399\n",
            "Epoch 00105: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0400 - categorical_crossentropy: 0.0400 - val_loss: 0.0588 - val_categorical_crossentropy: 0.0588 - lr: 0.0100\n",
            "Epoch 106/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.0392 - categorical_crossentropy: 0.0392\n",
            "Epoch 00106: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0393 - categorical_crossentropy: 0.0393 - val_loss: 0.0607 - val_categorical_crossentropy: 0.0607 - lr: 0.0100\n",
            "Epoch 107/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.0389 - categorical_crossentropy: 0.0389\n",
            "Epoch 00107: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0389 - categorical_crossentropy: 0.0389 - val_loss: 0.0614 - val_categorical_crossentropy: 0.0614 - lr: 0.0100\n",
            "Epoch 108/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0379 - categorical_crossentropy: 0.0379\n",
            "Epoch 00108: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0380 - categorical_crossentropy: 0.0380 - val_loss: 0.0676 - val_categorical_crossentropy: 0.0676 - lr: 0.0100\n",
            "Epoch 109/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.0374 - categorical_crossentropy: 0.0374\n",
            "Epoch 00109: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0374 - categorical_crossentropy: 0.0374 - val_loss: 0.0720 - val_categorical_crossentropy: 0.0720 - lr: 0.0100\n",
            "Epoch 110/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.0367 - categorical_crossentropy: 0.0367\n",
            "Epoch 00110: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0367 - categorical_crossentropy: 0.0367 - val_loss: 0.0677 - val_categorical_crossentropy: 0.0677 - lr: 0.0100\n",
            "Epoch 111/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.0359 - categorical_crossentropy: 0.0359\n",
            "Epoch 00111: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0360 - categorical_crossentropy: 0.0360 - val_loss: 0.0609 - val_categorical_crossentropy: 0.0609 - lr: 0.0100\n",
            "Epoch 112/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0354 - categorical_crossentropy: 0.0354\n",
            "Epoch 00112: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0355 - categorical_crossentropy: 0.0355 - val_loss: 0.0708 - val_categorical_crossentropy: 0.0708 - lr: 0.0100\n",
            "Epoch 113/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0350 - categorical_crossentropy: 0.0350\n",
            "Epoch 00113: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0351 - categorical_crossentropy: 0.0351 - val_loss: 0.0514 - val_categorical_crossentropy: 0.0514 - lr: 0.0100\n",
            "Epoch 114/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0328 - categorical_crossentropy: 0.0328\n",
            "Epoch 00114: val_categorical_crossentropy did not improve from 0.04276\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0329 - categorical_crossentropy: 0.0329 - val_loss: 0.0633 - val_categorical_crossentropy: 0.0633 - lr: 0.0100\n",
            "Epoch 115/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.0343 - categorical_crossentropy: 0.0343\n",
            "Epoch 00115: val_categorical_crossentropy improved from 0.04276 to 0.04146, saving model to hemo_0.04146__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0345 - categorical_crossentropy: 0.0345 - val_loss: 0.0415 - val_categorical_crossentropy: 0.0415 - lr: 0.0100\n",
            "Epoch 116/150\n",
            "3925/3942 [============================>.] - ETA: 0s - loss: 0.0343 - categorical_crossentropy: 0.0343\n",
            "Epoch 00116: val_categorical_crossentropy improved from 0.04146 to 0.03212, saving model to hemo_0.03212__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0344 - categorical_crossentropy: 0.0344 - val_loss: 0.0321 - val_categorical_crossentropy: 0.0321 - lr: 0.0100\n",
            "Epoch 117/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.0340 - categorical_crossentropy: 0.0340\n",
            "Epoch 00117: val_categorical_crossentropy improved from 0.03212 to 0.03059, saving model to hemo_0.03059__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0341 - categorical_crossentropy: 0.0341 - val_loss: 0.0306 - val_categorical_crossentropy: 0.0306 - lr: 0.0100\n",
            "Epoch 118/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.0339 - categorical_crossentropy: 0.0339\n",
            "Epoch 00118: val_categorical_crossentropy did not improve from 0.03059\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0340 - categorical_crossentropy: 0.0340 - val_loss: 0.0327 - val_categorical_crossentropy: 0.0327 - lr: 0.0100\n",
            "Epoch 119/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0333 - categorical_crossentropy: 0.0333\n",
            "Epoch 00119: val_categorical_crossentropy improved from 0.03059 to 0.02829, saving model to hemo_0.02829__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0334 - categorical_crossentropy: 0.0334 - val_loss: 0.0283 - val_categorical_crossentropy: 0.0283 - lr: 0.0100\n",
            "Epoch 120/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.0322 - categorical_crossentropy: 0.0322\n",
            "Epoch 00120: val_categorical_crossentropy did not improve from 0.02829\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0323 - categorical_crossentropy: 0.0323 - val_loss: 0.0290 - val_categorical_crossentropy: 0.0290 - lr: 0.0100\n",
            "Epoch 121/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.0320 - categorical_crossentropy: 0.0320\n",
            "Epoch 00121: val_categorical_crossentropy improved from 0.02829 to 0.02775, saving model to hemo_0.02775__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0320 - categorical_crossentropy: 0.0320 - val_loss: 0.0278 - val_categorical_crossentropy: 0.0278 - lr: 0.0100\n",
            "Epoch 122/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.0327 - categorical_crossentropy: 0.0327\n",
            "Epoch 00122: val_categorical_crossentropy improved from 0.02775 to 0.02476, saving model to hemo_0.02476__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0327 - categorical_crossentropy: 0.0327 - val_loss: 0.0248 - val_categorical_crossentropy: 0.0248 - lr: 0.0100\n",
            "Epoch 123/150\n",
            "3936/3942 [============================>.] - ETA: 0s - loss: 0.0312 - categorical_crossentropy: 0.0312\n",
            "Epoch 00123: val_categorical_crossentropy improved from 0.02476 to 0.02462, saving model to hemo_0.02462__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0313 - categorical_crossentropy: 0.0313 - val_loss: 0.0246 - val_categorical_crossentropy: 0.0246 - lr: 0.0100\n",
            "Epoch 124/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.0312 - categorical_crossentropy: 0.0312\n",
            "Epoch 00124: val_categorical_crossentropy improved from 0.02462 to 0.02278, saving model to hemo_0.02278__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0313 - categorical_crossentropy: 0.0313 - val_loss: 0.0228 - val_categorical_crossentropy: 0.0228 - lr: 0.0100\n",
            "Epoch 125/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.0301 - categorical_crossentropy: 0.0301\n",
            "Epoch 00125: val_categorical_crossentropy improved from 0.02278 to 0.01989, saving model to hemo_0.01989__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0302 - categorical_crossentropy: 0.0302 - val_loss: 0.0199 - val_categorical_crossentropy: 0.0199 - lr: 0.0100\n",
            "Epoch 126/150\n",
            "3941/3942 [============================>.] - ETA: 0s - loss: 0.0303 - categorical_crossentropy: 0.0303\n",
            "Epoch 00126: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0303 - categorical_crossentropy: 0.0303 - val_loss: 0.0199 - val_categorical_crossentropy: 0.0199 - lr: 0.0100\n",
            "Epoch 127/150\n",
            "3920/3942 [============================>.] - ETA: 0s - loss: 0.0299 - categorical_crossentropy: 0.0299\n",
            "Epoch 00127: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0299 - categorical_crossentropy: 0.0299 - val_loss: 0.0261 - val_categorical_crossentropy: 0.0261 - lr: 0.0100\n",
            "Epoch 128/150\n",
            "3923/3942 [============================>.] - ETA: 0s - loss: 0.0293 - categorical_crossentropy: 0.0293\n",
            "Epoch 00128: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0294 - categorical_crossentropy: 0.0294 - val_loss: 0.0257 - val_categorical_crossentropy: 0.0257 - lr: 0.0100\n",
            "Epoch 129/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.0295 - categorical_crossentropy: 0.0295\n",
            "Epoch 00129: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0296 - categorical_crossentropy: 0.0296 - val_loss: 0.0218 - val_categorical_crossentropy: 0.0218 - lr: 0.0100\n",
            "Epoch 130/150\n",
            "3932/3942 [============================>.] - ETA: 0s - loss: 0.0287 - categorical_crossentropy: 0.0287\n",
            "Epoch 00130: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0288 - categorical_crossentropy: 0.0288 - val_loss: 0.0200 - val_categorical_crossentropy: 0.0200 - lr: 0.0100\n",
            "Epoch 131/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0281 - categorical_crossentropy: 0.0281\n",
            "Epoch 00131: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0281 - categorical_crossentropy: 0.0281 - val_loss: 0.0210 - val_categorical_crossentropy: 0.0210 - lr: 0.0100\n",
            "Epoch 132/150\n",
            "3919/3942 [============================>.] - ETA: 0s - loss: 0.0275 - categorical_crossentropy: 0.0275\n",
            "Epoch 00132: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0275 - categorical_crossentropy: 0.0275 - val_loss: 0.0298 - val_categorical_crossentropy: 0.0298 - lr: 0.0100\n",
            "Epoch 133/150\n",
            "3942/3942 [==============================] - ETA: 0s - loss: 0.0279 - categorical_crossentropy: 0.0279\n",
            "Epoch 00133: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0279 - categorical_crossentropy: 0.0279 - val_loss: 0.0237 - val_categorical_crossentropy: 0.0237 - lr: 0.0100\n",
            "Epoch 134/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.0275 - categorical_crossentropy: 0.0275\n",
            "Epoch 00134: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0276 - categorical_crossentropy: 0.0276 - val_loss: 0.0235 - val_categorical_crossentropy: 0.0235 - lr: 0.0100\n",
            "Epoch 135/150\n",
            "3922/3942 [============================>.] - ETA: 0s - loss: 0.0269 - categorical_crossentropy: 0.0269\n",
            "Epoch 00135: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0270 - categorical_crossentropy: 0.0270 - val_loss: 0.0223 - val_categorical_crossentropy: 0.0223 - lr: 0.0100\n",
            "Epoch 136/150\n",
            "3929/3942 [============================>.] - ETA: 0s - loss: 0.0272 - categorical_crossentropy: 0.0272\n",
            "Epoch 00136: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0273 - categorical_crossentropy: 0.0273 - val_loss: 0.0215 - val_categorical_crossentropy: 0.0215 - lr: 0.0100\n",
            "Epoch 137/150\n",
            "3937/3942 [============================>.] - ETA: 0s - loss: 0.0270 - categorical_crossentropy: 0.0270\n",
            "Epoch 00137: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0270 - categorical_crossentropy: 0.0270 - val_loss: 0.0218 - val_categorical_crossentropy: 0.0218 - lr: 0.0100\n",
            "Epoch 138/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.0257 - categorical_crossentropy: 0.0257\n",
            "Epoch 00138: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0257 - categorical_crossentropy: 0.0257 - val_loss: 0.0258 - val_categorical_crossentropy: 0.0258 - lr: 0.0100\n",
            "Epoch 139/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0276 - categorical_crossentropy: 0.0276\n",
            "Epoch 00139: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0276 - categorical_crossentropy: 0.0276 - val_loss: 0.0212 - val_categorical_crossentropy: 0.0212 - lr: 0.0100\n",
            "Epoch 140/150\n",
            "3940/3942 [============================>.] - ETA: 0s - loss: 0.0253 - categorical_crossentropy: 0.0253\n",
            "Epoch 00140: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0254 - categorical_crossentropy: 0.0254 - val_loss: 0.0214 - val_categorical_crossentropy: 0.0214 - lr: 0.0100\n",
            "Epoch 141/150\n",
            "3938/3942 [============================>.] - ETA: 0s - loss: 0.0247 - categorical_crossentropy: 0.0247\n",
            "Epoch 00141: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0247 - categorical_crossentropy: 0.0247 - val_loss: 0.0229 - val_categorical_crossentropy: 0.0229 - lr: 0.0100\n",
            "Epoch 142/150\n",
            "3921/3942 [============================>.] - ETA: 0s - loss: 0.0251 - categorical_crossentropy: 0.0251\n",
            "Epoch 00142: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0251 - categorical_crossentropy: 0.0251 - val_loss: 0.0252 - val_categorical_crossentropy: 0.0252 - lr: 0.0100\n",
            "Epoch 143/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.0244 - categorical_crossentropy: 0.0244\n",
            "Epoch 00143: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0246 - categorical_crossentropy: 0.0246 - val_loss: 0.0237 - val_categorical_crossentropy: 0.0237 - lr: 0.0100\n",
            "Epoch 144/150\n",
            "3924/3942 [============================>.] - ETA: 0s - loss: 0.0242 - categorical_crossentropy: 0.0242\n",
            "Epoch 00144: val_categorical_crossentropy did not improve from 0.01989\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0242 - categorical_crossentropy: 0.0242 - val_loss: 0.0213 - val_categorical_crossentropy: 0.0213 - lr: 0.0100\n",
            "Epoch 145/150\n",
            "3935/3942 [============================>.] - ETA: 0s - loss: 0.0243 - categorical_crossentropy: 0.0243\n",
            "Epoch 00145: val_categorical_crossentropy improved from 0.01989 to 0.01979, saving model to hemo_0.01979__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0244 - categorical_crossentropy: 0.0244 - val_loss: 0.0198 - val_categorical_crossentropy: 0.0198 - lr: 0.0100\n",
            "Epoch 146/150\n",
            "3933/3942 [============================>.] - ETA: 0s - loss: 0.0227 - categorical_crossentropy: 0.0227\n",
            "Epoch 00146: val_categorical_crossentropy did not improve from 0.01979\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0228 - categorical_crossentropy: 0.0228 - val_loss: 0.0270 - val_categorical_crossentropy: 0.0270 - lr: 0.0100\n",
            "Epoch 147/150\n",
            "3915/3942 [============================>.] - ETA: 0s - loss: 0.0238 - categorical_crossentropy: 0.0238\n",
            "Epoch 00147: val_categorical_crossentropy improved from 0.01979 to 0.01813, saving model to hemo_0.01813__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0238 - categorical_crossentropy: 0.0238 - val_loss: 0.0181 - val_categorical_crossentropy: 0.0181 - lr: 0.0100\n",
            "Epoch 148/150\n",
            "3928/3942 [============================>.] - ETA: 0s - loss: 0.0240 - categorical_crossentropy: 0.0240\n",
            "Epoch 00148: val_categorical_crossentropy improved from 0.01813 to 0.01809, saving model to hemo_0.01809__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0240 - categorical_crossentropy: 0.0240 - val_loss: 0.0181 - val_categorical_crossentropy: 0.0181 - lr: 0.0100\n",
            "Epoch 149/150\n",
            "3927/3942 [============================>.] - ETA: 0s - loss: 0.0223 - categorical_crossentropy: 0.0223\n",
            "Epoch 00149: val_categorical_crossentropy did not improve from 0.01809\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0224 - categorical_crossentropy: 0.0224 - val_loss: 0.0214 - val_categorical_crossentropy: 0.0214 - lr: 0.0100\n",
            "Epoch 150/150\n",
            "3926/3942 [============================>.] - ETA: 0s - loss: 0.0228 - categorical_crossentropy: 0.0228\n",
            "Epoch 00150: val_categorical_crossentropy improved from 0.01809 to 0.01657, saving model to hemo_0.01657__.hdf5\n",
            "3942/3942 [==============================] - 8s 2ms/step - loss: 0.0228 - categorical_crossentropy: 0.0228 - val_loss: 0.0166 - val_categorical_crossentropy: 0.0166 - lr: 0.0100\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4fb88c2f90>"
            ]
          },
          "metadata": {},
          "execution_count": 160
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGCsVpcqGqxW"
      },
      "source": [
        ""
      ],
      "execution_count": 160,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IRG7_4hLQdc2"
      },
      "source": [
        "col_pred=col_model.predict(X)\n",
        "hemo_pred=hemo_model.predict(X)\n",
        "hdl_pred=hdl_model.predict(X)\n"
      ],
      "execution_count": 161,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6erc4HdQnWu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7cf9433-dda7-4908-f685-19ad060eccff"
      },
      "source": [
        "hemo_pred"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.6322988e-13, 3.4370417e-12, 1.0000000e+00],\n",
              "       [9.9965394e-01, 4.4083026e-10, 3.4606221e-04],\n",
              "       [3.5722416e-15, 5.0215886e-12, 1.0000000e+00],\n",
              "       ...,\n",
              "       [5.2713111e-02, 2.9587836e-04, 9.4699097e-01],\n",
              "       [9.4334228e-11, 0.0000000e+00, 1.0000000e+00],\n",
              "       [5.4857360e-08, 6.5788578e-07, 9.9999928e-01]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 162
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPrb8UWtRF_W"
      },
      "source": [
        "import numpy as np\n",
        "col_argm=np.argmax(col_pred,axis=1)\n",
        "hemo_argm=np.argmax(hemo_pred,axis=1)\n",
        "hdl_argm=np.argmax(hdl_pred,axis=1)\n",
        "\n",
        "\n"
      ],
      "execution_count": 163,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ct_bAj1a72Sx"
      },
      "source": [
        "hemo_argm=hemo_argm.reshape(-1,1)\n",
        "hemo_predicted=hemo_onehot.inverse_transform(hemo_pred)\n",
        "\n",
        "col_argm=col_argm.reshape(-1,1)\n",
        "col_predicted=col_onehot.inverse_transform(col_pred)\n",
        "\n",
        "hdl_argm=hdl_argm.reshape(-1,1)\n",
        "hdl_predicted=hdl_onehot.inverse_transform(hdl_pred)\n",
        "\n",
        "\n"
      ],
      "execution_count": 164,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yaI_Aww88-nw"
      },
      "source": [
        "def calc_status(pred,dfcolumn):\n",
        "    i=0\n",
        "    for index,element in enumerate(pred):\n",
        "\n",
        "        if element != dfcolumn.iloc[index]:\n",
        "            #print(f\"{index}\")\n",
        "            i +=1\n",
        "    print(f\"{i} hibás, {i/len(pred)*100} % hiba\")        "
      ],
      "execution_count": 165,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s6tAhxQw-BlL",
        "outputId": "bc281bc4-0d37-4498-9b7f-cc0d3326cf04"
      },
      "source": [
        "calc_status(hemo_predicted,df[\"hemoglobin(hgb)_human\"])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48 hibás, 0.365296803652968 % hiba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIWbc4Co_1k0",
        "outputId": "e4928372-8439-4bb5-f6a9-e39bc02750f7"
      },
      "source": [
        "df.columns"
      ],
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['Reading_ID', 'absorbance0', 'absorbance1', 'absorbance2',\n",
              "       'absorbance3', 'absorbance4', 'absorbance5', 'absorbance6',\n",
              "       'absorbance7', 'absorbance8',\n",
              "       ...\n",
              "       'absorbance165', 'absorbance166', 'absorbance167', 'absorbance168',\n",
              "       'absorbance169', 'temperature', 'humidity', 'hdl_cholesterol_human',\n",
              "       'hemoglobin(hgb)_human', 'cholesterol_ldl_human'],\n",
              "      dtype='object', length=176)"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1N9WpCjJ_xbh",
        "outputId": "78fbe107-f413-4794-a2ab-a39ca2847e6f"
      },
      "source": [
        "calc_status(col_predicted,df[\"cholesterol_ldl_human\"])"
      ],
      "execution_count": 168,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "732 hibás, 5.570776255707763 % hiba\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jZCPalQ3AZoT",
        "outputId": "4151299c-4280-431d-ab77-da2d6849100a"
      },
      "source": [
        "calc_status(hdl_predicted,df[\"hdl_cholesterol_human\"])"
      ],
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "554 hibás, 4.216133942161339 % hiba\n"
          ]
        }
      ]
    }
  ]
}