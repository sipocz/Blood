{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blood_ldl_model_20211215.ipynb másolata",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM97uYRQlz7iHCSpPznBn3W",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5008f884d889486096e5bf4d56d9c237": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_06c3c56069b64754811c97cbb92584eb",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_6990d76a960147ddb4af0afe7e200913",
              "IPY_MODEL_00c81acd50e74486a0baa78ef4d57e22"
            ]
          }
        },
        "06c3c56069b64754811c97cbb92584eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6990d76a960147ddb4af0afe7e200913": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_abedb1c0c2664ebc9513de5f50d57536",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9.02MB of 9.02MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a857d064a5bf44f7b8cc13dc43d1c3df"
          }
        },
        "00c81acd50e74486a0baa78ef4d57e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_8dc1b96c51d1485494e4c8745c92211a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8f454602a294478686705b6a73b0620e"
          }
        },
        "abedb1c0c2664ebc9513de5f50d57536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a857d064a5bf44f7b8cc13dc43d1c3df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8dc1b96c51d1485494e4c8745c92211a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8f454602a294478686705b6a73b0620e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sipocz/Blood/blob/main/Blood_ldl_model_20211215_ipynb_m%C3%A1solata.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install lazypredict"
      ],
      "metadata": {
        "id": "lkC-pw0r-iXW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__PROJECT_SOURCE__=\"COLAB\""
      ],
      "metadata": {
        "id": "k_VTudIT_T2U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNkYLOFrij5B",
        "outputId": "ab5b3e79-a656-4b42-e018-f19238aa5dc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if __PROJECT_SOURCE__==\"COLAB\":\n",
        "    # Import PyDrive and associated libraries.\n",
        "    # This only needs to be done once per notebook.\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    # This only needs to be done once per notebook.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    fname_dir=\"/content/blood/\"\n",
        "    fname_url=\"/content/drive/MyDrive/blood/rework/*agg.csv\"\n",
        "    fname=fname_url.split(\"/\")[-1]\n",
        "elif __PROJECT_SOURCE__==\"LOCAL\":\n",
        "    fname_dir=\"sdfsdfsd\" #working dir\n",
        "    fname_url=\"/content/drive/MyDrive/blood/rework/*agg.csv\" #data source dir\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m7yB5l-7i7dS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z3v1BBbJjBqa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir $fname_dir"
      ],
      "metadata": {
        "id": "zU1ZwzodjSD4"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cp $fname_url $fname_dir"
      ],
      "metadata": {
        "id": "vXyl_8ELjgrT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __PROJECT_SOURCE__==\"COLAB\":\n",
        "    drive.flush_and_unmount()\n",
        "    print('Unmount Google Drive :-(')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Y7z49_kBNx",
        "outputId": "5a5ae6c6-7c89-4bcb-8f2f-3e27ec480343"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unmount Google Drive :-(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feldolgozás"
      ],
      "metadata": {
        "id": "JCOsDGPtxztL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy1mbPDRizA2",
        "outputId": "f72721bb-6fbb-428f-ce8e-bda8b1cacd8d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.hdf5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LjJtfapexzDo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F6TPT1jWknJD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames_list=[\"ldl1_agg.csv\",\"ldl2_agg.csv\"]"
      ],
      "metadata": {
        "id": "5MmhW6aekVHb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name1=fname_dir+fnames_list[0]\n",
        "print(file_name1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YCcXLaMkrlf",
        "outputId": "a318ec28-1a51-47c0-f356-7ae8d99f2e76"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blood/ldl1_agg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name2=fname_dir+fnames_list[1]\n",
        "print(file_name2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiMNf4HPBAnV",
        "outputId": "2fb09972-b546-4789-b69e-d6c8e3fd2404"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blood/ldl2_agg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TkP2-bFrx4ra"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg1= pd.read_csv(file_name1)\n",
        "df_agg1.describe()\n",
        "df_agg1.drop(df_agg1[df_agg1.absorbance0 < 0].index, inplace=True) # kill the negative elements\n"
      ],
      "metadata": {
        "id": "mPW32Ow0k9BQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "VSTvJ9RvzrI8",
        "outputId": "6dc83be5-8709-44ea-98c0-599c2148d1a5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.535147</td>\n",
              "      <td>0.537960</td>\n",
              "      <td>0.541943</td>\n",
              "      <td>0.548029</td>\n",
              "      <td>0.553548</td>\n",
              "      <td>0.560042</td>\n",
              "      <td>0.566466</td>\n",
              "      <td>0.574085</td>\n",
              "      <td>0.586266</td>\n",
              "      <td>0.599614</td>\n",
              "      <td>0.615089</td>\n",
              "      <td>0.626659</td>\n",
              "      <td>0.633565</td>\n",
              "      <td>0.637011</td>\n",
              "      <td>0.639238</td>\n",
              "      <td>0.638972</td>\n",
              "      <td>0.637331</td>\n",
              "      <td>0.635508</td>\n",
              "      <td>0.632157</td>\n",
              "      <td>0.628089</td>\n",
              "      <td>0.624585</td>\n",
              "      <td>0.621561</td>\n",
              "      <td>0.617845</td>\n",
              "      <td>0.614378</td>\n",
              "      <td>0.610104</td>\n",
              "      <td>0.607131</td>\n",
              "      <td>0.604818</td>\n",
              "      <td>0.602482</td>\n",
              "      <td>0.601290</td>\n",
              "      <td>0.599995</td>\n",
              "      <td>0.598909</td>\n",
              "      <td>0.599053</td>\n",
              "      <td>0.599842</td>\n",
              "      <td>0.601187</td>\n",
              "      <td>0.602825</td>\n",
              "      <td>0.606265</td>\n",
              "      <td>0.609677</td>\n",
              "      <td>0.614040</td>\n",
              "      <td>0.618327</td>\n",
              "      <td>...</td>\n",
              "      <td>1.688604</td>\n",
              "      <td>1.674469</td>\n",
              "      <td>1.660716</td>\n",
              "      <td>1.648262</td>\n",
              "      <td>1.630642</td>\n",
              "      <td>1.618590</td>\n",
              "      <td>1.606035</td>\n",
              "      <td>1.591950</td>\n",
              "      <td>1.580556</td>\n",
              "      <td>1.565986</td>\n",
              "      <td>1.555014</td>\n",
              "      <td>1.542866</td>\n",
              "      <td>1.533852</td>\n",
              "      <td>1.525125</td>\n",
              "      <td>1.514723</td>\n",
              "      <td>1.503877</td>\n",
              "      <td>1.495593</td>\n",
              "      <td>1.488977</td>\n",
              "      <td>1.479454</td>\n",
              "      <td>1.472006</td>\n",
              "      <td>1.465699</td>\n",
              "      <td>1.460134</td>\n",
              "      <td>1.458216</td>\n",
              "      <td>1.456837</td>\n",
              "      <td>1.456203</td>\n",
              "      <td>1.459403</td>\n",
              "      <td>1.460975</td>\n",
              "      <td>1.457055</td>\n",
              "      <td>1.457455</td>\n",
              "      <td>1.454225</td>\n",
              "      <td>1.447507</td>\n",
              "      <td>1.450268</td>\n",
              "      <td>1.451860</td>\n",
              "      <td>1.461692</td>\n",
              "      <td>1.471279</td>\n",
              "      <td>2440</td>\n",
              "      <td>39.057167</td>\n",
              "      <td>46.649500</td>\n",
              "      <td>105.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.538019</td>\n",
              "      <td>0.538283</td>\n",
              "      <td>0.542379</td>\n",
              "      <td>0.545535</td>\n",
              "      <td>0.549656</td>\n",
              "      <td>0.555612</td>\n",
              "      <td>0.563389</td>\n",
              "      <td>0.570042</td>\n",
              "      <td>0.579115</td>\n",
              "      <td>0.590913</td>\n",
              "      <td>0.604809</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.628157</td>\n",
              "      <td>0.630772</td>\n",
              "      <td>0.631071</td>\n",
              "      <td>0.630588</td>\n",
              "      <td>0.628943</td>\n",
              "      <td>0.627554</td>\n",
              "      <td>0.625168</td>\n",
              "      <td>0.623034</td>\n",
              "      <td>0.620265</td>\n",
              "      <td>0.617370</td>\n",
              "      <td>0.614614</td>\n",
              "      <td>0.611324</td>\n",
              "      <td>0.608916</td>\n",
              "      <td>0.606037</td>\n",
              "      <td>0.603702</td>\n",
              "      <td>0.602188</td>\n",
              "      <td>0.600821</td>\n",
              "      <td>0.599539</td>\n",
              "      <td>0.598978</td>\n",
              "      <td>0.599364</td>\n",
              "      <td>0.599753</td>\n",
              "      <td>0.601123</td>\n",
              "      <td>0.603251</td>\n",
              "      <td>0.605764</td>\n",
              "      <td>0.608760</td>\n",
              "      <td>0.612281</td>\n",
              "      <td>0.615855</td>\n",
              "      <td>...</td>\n",
              "      <td>1.564822</td>\n",
              "      <td>1.550711</td>\n",
              "      <td>1.534420</td>\n",
              "      <td>1.521360</td>\n",
              "      <td>1.509736</td>\n",
              "      <td>1.496078</td>\n",
              "      <td>1.484581</td>\n",
              "      <td>1.472423</td>\n",
              "      <td>1.458472</td>\n",
              "      <td>1.450467</td>\n",
              "      <td>1.436744</td>\n",
              "      <td>1.426040</td>\n",
              "      <td>1.417529</td>\n",
              "      <td>1.408495</td>\n",
              "      <td>1.397023</td>\n",
              "      <td>1.391276</td>\n",
              "      <td>1.383204</td>\n",
              "      <td>1.374296</td>\n",
              "      <td>1.368845</td>\n",
              "      <td>1.361276</td>\n",
              "      <td>1.354829</td>\n",
              "      <td>1.349905</td>\n",
              "      <td>1.344804</td>\n",
              "      <td>1.343582</td>\n",
              "      <td>1.340867</td>\n",
              "      <td>1.341139</td>\n",
              "      <td>1.342966</td>\n",
              "      <td>1.342546</td>\n",
              "      <td>1.342661</td>\n",
              "      <td>1.343233</td>\n",
              "      <td>1.339520</td>\n",
              "      <td>1.331402</td>\n",
              "      <td>1.329552</td>\n",
              "      <td>1.334918</td>\n",
              "      <td>1.308238</td>\n",
              "      <td>2476</td>\n",
              "      <td>38.457833</td>\n",
              "      <td>51.680833</td>\n",
              "      <td>72.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.480685</td>\n",
              "      <td>0.484010</td>\n",
              "      <td>0.489706</td>\n",
              "      <td>0.494084</td>\n",
              "      <td>0.500947</td>\n",
              "      <td>0.507894</td>\n",
              "      <td>0.513360</td>\n",
              "      <td>0.520308</td>\n",
              "      <td>0.530952</td>\n",
              "      <td>0.543218</td>\n",
              "      <td>0.557018</td>\n",
              "      <td>0.567722</td>\n",
              "      <td>0.574330</td>\n",
              "      <td>0.577556</td>\n",
              "      <td>0.578793</td>\n",
              "      <td>0.578903</td>\n",
              "      <td>0.577392</td>\n",
              "      <td>0.575224</td>\n",
              "      <td>0.572716</td>\n",
              "      <td>0.568465</td>\n",
              "      <td>0.565585</td>\n",
              "      <td>0.562637</td>\n",
              "      <td>0.559241</td>\n",
              "      <td>0.556354</td>\n",
              "      <td>0.552418</td>\n",
              "      <td>0.549454</td>\n",
              "      <td>0.547636</td>\n",
              "      <td>0.545892</td>\n",
              "      <td>0.544410</td>\n",
              "      <td>0.543401</td>\n",
              "      <td>0.542475</td>\n",
              "      <td>0.543075</td>\n",
              "      <td>0.543687</td>\n",
              "      <td>0.544753</td>\n",
              "      <td>0.546528</td>\n",
              "      <td>0.549802</td>\n",
              "      <td>0.553025</td>\n",
              "      <td>0.556793</td>\n",
              "      <td>0.560706</td>\n",
              "      <td>...</td>\n",
              "      <td>1.539243</td>\n",
              "      <td>1.526498</td>\n",
              "      <td>1.512998</td>\n",
              "      <td>1.500887</td>\n",
              "      <td>1.486508</td>\n",
              "      <td>1.474466</td>\n",
              "      <td>1.461664</td>\n",
              "      <td>1.452243</td>\n",
              "      <td>1.439495</td>\n",
              "      <td>1.427213</td>\n",
              "      <td>1.416068</td>\n",
              "      <td>1.406622</td>\n",
              "      <td>1.398762</td>\n",
              "      <td>1.389438</td>\n",
              "      <td>1.379374</td>\n",
              "      <td>1.372073</td>\n",
              "      <td>1.363920</td>\n",
              "      <td>1.357083</td>\n",
              "      <td>1.351891</td>\n",
              "      <td>1.343235</td>\n",
              "      <td>1.339888</td>\n",
              "      <td>1.334037</td>\n",
              "      <td>1.332106</td>\n",
              "      <td>1.329624</td>\n",
              "      <td>1.332323</td>\n",
              "      <td>1.330829</td>\n",
              "      <td>1.334422</td>\n",
              "      <td>1.328862</td>\n",
              "      <td>1.324434</td>\n",
              "      <td>1.315813</td>\n",
              "      <td>1.300146</td>\n",
              "      <td>1.295544</td>\n",
              "      <td>1.293936</td>\n",
              "      <td>1.296249</td>\n",
              "      <td>1.294994</td>\n",
              "      <td>2478</td>\n",
              "      <td>42.028167</td>\n",
              "      <td>43.518667</td>\n",
              "      <td>68.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.505587</td>\n",
              "      <td>0.510463</td>\n",
              "      <td>0.515027</td>\n",
              "      <td>0.520545</td>\n",
              "      <td>0.528644</td>\n",
              "      <td>0.539653</td>\n",
              "      <td>0.547931</td>\n",
              "      <td>0.553654</td>\n",
              "      <td>0.560278</td>\n",
              "      <td>0.571240</td>\n",
              "      <td>0.582939</td>\n",
              "      <td>0.597353</td>\n",
              "      <td>0.603132</td>\n",
              "      <td>0.606054</td>\n",
              "      <td>0.607849</td>\n",
              "      <td>0.608324</td>\n",
              "      <td>0.607437</td>\n",
              "      <td>0.605870</td>\n",
              "      <td>0.602386</td>\n",
              "      <td>0.599726</td>\n",
              "      <td>0.596480</td>\n",
              "      <td>0.593364</td>\n",
              "      <td>0.590350</td>\n",
              "      <td>0.587280</td>\n",
              "      <td>0.584094</td>\n",
              "      <td>0.580810</td>\n",
              "      <td>0.578204</td>\n",
              "      <td>0.576421</td>\n",
              "      <td>0.574798</td>\n",
              "      <td>0.572748</td>\n",
              "      <td>0.572369</td>\n",
              "      <td>0.572139</td>\n",
              "      <td>0.572084</td>\n",
              "      <td>0.572998</td>\n",
              "      <td>0.574243</td>\n",
              "      <td>0.576048</td>\n",
              "      <td>0.578608</td>\n",
              "      <td>0.582545</td>\n",
              "      <td>0.586093</td>\n",
              "      <td>...</td>\n",
              "      <td>1.453705</td>\n",
              "      <td>1.442506</td>\n",
              "      <td>1.431383</td>\n",
              "      <td>1.420372</td>\n",
              "      <td>1.409361</td>\n",
              "      <td>1.398739</td>\n",
              "      <td>1.384815</td>\n",
              "      <td>1.375346</td>\n",
              "      <td>1.364956</td>\n",
              "      <td>1.358352</td>\n",
              "      <td>1.346600</td>\n",
              "      <td>1.337193</td>\n",
              "      <td>1.329186</td>\n",
              "      <td>1.319507</td>\n",
              "      <td>1.312434</td>\n",
              "      <td>1.305864</td>\n",
              "      <td>1.297765</td>\n",
              "      <td>1.292396</td>\n",
              "      <td>1.287311</td>\n",
              "      <td>1.279727</td>\n",
              "      <td>1.274297</td>\n",
              "      <td>1.272475</td>\n",
              "      <td>1.267522</td>\n",
              "      <td>1.265180</td>\n",
              "      <td>1.264382</td>\n",
              "      <td>1.263661</td>\n",
              "      <td>1.261225</td>\n",
              "      <td>1.255700</td>\n",
              "      <td>1.249614</td>\n",
              "      <td>1.239259</td>\n",
              "      <td>1.224037</td>\n",
              "      <td>1.213512</td>\n",
              "      <td>1.204171</td>\n",
              "      <td>1.216543</td>\n",
              "      <td>1.208677</td>\n",
              "      <td>2559</td>\n",
              "      <td>43.543333</td>\n",
              "      <td>23.219333</td>\n",
              "      <td>106.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.546671</td>\n",
              "      <td>0.548270</td>\n",
              "      <td>0.551964</td>\n",
              "      <td>0.555424</td>\n",
              "      <td>0.560574</td>\n",
              "      <td>0.566887</td>\n",
              "      <td>0.572948</td>\n",
              "      <td>0.580261</td>\n",
              "      <td>0.591718</td>\n",
              "      <td>0.605186</td>\n",
              "      <td>0.620562</td>\n",
              "      <td>0.632075</td>\n",
              "      <td>0.639916</td>\n",
              "      <td>0.643679</td>\n",
              "      <td>0.645930</td>\n",
              "      <td>0.645582</td>\n",
              "      <td>0.644002</td>\n",
              "      <td>0.641777</td>\n",
              "      <td>0.638956</td>\n",
              "      <td>0.634722</td>\n",
              "      <td>0.631684</td>\n",
              "      <td>0.627920</td>\n",
              "      <td>0.624216</td>\n",
              "      <td>0.620549</td>\n",
              "      <td>0.615449</td>\n",
              "      <td>0.612441</td>\n",
              "      <td>0.609487</td>\n",
              "      <td>0.606799</td>\n",
              "      <td>0.604819</td>\n",
              "      <td>0.602992</td>\n",
              "      <td>0.601829</td>\n",
              "      <td>0.600981</td>\n",
              "      <td>0.601903</td>\n",
              "      <td>0.602502</td>\n",
              "      <td>0.604280</td>\n",
              "      <td>0.606917</td>\n",
              "      <td>0.610150</td>\n",
              "      <td>0.614290</td>\n",
              "      <td>0.618448</td>\n",
              "      <td>...</td>\n",
              "      <td>1.752160</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.729738</td>\n",
              "      <td>1.718851</td>\n",
              "      <td>1.703801</td>\n",
              "      <td>1.691461</td>\n",
              "      <td>1.676763</td>\n",
              "      <td>1.665650</td>\n",
              "      <td>1.656921</td>\n",
              "      <td>1.639813</td>\n",
              "      <td>1.628050</td>\n",
              "      <td>1.619605</td>\n",
              "      <td>1.606320</td>\n",
              "      <td>1.599001</td>\n",
              "      <td>1.585844</td>\n",
              "      <td>1.576560</td>\n",
              "      <td>1.566763</td>\n",
              "      <td>1.559222</td>\n",
              "      <td>1.548711</td>\n",
              "      <td>1.540025</td>\n",
              "      <td>1.533041</td>\n",
              "      <td>1.530444</td>\n",
              "      <td>1.526929</td>\n",
              "      <td>1.524400</td>\n",
              "      <td>1.525873</td>\n",
              "      <td>1.531209</td>\n",
              "      <td>1.535637</td>\n",
              "      <td>1.539162</td>\n",
              "      <td>1.538592</td>\n",
              "      <td>1.541236</td>\n",
              "      <td>1.543308</td>\n",
              "      <td>1.550344</td>\n",
              "      <td>1.545398</td>\n",
              "      <td>1.560044</td>\n",
              "      <td>1.571627</td>\n",
              "      <td>2581</td>\n",
              "      <td>38.824833</td>\n",
              "      <td>45.364167</td>\n",
              "      <td>31.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  absorbance0  ...  cholesterol_ldl_value  cholesterol_ldl_human\n",
              "0           0     0.535147  ...                  105.6                      1\n",
              "1           1     0.538019  ...                   72.4                      1\n",
              "2           2     0.480685  ...                   68.4                      1\n",
              "3           3     0.505587  ...                  106.0                      1\n",
              "4           4     0.546671  ...                   31.2                      0\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg2= pd.read_csv(file_name2)\n",
        "df_agg2.describe()\n",
        "df_agg2.drop(df_agg2[df_agg2.absorbance0 < 0].index, inplace=True) # kill the negative elements\n"
      ],
      "metadata": {
        "id": "VbOP7ZtiBKBM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg=pd.concat([df_agg1,df_agg2], ignore_index=True)"
      ],
      "metadata": {
        "id": "6hFjkYViA-Di"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Y5xTiFTT0r-n",
        "outputId": "a9c50736-3381-42fe-d829-26df0555d3db"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>405</td>\n",
              "      <td>0.559508</td>\n",
              "      <td>0.557981</td>\n",
              "      <td>0.562073</td>\n",
              "      <td>0.566272</td>\n",
              "      <td>0.571244</td>\n",
              "      <td>0.577397</td>\n",
              "      <td>0.583213</td>\n",
              "      <td>0.590163</td>\n",
              "      <td>0.598694</td>\n",
              "      <td>0.610475</td>\n",
              "      <td>0.623472</td>\n",
              "      <td>0.636641</td>\n",
              "      <td>0.642579</td>\n",
              "      <td>0.645626</td>\n",
              "      <td>0.647103</td>\n",
              "      <td>0.645668</td>\n",
              "      <td>0.643687</td>\n",
              "      <td>0.640753</td>\n",
              "      <td>0.637975</td>\n",
              "      <td>0.635578</td>\n",
              "      <td>0.633089</td>\n",
              "      <td>0.630470</td>\n",
              "      <td>0.627600</td>\n",
              "      <td>0.624106</td>\n",
              "      <td>0.622028</td>\n",
              "      <td>0.619228</td>\n",
              "      <td>0.617372</td>\n",
              "      <td>0.616006</td>\n",
              "      <td>0.614953</td>\n",
              "      <td>0.614089</td>\n",
              "      <td>0.614401</td>\n",
              "      <td>0.614494</td>\n",
              "      <td>0.615416</td>\n",
              "      <td>0.617037</td>\n",
              "      <td>0.619308</td>\n",
              "      <td>0.623084</td>\n",
              "      <td>0.626258</td>\n",
              "      <td>0.629867</td>\n",
              "      <td>0.634516</td>\n",
              "      <td>...</td>\n",
              "      <td>1.440180</td>\n",
              "      <td>1.429945</td>\n",
              "      <td>1.421697</td>\n",
              "      <td>1.412292</td>\n",
              "      <td>1.403772</td>\n",
              "      <td>1.395936</td>\n",
              "      <td>1.385628</td>\n",
              "      <td>1.377189</td>\n",
              "      <td>1.370285</td>\n",
              "      <td>1.363224</td>\n",
              "      <td>1.354198</td>\n",
              "      <td>1.347067</td>\n",
              "      <td>1.339531</td>\n",
              "      <td>1.332175</td>\n",
              "      <td>1.326958</td>\n",
              "      <td>1.320780</td>\n",
              "      <td>1.315768</td>\n",
              "      <td>1.311262</td>\n",
              "      <td>1.304993</td>\n",
              "      <td>1.299778</td>\n",
              "      <td>1.296999</td>\n",
              "      <td>1.292156</td>\n",
              "      <td>1.289398</td>\n",
              "      <td>1.285569</td>\n",
              "      <td>1.286972</td>\n",
              "      <td>1.283523</td>\n",
              "      <td>1.281386</td>\n",
              "      <td>1.279686</td>\n",
              "      <td>1.274868</td>\n",
              "      <td>1.271164</td>\n",
              "      <td>1.265842</td>\n",
              "      <td>1.263005</td>\n",
              "      <td>1.266337</td>\n",
              "      <td>1.297181</td>\n",
              "      <td>1.266201</td>\n",
              "      <td>11886</td>\n",
              "      <td>36.536500</td>\n",
              "      <td>65.203000</td>\n",
              "      <td>88.54285</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>406</td>\n",
              "      <td>0.587427</td>\n",
              "      <td>0.585250</td>\n",
              "      <td>0.589764</td>\n",
              "      <td>0.594061</td>\n",
              "      <td>0.599806</td>\n",
              "      <td>0.604902</td>\n",
              "      <td>0.609408</td>\n",
              "      <td>0.614733</td>\n",
              "      <td>0.620852</td>\n",
              "      <td>0.630668</td>\n",
              "      <td>0.641564</td>\n",
              "      <td>0.653106</td>\n",
              "      <td>0.657908</td>\n",
              "      <td>0.659866</td>\n",
              "      <td>0.660317</td>\n",
              "      <td>0.658655</td>\n",
              "      <td>0.655788</td>\n",
              "      <td>0.652237</td>\n",
              "      <td>0.648927</td>\n",
              "      <td>0.646154</td>\n",
              "      <td>0.643198</td>\n",
              "      <td>0.640632</td>\n",
              "      <td>0.637537</td>\n",
              "      <td>0.633689</td>\n",
              "      <td>0.631356</td>\n",
              "      <td>0.628085</td>\n",
              "      <td>0.625785</td>\n",
              "      <td>0.624119</td>\n",
              "      <td>0.622667</td>\n",
              "      <td>0.621206</td>\n",
              "      <td>0.620691</td>\n",
              "      <td>0.620383</td>\n",
              "      <td>0.620737</td>\n",
              "      <td>0.621599</td>\n",
              "      <td>0.623025</td>\n",
              "      <td>0.625937</td>\n",
              "      <td>0.628400</td>\n",
              "      <td>0.631338</td>\n",
              "      <td>0.635659</td>\n",
              "      <td>...</td>\n",
              "      <td>1.463203</td>\n",
              "      <td>1.454463</td>\n",
              "      <td>1.445494</td>\n",
              "      <td>1.436059</td>\n",
              "      <td>1.426811</td>\n",
              "      <td>1.417521</td>\n",
              "      <td>1.406092</td>\n",
              "      <td>1.397415</td>\n",
              "      <td>1.388936</td>\n",
              "      <td>1.383776</td>\n",
              "      <td>1.372381</td>\n",
              "      <td>1.364249</td>\n",
              "      <td>1.355797</td>\n",
              "      <td>1.347775</td>\n",
              "      <td>1.341057</td>\n",
              "      <td>1.333965</td>\n",
              "      <td>1.328754</td>\n",
              "      <td>1.321832</td>\n",
              "      <td>1.315572</td>\n",
              "      <td>1.309635</td>\n",
              "      <td>1.303772</td>\n",
              "      <td>1.299180</td>\n",
              "      <td>1.295138</td>\n",
              "      <td>1.292522</td>\n",
              "      <td>1.289935</td>\n",
              "      <td>1.287213</td>\n",
              "      <td>1.282831</td>\n",
              "      <td>1.277070</td>\n",
              "      <td>1.270260</td>\n",
              "      <td>1.253691</td>\n",
              "      <td>1.242504</td>\n",
              "      <td>1.236326</td>\n",
              "      <td>1.236166</td>\n",
              "      <td>1.250518</td>\n",
              "      <td>1.236380</td>\n",
              "      <td>11889</td>\n",
              "      <td>39.054667</td>\n",
              "      <td>56.012500</td>\n",
              "      <td>159.68645</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>407</td>\n",
              "      <td>0.477241</td>\n",
              "      <td>0.481792</td>\n",
              "      <td>0.483792</td>\n",
              "      <td>0.489861</td>\n",
              "      <td>0.497037</td>\n",
              "      <td>0.504735</td>\n",
              "      <td>0.509637</td>\n",
              "      <td>0.514202</td>\n",
              "      <td>0.520144</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.541345</td>\n",
              "      <td>0.550381</td>\n",
              "      <td>0.555803</td>\n",
              "      <td>0.558563</td>\n",
              "      <td>0.559343</td>\n",
              "      <td>0.558502</td>\n",
              "      <td>0.556676</td>\n",
              "      <td>0.554130</td>\n",
              "      <td>0.551494</td>\n",
              "      <td>0.547095</td>\n",
              "      <td>0.543851</td>\n",
              "      <td>0.540543</td>\n",
              "      <td>0.537108</td>\n",
              "      <td>0.533737</td>\n",
              "      <td>0.530775</td>\n",
              "      <td>0.527208</td>\n",
              "      <td>0.524530</td>\n",
              "      <td>0.523219</td>\n",
              "      <td>0.521271</td>\n",
              "      <td>0.520013</td>\n",
              "      <td>0.519327</td>\n",
              "      <td>0.518718</td>\n",
              "      <td>0.518963</td>\n",
              "      <td>0.519878</td>\n",
              "      <td>0.520868</td>\n",
              "      <td>0.522727</td>\n",
              "      <td>0.525264</td>\n",
              "      <td>0.528919</td>\n",
              "      <td>0.532656</td>\n",
              "      <td>...</td>\n",
              "      <td>1.494378</td>\n",
              "      <td>1.478303</td>\n",
              "      <td>1.463623</td>\n",
              "      <td>1.448563</td>\n",
              "      <td>1.434307</td>\n",
              "      <td>1.419192</td>\n",
              "      <td>1.402043</td>\n",
              "      <td>1.388613</td>\n",
              "      <td>1.375551</td>\n",
              "      <td>1.366222</td>\n",
              "      <td>1.351601</td>\n",
              "      <td>1.340437</td>\n",
              "      <td>1.327134</td>\n",
              "      <td>1.318319</td>\n",
              "      <td>1.309765</td>\n",
              "      <td>1.300136</td>\n",
              "      <td>1.292696</td>\n",
              "      <td>1.283508</td>\n",
              "      <td>1.275061</td>\n",
              "      <td>1.269340</td>\n",
              "      <td>1.263197</td>\n",
              "      <td>1.258333</td>\n",
              "      <td>1.253829</td>\n",
              "      <td>1.251677</td>\n",
              "      <td>1.249435</td>\n",
              "      <td>1.246182</td>\n",
              "      <td>1.243464</td>\n",
              "      <td>1.237783</td>\n",
              "      <td>1.227517</td>\n",
              "      <td>1.206936</td>\n",
              "      <td>1.179302</td>\n",
              "      <td>1.162252</td>\n",
              "      <td>1.156460</td>\n",
              "      <td>1.167845</td>\n",
              "      <td>1.159627</td>\n",
              "      <td>11890</td>\n",
              "      <td>46.116000</td>\n",
              "      <td>46.849000</td>\n",
              "      <td>99.20000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>408</td>\n",
              "      <td>0.565670</td>\n",
              "      <td>0.564820</td>\n",
              "      <td>0.569749</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.580790</td>\n",
              "      <td>0.586531</td>\n",
              "      <td>0.591289</td>\n",
              "      <td>0.596792</td>\n",
              "      <td>0.603492</td>\n",
              "      <td>0.613606</td>\n",
              "      <td>0.625469</td>\n",
              "      <td>0.637730</td>\n",
              "      <td>0.642756</td>\n",
              "      <td>0.645300</td>\n",
              "      <td>0.646095</td>\n",
              "      <td>0.644386</td>\n",
              "      <td>0.641853</td>\n",
              "      <td>0.638425</td>\n",
              "      <td>0.635619</td>\n",
              "      <td>0.633162</td>\n",
              "      <td>0.630523</td>\n",
              "      <td>0.627955</td>\n",
              "      <td>0.624878</td>\n",
              "      <td>0.621162</td>\n",
              "      <td>0.618793</td>\n",
              "      <td>0.615519</td>\n",
              "      <td>0.613485</td>\n",
              "      <td>0.611918</td>\n",
              "      <td>0.610617</td>\n",
              "      <td>0.609168</td>\n",
              "      <td>0.608868</td>\n",
              "      <td>0.608688</td>\n",
              "      <td>0.609372</td>\n",
              "      <td>0.610378</td>\n",
              "      <td>0.612109</td>\n",
              "      <td>0.615408</td>\n",
              "      <td>0.618294</td>\n",
              "      <td>0.621513</td>\n",
              "      <td>0.626012</td>\n",
              "      <td>...</td>\n",
              "      <td>1.506006</td>\n",
              "      <td>1.496877</td>\n",
              "      <td>1.486883</td>\n",
              "      <td>1.475934</td>\n",
              "      <td>1.466819</td>\n",
              "      <td>1.456462</td>\n",
              "      <td>1.445260</td>\n",
              "      <td>1.435506</td>\n",
              "      <td>1.426113</td>\n",
              "      <td>1.419729</td>\n",
              "      <td>1.409409</td>\n",
              "      <td>1.402097</td>\n",
              "      <td>1.391310</td>\n",
              "      <td>1.384826</td>\n",
              "      <td>1.377594</td>\n",
              "      <td>1.371748</td>\n",
              "      <td>1.366276</td>\n",
              "      <td>1.358685</td>\n",
              "      <td>1.354177</td>\n",
              "      <td>1.348492</td>\n",
              "      <td>1.342599</td>\n",
              "      <td>1.337098</td>\n",
              "      <td>1.332178</td>\n",
              "      <td>1.329692</td>\n",
              "      <td>1.329856</td>\n",
              "      <td>1.327782</td>\n",
              "      <td>1.323786</td>\n",
              "      <td>1.317777</td>\n",
              "      <td>1.305306</td>\n",
              "      <td>1.290804</td>\n",
              "      <td>1.276198</td>\n",
              "      <td>1.268350</td>\n",
              "      <td>1.263184</td>\n",
              "      <td>1.284706</td>\n",
              "      <td>1.269695</td>\n",
              "      <td>11891</td>\n",
              "      <td>40.384833</td>\n",
              "      <td>51.917167</td>\n",
              "      <td>164.71290</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>409</td>\n",
              "      <td>0.462006</td>\n",
              "      <td>0.463919</td>\n",
              "      <td>0.466006</td>\n",
              "      <td>0.470398</td>\n",
              "      <td>0.474160</td>\n",
              "      <td>0.479535</td>\n",
              "      <td>0.484806</td>\n",
              "      <td>0.490237</td>\n",
              "      <td>0.499680</td>\n",
              "      <td>0.509926</td>\n",
              "      <td>0.522268</td>\n",
              "      <td>0.532148</td>\n",
              "      <td>0.538059</td>\n",
              "      <td>0.542097</td>\n",
              "      <td>0.543162</td>\n",
              "      <td>0.542517</td>\n",
              "      <td>0.541707</td>\n",
              "      <td>0.539483</td>\n",
              "      <td>0.537062</td>\n",
              "      <td>0.533966</td>\n",
              "      <td>0.531556</td>\n",
              "      <td>0.529130</td>\n",
              "      <td>0.525741</td>\n",
              "      <td>0.521571</td>\n",
              "      <td>0.516842</td>\n",
              "      <td>0.513043</td>\n",
              "      <td>0.510353</td>\n",
              "      <td>0.507669</td>\n",
              "      <td>0.505915</td>\n",
              "      <td>0.503540</td>\n",
              "      <td>0.502311</td>\n",
              "      <td>0.502070</td>\n",
              "      <td>0.501865</td>\n",
              "      <td>0.502888</td>\n",
              "      <td>0.503878</td>\n",
              "      <td>0.506279</td>\n",
              "      <td>0.508641</td>\n",
              "      <td>0.511671</td>\n",
              "      <td>0.515179</td>\n",
              "      <td>...</td>\n",
              "      <td>1.332708</td>\n",
              "      <td>1.319521</td>\n",
              "      <td>1.308515</td>\n",
              "      <td>1.298146</td>\n",
              "      <td>1.288379</td>\n",
              "      <td>1.277194</td>\n",
              "      <td>1.267351</td>\n",
              "      <td>1.254786</td>\n",
              "      <td>1.246378</td>\n",
              "      <td>1.239163</td>\n",
              "      <td>1.228400</td>\n",
              "      <td>1.219838</td>\n",
              "      <td>1.210519</td>\n",
              "      <td>1.203041</td>\n",
              "      <td>1.196190</td>\n",
              "      <td>1.189647</td>\n",
              "      <td>1.183361</td>\n",
              "      <td>1.174566</td>\n",
              "      <td>1.170406</td>\n",
              "      <td>1.165284</td>\n",
              "      <td>1.159469</td>\n",
              "      <td>1.155536</td>\n",
              "      <td>1.153178</td>\n",
              "      <td>1.149640</td>\n",
              "      <td>1.149195</td>\n",
              "      <td>1.147773</td>\n",
              "      <td>1.148173</td>\n",
              "      <td>1.147194</td>\n",
              "      <td>1.146067</td>\n",
              "      <td>1.146250</td>\n",
              "      <td>1.149830</td>\n",
              "      <td>1.153864</td>\n",
              "      <td>1.162086</td>\n",
              "      <td>1.177496</td>\n",
              "      <td>1.168120</td>\n",
              "      <td>11966</td>\n",
              "      <td>36.485833</td>\n",
              "      <td>50.900167</td>\n",
              "      <td>98.99000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  absorbance0  ...  cholesterol_ldl_value  cholesterol_ldl_human\n",
              "537         405     0.559508  ...               88.54285                      1\n",
              "538         406     0.587427  ...              159.68645                      3\n",
              "539         407     0.477241  ...               99.20000                      1\n",
              "540         408     0.565670  ...              164.71290                      3\n",
              "541         409     0.462006  ...               98.99000                      1\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "NzXlFWOu0W2K",
        "outputId": "b30ba460-4951-45a1-f392-1632ac9cb40c"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>170.702952</td>\n",
              "      <td>0.520451</td>\n",
              "      <td>0.522427</td>\n",
              "      <td>0.525972</td>\n",
              "      <td>0.530680</td>\n",
              "      <td>0.536854</td>\n",
              "      <td>0.543551</td>\n",
              "      <td>0.549648</td>\n",
              "      <td>0.555232</td>\n",
              "      <td>0.563119</td>\n",
              "      <td>0.574407</td>\n",
              "      <td>0.587313</td>\n",
              "      <td>0.598591</td>\n",
              "      <td>0.605160</td>\n",
              "      <td>0.608182</td>\n",
              "      <td>0.609431</td>\n",
              "      <td>0.609150</td>\n",
              "      <td>0.607543</td>\n",
              "      <td>0.605090</td>\n",
              "      <td>0.602129</td>\n",
              "      <td>0.598775</td>\n",
              "      <td>0.595773</td>\n",
              "      <td>0.592707</td>\n",
              "      <td>0.589487</td>\n",
              "      <td>0.585683</td>\n",
              "      <td>0.582415</td>\n",
              "      <td>0.579220</td>\n",
              "      <td>0.576781</td>\n",
              "      <td>0.574689</td>\n",
              "      <td>0.572943</td>\n",
              "      <td>0.571480</td>\n",
              "      <td>0.570644</td>\n",
              "      <td>0.570247</td>\n",
              "      <td>0.570480</td>\n",
              "      <td>0.571400</td>\n",
              "      <td>0.572933</td>\n",
              "      <td>0.575169</td>\n",
              "      <td>0.577869</td>\n",
              "      <td>0.581283</td>\n",
              "      <td>0.585061</td>\n",
              "      <td>...</td>\n",
              "      <td>1.497814</td>\n",
              "      <td>1.485553</td>\n",
              "      <td>1.474109</td>\n",
              "      <td>1.463112</td>\n",
              "      <td>1.450626</td>\n",
              "      <td>1.439371</td>\n",
              "      <td>1.428135</td>\n",
              "      <td>1.417230</td>\n",
              "      <td>1.406798</td>\n",
              "      <td>1.398556</td>\n",
              "      <td>1.385953</td>\n",
              "      <td>1.377019</td>\n",
              "      <td>1.368533</td>\n",
              "      <td>1.360171</td>\n",
              "      <td>1.351819</td>\n",
              "      <td>1.344645</td>\n",
              "      <td>1.337181</td>\n",
              "      <td>1.330049</td>\n",
              "      <td>1.323455</td>\n",
              "      <td>1.316835</td>\n",
              "      <td>1.311906</td>\n",
              "      <td>1.307739</td>\n",
              "      <td>1.304195</td>\n",
              "      <td>1.301982</td>\n",
              "      <td>1.301178</td>\n",
              "      <td>1.300802</td>\n",
              "      <td>1.300394</td>\n",
              "      <td>1.299197</td>\n",
              "      <td>1.295539</td>\n",
              "      <td>1.289453</td>\n",
              "      <td>1.282270</td>\n",
              "      <td>1.275940</td>\n",
              "      <td>1.272953</td>\n",
              "      <td>1.287608</td>\n",
              "      <td>1.275556</td>\n",
              "      <td>6927.533210</td>\n",
              "      <td>42.111418</td>\n",
              "      <td>39.222955</td>\n",
              "      <td>111.436592</td>\n",
              "      <td>1.485240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>120.507914</td>\n",
              "      <td>0.040351</td>\n",
              "      <td>0.039984</td>\n",
              "      <td>0.040093</td>\n",
              "      <td>0.040035</td>\n",
              "      <td>0.040144</td>\n",
              "      <td>0.040521</td>\n",
              "      <td>0.040435</td>\n",
              "      <td>0.040376</td>\n",
              "      <td>0.040171</td>\n",
              "      <td>0.039981</td>\n",
              "      <td>0.039891</td>\n",
              "      <td>0.039888</td>\n",
              "      <td>0.039833</td>\n",
              "      <td>0.039825</td>\n",
              "      <td>0.039900</td>\n",
              "      <td>0.039866</td>\n",
              "      <td>0.039767</td>\n",
              "      <td>0.039718</td>\n",
              "      <td>0.039647</td>\n",
              "      <td>0.039748</td>\n",
              "      <td>0.039703</td>\n",
              "      <td>0.039702</td>\n",
              "      <td>0.039726</td>\n",
              "      <td>0.039846</td>\n",
              "      <td>0.040008</td>\n",
              "      <td>0.040042</td>\n",
              "      <td>0.040114</td>\n",
              "      <td>0.040156</td>\n",
              "      <td>0.040191</td>\n",
              "      <td>0.040191</td>\n",
              "      <td>0.040187</td>\n",
              "      <td>0.040152</td>\n",
              "      <td>0.040178</td>\n",
              "      <td>0.040161</td>\n",
              "      <td>0.040140</td>\n",
              "      <td>0.040145</td>\n",
              "      <td>0.040169</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.040271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091700</td>\n",
              "      <td>0.090305</td>\n",
              "      <td>0.088924</td>\n",
              "      <td>0.087603</td>\n",
              "      <td>0.085971</td>\n",
              "      <td>0.084668</td>\n",
              "      <td>0.083410</td>\n",
              "      <td>0.082486</td>\n",
              "      <td>0.081525</td>\n",
              "      <td>0.080697</td>\n",
              "      <td>0.079354</td>\n",
              "      <td>0.078591</td>\n",
              "      <td>0.077906</td>\n",
              "      <td>0.077287</td>\n",
              "      <td>0.076565</td>\n",
              "      <td>0.076015</td>\n",
              "      <td>0.075402</td>\n",
              "      <td>0.074832</td>\n",
              "      <td>0.074297</td>\n",
              "      <td>0.073843</td>\n",
              "      <td>0.073528</td>\n",
              "      <td>0.073310</td>\n",
              "      <td>0.073180</td>\n",
              "      <td>0.073283</td>\n",
              "      <td>0.073591</td>\n",
              "      <td>0.074264</td>\n",
              "      <td>0.075230</td>\n",
              "      <td>0.076718</td>\n",
              "      <td>0.079323</td>\n",
              "      <td>0.083310</td>\n",
              "      <td>0.088549</td>\n",
              "      <td>0.093928</td>\n",
              "      <td>0.098151</td>\n",
              "      <td>0.104133</td>\n",
              "      <td>0.101605</td>\n",
              "      <td>3094.955454</td>\n",
              "      <td>3.420590</td>\n",
              "      <td>8.948404</td>\n",
              "      <td>44.306810</td>\n",
              "      <td>1.031287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400606</td>\n",
              "      <td>0.403625</td>\n",
              "      <td>0.407266</td>\n",
              "      <td>0.411518</td>\n",
              "      <td>0.416126</td>\n",
              "      <td>0.421518</td>\n",
              "      <td>0.426629</td>\n",
              "      <td>0.433139</td>\n",
              "      <td>0.444069</td>\n",
              "      <td>0.456182</td>\n",
              "      <td>0.469249</td>\n",
              "      <td>0.480239</td>\n",
              "      <td>0.486473</td>\n",
              "      <td>0.489661</td>\n",
              "      <td>0.490777</td>\n",
              "      <td>0.490226</td>\n",
              "      <td>0.488814</td>\n",
              "      <td>0.486476</td>\n",
              "      <td>0.483668</td>\n",
              "      <td>0.479653</td>\n",
              "      <td>0.476146</td>\n",
              "      <td>0.473155</td>\n",
              "      <td>0.469834</td>\n",
              "      <td>0.466424</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.459335</td>\n",
              "      <td>0.457297</td>\n",
              "      <td>0.455538</td>\n",
              "      <td>0.453853</td>\n",
              "      <td>0.452486</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>0.451624</td>\n",
              "      <td>0.452026</td>\n",
              "      <td>0.453133</td>\n",
              "      <td>0.454851</td>\n",
              "      <td>0.457279</td>\n",
              "      <td>0.460600</td>\n",
              "      <td>0.463820</td>\n",
              "      <td>0.467916</td>\n",
              "      <td>...</td>\n",
              "      <td>1.202957</td>\n",
              "      <td>1.193580</td>\n",
              "      <td>1.184336</td>\n",
              "      <td>1.175013</td>\n",
              "      <td>1.163368</td>\n",
              "      <td>1.154944</td>\n",
              "      <td>1.145616</td>\n",
              "      <td>1.137301</td>\n",
              "      <td>1.128859</td>\n",
              "      <td>1.118666</td>\n",
              "      <td>1.110864</td>\n",
              "      <td>1.102883</td>\n",
              "      <td>1.096296</td>\n",
              "      <td>1.089810</td>\n",
              "      <td>1.080864</td>\n",
              "      <td>1.075002</td>\n",
              "      <td>1.068992</td>\n",
              "      <td>1.063488</td>\n",
              "      <td>1.057937</td>\n",
              "      <td>1.052125</td>\n",
              "      <td>1.047685</td>\n",
              "      <td>1.044301</td>\n",
              "      <td>1.041746</td>\n",
              "      <td>1.039873</td>\n",
              "      <td>1.038416</td>\n",
              "      <td>1.039063</td>\n",
              "      <td>1.037324</td>\n",
              "      <td>1.036100</td>\n",
              "      <td>1.032403</td>\n",
              "      <td>1.026585</td>\n",
              "      <td>1.020585</td>\n",
              "      <td>1.004983</td>\n",
              "      <td>0.981338</td>\n",
              "      <td>0.969416</td>\n",
              "      <td>0.950952</td>\n",
              "      <td>1974.000000</td>\n",
              "      <td>31.384333</td>\n",
              "      <td>16.136667</td>\n",
              "      <td>5.300000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>68.000000</td>\n",
              "      <td>0.492162</td>\n",
              "      <td>0.494457</td>\n",
              "      <td>0.498396</td>\n",
              "      <td>0.502493</td>\n",
              "      <td>0.508495</td>\n",
              "      <td>0.513840</td>\n",
              "      <td>0.520323</td>\n",
              "      <td>0.526687</td>\n",
              "      <td>0.535248</td>\n",
              "      <td>0.547601</td>\n",
              "      <td>0.561213</td>\n",
              "      <td>0.572464</td>\n",
              "      <td>0.579220</td>\n",
              "      <td>0.582228</td>\n",
              "      <td>0.583382</td>\n",
              "      <td>0.583044</td>\n",
              "      <td>0.581269</td>\n",
              "      <td>0.578919</td>\n",
              "      <td>0.576212</td>\n",
              "      <td>0.572606</td>\n",
              "      <td>0.569006</td>\n",
              "      <td>0.565879</td>\n",
              "      <td>0.562660</td>\n",
              "      <td>0.558955</td>\n",
              "      <td>0.555664</td>\n",
              "      <td>0.552372</td>\n",
              "      <td>0.549645</td>\n",
              "      <td>0.547153</td>\n",
              "      <td>0.545535</td>\n",
              "      <td>0.543872</td>\n",
              "      <td>0.543126</td>\n",
              "      <td>0.542971</td>\n",
              "      <td>0.543221</td>\n",
              "      <td>0.544237</td>\n",
              "      <td>0.545786</td>\n",
              "      <td>0.548325</td>\n",
              "      <td>0.551150</td>\n",
              "      <td>0.554563</td>\n",
              "      <td>0.558216</td>\n",
              "      <td>...</td>\n",
              "      <td>1.438055</td>\n",
              "      <td>1.427390</td>\n",
              "      <td>1.418313</td>\n",
              "      <td>1.408198</td>\n",
              "      <td>1.397676</td>\n",
              "      <td>1.386605</td>\n",
              "      <td>1.375685</td>\n",
              "      <td>1.365357</td>\n",
              "      <td>1.355396</td>\n",
              "      <td>1.347791</td>\n",
              "      <td>1.337279</td>\n",
              "      <td>1.328478</td>\n",
              "      <td>1.320503</td>\n",
              "      <td>1.312188</td>\n",
              "      <td>1.303734</td>\n",
              "      <td>1.297646</td>\n",
              "      <td>1.291290</td>\n",
              "      <td>1.283525</td>\n",
              "      <td>1.277472</td>\n",
              "      <td>1.271808</td>\n",
              "      <td>1.266912</td>\n",
              "      <td>1.263202</td>\n",
              "      <td>1.259202</td>\n",
              "      <td>1.257043</td>\n",
              "      <td>1.255547</td>\n",
              "      <td>1.254069</td>\n",
              "      <td>1.251106</td>\n",
              "      <td>1.250801</td>\n",
              "      <td>1.245398</td>\n",
              "      <td>1.236956</td>\n",
              "      <td>1.228441</td>\n",
              "      <td>1.216640</td>\n",
              "      <td>1.214438</td>\n",
              "      <td>1.219523</td>\n",
              "      <td>1.210930</td>\n",
              "      <td>4150.500000</td>\n",
              "      <td>39.420750</td>\n",
              "      <td>33.587333</td>\n",
              "      <td>81.347500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>136.500000</td>\n",
              "      <td>0.517637</td>\n",
              "      <td>0.519890</td>\n",
              "      <td>0.524229</td>\n",
              "      <td>0.528808</td>\n",
              "      <td>0.534595</td>\n",
              "      <td>0.541708</td>\n",
              "      <td>0.547380</td>\n",
              "      <td>0.553991</td>\n",
              "      <td>0.561483</td>\n",
              "      <td>0.572200</td>\n",
              "      <td>0.585533</td>\n",
              "      <td>0.597325</td>\n",
              "      <td>0.603621</td>\n",
              "      <td>0.607155</td>\n",
              "      <td>0.608601</td>\n",
              "      <td>0.608395</td>\n",
              "      <td>0.606871</td>\n",
              "      <td>0.604356</td>\n",
              "      <td>0.601704</td>\n",
              "      <td>0.598210</td>\n",
              "      <td>0.594825</td>\n",
              "      <td>0.591652</td>\n",
              "      <td>0.588568</td>\n",
              "      <td>0.584530</td>\n",
              "      <td>0.581085</td>\n",
              "      <td>0.577706</td>\n",
              "      <td>0.575218</td>\n",
              "      <td>0.573276</td>\n",
              "      <td>0.571611</td>\n",
              "      <td>0.570549</td>\n",
              "      <td>0.569648</td>\n",
              "      <td>0.569021</td>\n",
              "      <td>0.568656</td>\n",
              "      <td>0.569177</td>\n",
              "      <td>0.570510</td>\n",
              "      <td>0.572768</td>\n",
              "      <td>0.575309</td>\n",
              "      <td>0.578820</td>\n",
              "      <td>0.582439</td>\n",
              "      <td>...</td>\n",
              "      <td>1.509993</td>\n",
              "      <td>1.496886</td>\n",
              "      <td>1.484712</td>\n",
              "      <td>1.472726</td>\n",
              "      <td>1.459901</td>\n",
              "      <td>1.447849</td>\n",
              "      <td>1.434799</td>\n",
              "      <td>1.422255</td>\n",
              "      <td>1.411197</td>\n",
              "      <td>1.402132</td>\n",
              "      <td>1.389828</td>\n",
              "      <td>1.380030</td>\n",
              "      <td>1.371024</td>\n",
              "      <td>1.361727</td>\n",
              "      <td>1.351912</td>\n",
              "      <td>1.344241</td>\n",
              "      <td>1.337637</td>\n",
              "      <td>1.330476</td>\n",
              "      <td>1.324922</td>\n",
              "      <td>1.318474</td>\n",
              "      <td>1.313972</td>\n",
              "      <td>1.309751</td>\n",
              "      <td>1.306991</td>\n",
              "      <td>1.304351</td>\n",
              "      <td>1.303537</td>\n",
              "      <td>1.303173</td>\n",
              "      <td>1.302108</td>\n",
              "      <td>1.300589</td>\n",
              "      <td>1.296182</td>\n",
              "      <td>1.291905</td>\n",
              "      <td>1.282205</td>\n",
              "      <td>1.273889</td>\n",
              "      <td>1.269692</td>\n",
              "      <td>1.286520</td>\n",
              "      <td>1.271123</td>\n",
              "      <td>7086.500000</td>\n",
              "      <td>41.806083</td>\n",
              "      <td>38.467583</td>\n",
              "      <td>109.850000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>273.750000</td>\n",
              "      <td>0.545690</td>\n",
              "      <td>0.547908</td>\n",
              "      <td>0.552079</td>\n",
              "      <td>0.557400</td>\n",
              "      <td>0.564152</td>\n",
              "      <td>0.571383</td>\n",
              "      <td>0.577585</td>\n",
              "      <td>0.582518</td>\n",
              "      <td>0.590188</td>\n",
              "      <td>0.600986</td>\n",
              "      <td>0.614931</td>\n",
              "      <td>0.626134</td>\n",
              "      <td>0.632340</td>\n",
              "      <td>0.635539</td>\n",
              "      <td>0.636588</td>\n",
              "      <td>0.635807</td>\n",
              "      <td>0.634326</td>\n",
              "      <td>0.632133</td>\n",
              "      <td>0.629180</td>\n",
              "      <td>0.625997</td>\n",
              "      <td>0.623065</td>\n",
              "      <td>0.619732</td>\n",
              "      <td>0.616301</td>\n",
              "      <td>0.611912</td>\n",
              "      <td>0.609308</td>\n",
              "      <td>0.606632</td>\n",
              "      <td>0.604324</td>\n",
              "      <td>0.602069</td>\n",
              "      <td>0.600773</td>\n",
              "      <td>0.599572</td>\n",
              "      <td>0.598980</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.599201</td>\n",
              "      <td>0.600384</td>\n",
              "      <td>0.602131</td>\n",
              "      <td>0.604517</td>\n",
              "      <td>0.607264</td>\n",
              "      <td>0.610573</td>\n",
              "      <td>0.614266</td>\n",
              "      <td>...</td>\n",
              "      <td>1.559759</td>\n",
              "      <td>1.547204</td>\n",
              "      <td>1.533789</td>\n",
              "      <td>1.521686</td>\n",
              "      <td>1.508628</td>\n",
              "      <td>1.496775</td>\n",
              "      <td>1.483442</td>\n",
              "      <td>1.472454</td>\n",
              "      <td>1.460157</td>\n",
              "      <td>1.452229</td>\n",
              "      <td>1.438763</td>\n",
              "      <td>1.429058</td>\n",
              "      <td>1.419599</td>\n",
              "      <td>1.409979</td>\n",
              "      <td>1.400365</td>\n",
              "      <td>1.392393</td>\n",
              "      <td>1.383955</td>\n",
              "      <td>1.376497</td>\n",
              "      <td>1.369636</td>\n",
              "      <td>1.363443</td>\n",
              "      <td>1.357990</td>\n",
              "      <td>1.353691</td>\n",
              "      <td>1.350482</td>\n",
              "      <td>1.348698</td>\n",
              "      <td>1.347153</td>\n",
              "      <td>1.347032</td>\n",
              "      <td>1.346518</td>\n",
              "      <td>1.345891</td>\n",
              "      <td>1.342652</td>\n",
              "      <td>1.341573</td>\n",
              "      <td>1.338421</td>\n",
              "      <td>1.332192</td>\n",
              "      <td>1.332948</td>\n",
              "      <td>1.351667</td>\n",
              "      <td>1.339838</td>\n",
              "      <td>9618.500000</td>\n",
              "      <td>44.381750</td>\n",
              "      <td>45.301333</td>\n",
              "      <td>136.480000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>409.000000</td>\n",
              "      <td>0.696677</td>\n",
              "      <td>0.696001</td>\n",
              "      <td>0.697815</td>\n",
              "      <td>0.700696</td>\n",
              "      <td>0.704963</td>\n",
              "      <td>0.709175</td>\n",
              "      <td>0.716203</td>\n",
              "      <td>0.722194</td>\n",
              "      <td>0.731920</td>\n",
              "      <td>0.744236</td>\n",
              "      <td>0.757309</td>\n",
              "      <td>0.771393</td>\n",
              "      <td>0.778315</td>\n",
              "      <td>0.782179</td>\n",
              "      <td>0.782835</td>\n",
              "      <td>0.782078</td>\n",
              "      <td>0.779667</td>\n",
              "      <td>0.776754</td>\n",
              "      <td>0.774371</td>\n",
              "      <td>0.771715</td>\n",
              "      <td>0.769261</td>\n",
              "      <td>0.766669</td>\n",
              "      <td>0.763244</td>\n",
              "      <td>0.759328</td>\n",
              "      <td>0.756597</td>\n",
              "      <td>0.752855</td>\n",
              "      <td>0.750512</td>\n",
              "      <td>0.748430</td>\n",
              "      <td>0.746127</td>\n",
              "      <td>0.743960</td>\n",
              "      <td>0.742321</td>\n",
              "      <td>0.741800</td>\n",
              "      <td>0.741419</td>\n",
              "      <td>0.741988</td>\n",
              "      <td>0.743058</td>\n",
              "      <td>0.745721</td>\n",
              "      <td>0.748387</td>\n",
              "      <td>0.751435</td>\n",
              "      <td>0.755251</td>\n",
              "      <td>...</td>\n",
              "      <td>1.752160</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.729738</td>\n",
              "      <td>1.718851</td>\n",
              "      <td>1.703801</td>\n",
              "      <td>1.691461</td>\n",
              "      <td>1.676763</td>\n",
              "      <td>1.665650</td>\n",
              "      <td>1.656921</td>\n",
              "      <td>1.639813</td>\n",
              "      <td>1.628050</td>\n",
              "      <td>1.619605</td>\n",
              "      <td>1.606320</td>\n",
              "      <td>1.599001</td>\n",
              "      <td>1.585844</td>\n",
              "      <td>1.576560</td>\n",
              "      <td>1.566763</td>\n",
              "      <td>1.559222</td>\n",
              "      <td>1.548711</td>\n",
              "      <td>1.540025</td>\n",
              "      <td>1.533886</td>\n",
              "      <td>1.530444</td>\n",
              "      <td>1.530373</td>\n",
              "      <td>1.528527</td>\n",
              "      <td>1.529409</td>\n",
              "      <td>1.531209</td>\n",
              "      <td>1.535637</td>\n",
              "      <td>1.539162</td>\n",
              "      <td>1.539344</td>\n",
              "      <td>1.541236</td>\n",
              "      <td>1.543308</td>\n",
              "      <td>1.550344</td>\n",
              "      <td>1.552939</td>\n",
              "      <td>1.599580</td>\n",
              "      <td>1.575339</td>\n",
              "      <td>11966.000000</td>\n",
              "      <td>52.678500</td>\n",
              "      <td>65.659833</td>\n",
              "      <td>310.800000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  absorbance0  ...  cholesterol_ldl_value  cholesterol_ldl_human\n",
              "count  542.000000   542.000000  ...             542.000000             542.000000\n",
              "mean   170.702952     0.520451  ...             111.436592               1.485240\n",
              "std    120.507914     0.040351  ...              44.306810               1.031287\n",
              "min      0.000000     0.400606  ...               5.300000               0.000000\n",
              "25%     68.000000     0.492162  ...              81.347500               1.000000\n",
              "50%    136.500000     0.517637  ...             109.850000               1.000000\n",
              "75%    273.750000     0.545690  ...             136.480000               3.000000\n",
              "max    409.000000     0.696677  ...             310.800000               3.000000\n",
              "\n",
              "[8 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout,Embedding, Flatten,LayerNormalization\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "import keras.optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld,mse\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "YTJL576ZmB3U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_columns=df_agg.columns[1:-5]\n",
        "y_columns=df_agg.columns[-1] # the category is at the end of columns\n",
        "X_=df_agg[X_columns]\n",
        "y_=df_agg[y_columns]"
      ],
      "metadata": {
        "id": "tyAOeKvNmIFk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP82OUkI-Ban",
        "outputId": "a97f1ca1-6df0-4f10-f7ef-c6c5f7c65f63"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['absorbance0', 'absorbance1', 'absorbance2', 'absorbance3',\n",
              "       'absorbance4', 'absorbance5', 'absorbance6', 'absorbance7',\n",
              "       'absorbance8', 'absorbance9',\n",
              "       ...\n",
              "       'absorbance160', 'absorbance161', 'absorbance162', 'absorbance163',\n",
              "       'absorbance164', 'absorbance165', 'absorbance166', 'absorbance167',\n",
              "       'absorbance168', 'absorbance169'],\n",
              "      dtype='object', length=170)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_.replace(3,2, inplace=True) # the generated file contains  0,1,3 as the category label :-( "
      ],
      "metadata": {
        "id": "x-ZQDORU8dSB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_.replace(2,0, inplace=True)"
      ],
      "metadata": {
        "id": "ktI0xxn_8_1c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_=np.array(y_)"
      ],
      "metadata": {
        "id": "T1yh_hqpqEt4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_=y_.reshape(-1,1)"
      ],
      "metadata": {
        "id": "Mstvf1lXqSEe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "ugaWW0KOq90O",
        "outputId": "bcedbe45-6f25-4e48-b5c0-ba053e23006e"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cholesterol_ldl_human'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_columns=df_agg.columns[1:-5]\n",
        "y_columns=df_agg.columns[-1] # the category is at the end of columns\n",
        "X_=df_agg[X_columns]\n",
        "y_=df_agg[y_columns]\n",
        "\n",
        "y_.replace(3,2, inplace=True) # the generated file contains  0,1,3 as the category label :-( \n",
        "\n",
        "y_=np.array(y_)\n",
        "y_=y_.reshape(-1,1)\n",
        "\n",
        "\n",
        "\n",
        "__X_SCALER__=\"normal\"\n",
        "\n",
        "__DNN_MODE__=\"regression\"\n",
        "\n",
        "__Y_SCALER__=\"minmax\"\n",
        "\n",
        "if __DNN_MODE__==\"regression\":\n",
        "    y_columns=df_agg.columns[-2] # the category is at the  -2  column\n",
        "    y_=df_agg[y_columns]\n",
        "    y_=np.array(y_)\n",
        "    y_=y_.reshape(-1,1)\n",
        "\n",
        "\n",
        "elif __DNN_MODE__==\"classification\":\n",
        "    y_columns=df_agg.columns[-1] # the category is at the end of columns\n",
        "    y_=df_agg[y_columns]\n",
        "    y_.replace(3,2, inplace=True) # the generated file contains  0,1,3 as the category label :-( \n",
        "    y_=np.array(y_)\n",
        "    y_=y_.reshape(-1,1)\n",
        "\n",
        "y=y_\n",
        "print(y[0:10])\n",
        "\n",
        "if __X_SCALER__==\"normal\":\n",
        "    X_normalizer = StandardScaler() \n",
        "    X=X_normalizer.fit_transform(X_)\n",
        "\n",
        "if __Y_SCALER__==\"normal\":\n",
        "    y_normalizer = StandardScaler() \n",
        "    y=y_normalizer.fit_transform(y_,)\n",
        "    print(f\"Itt: {y}\")\n",
        "\n",
        "if __X_SCALER__==\"minmax\":\n",
        "    X_minmax = MinMaxScaler() \n",
        "    X=X_minmax.fit_transform(X_)\n",
        "\n",
        "if __Y_SCALER__==\"minmax\":\n",
        "    y_minmax = MinMaxScaler() \n",
        "    y=y_minmax.fit_transform(y_)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ__bM5BpIaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ab0fa8-1cf0-45d5-910d-43f6f688494f"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[105.6]\n",
            " [ 72.4]\n",
            " [ 68.4]\n",
            " [106. ]\n",
            " [ 31.2]\n",
            " [ 40.6]\n",
            " [ 82.6]\n",
            " [ 32.7]\n",
            " [ 64.2]\n",
            " [151. ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_normalizer.get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOpMpaVeFrrX",
        "outputId": "a2ab31d7-bdfb-4b15-b648-19c929a6fef1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'copy': True, 'with_mean': True, 'with_std': True}"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-ALi-6SrjHS",
        "outputId": "88ca2a2f-763f-494d-9455-cf467ecb015f"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.32831424],\n",
              "       [0.21963993],\n",
              "       [0.20654664],\n",
              "       [0.32962357],\n",
              "       [0.08477905],\n",
              "       [0.11554828],\n",
              "       [0.25302782],\n",
              "       [0.08968903],\n",
              "       [0.19279869],\n",
              "       [0.47692308],\n",
              "       [0.29296236],\n",
              "       [0.23993453],\n",
              "       [0.34533552],\n",
              "       [0.46775777],\n",
              "       [0.25302782],\n",
              "       [0.40883797],\n",
              "       [0.67528642],\n",
              "       [0.15351882],\n",
              "       [0.2811784 ],\n",
              "       [0.10572831],\n",
              "       [0.3806874 ],\n",
              "       [0.35711948],\n",
              "       [0.35777414],\n",
              "       [0.3695581 ],\n",
              "       [0.2386252 ],\n",
              "       [0.31914894],\n",
              "       [0.59934534],\n",
              "       [0.43371522],\n",
              "       [0.4212766 ],\n",
              "       [0.52405892],\n",
              "       [0.29427169],\n",
              "       [0.20589198],\n",
              "       [0.498527  ],\n",
              "       [0.498527  ],\n",
              "       [0.22487725],\n",
              "       [0.24189853],\n",
              "       [0.46513912],\n",
              "       [0.37217676],\n",
              "       [0.48281506],\n",
              "       [0.39849427],\n",
              "       [0.30343699],\n",
              "       [0.12981997],\n",
              "       [0.1894599 ],\n",
              "       [0.33165303],\n",
              "       [0.26278232],\n",
              "       [0.34297872],\n",
              "       [0.26599018],\n",
              "       [0.28229133],\n",
              "       [0.38291326],\n",
              "       [0.13780687],\n",
              "       [0.36667758],\n",
              "       [0.31849427],\n",
              "       [0.34507365],\n",
              "       [0.19810147],\n",
              "       [0.03777414],\n",
              "       [0.32635025],\n",
              "       [0.35895254],\n",
              "       [0.44779051],\n",
              "       [0.22160393],\n",
              "       [0.2608838 ],\n",
              "       [0.43469722],\n",
              "       [0.44844517],\n",
              "       [0.3802946 ],\n",
              "       [0.4599018 ],\n",
              "       [0.22612111],\n",
              "       [0.58232406],\n",
              "       [0.35057283],\n",
              "       [0.29237316],\n",
              "       [0.14310966],\n",
              "       [0.33204583],\n",
              "       [0.45806874],\n",
              "       [0.38788871],\n",
              "       [0.43780687],\n",
              "       [0.20523732],\n",
              "       [0.33780687],\n",
              "       [0.33865794],\n",
              "       [0.33027823],\n",
              "       [0.28379705],\n",
              "       [0.30343699],\n",
              "       [0.49047463],\n",
              "       [0.34599018],\n",
              "       [0.18402619],\n",
              "       [0.43764321],\n",
              "       [0.51947627],\n",
              "       [0.36445172],\n",
              "       [0.28484452],\n",
              "       [0.35581015],\n",
              "       [0.39495908],\n",
              "       [0.11155483],\n",
              "       [0.43613748],\n",
              "       [0.50860884],\n",
              "       [0.59456628],\n",
              "       [0.33459902],\n",
              "       [0.37211129],\n",
              "       [0.28248773],\n",
              "       [0.44517185],\n",
              "       [0.35816694],\n",
              "       [0.32104746],\n",
              "       [0.11816694],\n",
              "       [0.38448445],\n",
              "       [0.38121113],\n",
              "       [0.18533552],\n",
              "       [0.35279869],\n",
              "       [0.27423895],\n",
              "       [0.30513912],\n",
              "       [0.35319149],\n",
              "       [0.59495908],\n",
              "       [0.49846154],\n",
              "       [0.36235679],\n",
              "       [0.33289689],\n",
              "       [0.21178396],\n",
              "       [0.19214403],\n",
              "       [0.48346972],\n",
              "       [0.38134206],\n",
              "       [0.50821604],\n",
              "       [0.2888707 ],\n",
              "       [0.24451718],\n",
              "       [0.12890344],\n",
              "       [0.38559738],\n",
              "       [0.40818331],\n",
              "       [0.4801964 ],\n",
              "       [0.22487725],\n",
              "       [0.28477905],\n",
              "       [0.2903437 ],\n",
              "       [0.0893617 ],\n",
              "       [0.21178396],\n",
              "       [0.4801964 ],\n",
              "       [0.62094926],\n",
              "       [0.15220949],\n",
              "       [0.09067103],\n",
              "       [0.        ],\n",
              "       [0.        ],\n",
              "       [0.16726678],\n",
              "       [0.08707038],\n",
              "       [0.40883797],\n",
              "       [0.23600655],\n",
              "       [0.40294599],\n",
              "       [0.41276596],\n",
              "       [0.49263502],\n",
              "       [0.69492635],\n",
              "       [0.37283142],\n",
              "       [0.42454992],\n",
              "       [0.35973813],\n",
              "       [0.33027823],\n",
              "       [0.42978723],\n",
              "       [0.40490998],\n",
              "       [0.37675941],\n",
              "       [0.39509002],\n",
              "       [0.13060556],\n",
              "       [0.29885434],\n",
              "       [0.52471358],\n",
              "       [0.67463175],\n",
              "       [0.34991817],\n",
              "       [0.21374795],\n",
              "       [0.39639935],\n",
              "       [0.20458265],\n",
              "       [0.18494272],\n",
              "       [0.32176759],\n",
              "       [0.47037643],\n",
              "       [0.46317512],\n",
              "       [0.34337152],\n",
              "       [0.46382979],\n",
              "       [0.38788871],\n",
              "       [0.30081833],\n",
              "       [0.40883797],\n",
              "       [0.36301146],\n",
              "       [0.22291326],\n",
              "       [0.15482815],\n",
              "       [0.52144026],\n",
              "       [0.29165303],\n",
              "       [0.34402619],\n",
              "       [0.21833061],\n",
              "       [0.33486088],\n",
              "       [0.22880524],\n",
              "       [0.5888707 ],\n",
              "       [0.14173486],\n",
              "       [0.38003273],\n",
              "       [0.6386252 ],\n",
              "       [0.34140753],\n",
              "       [0.23404255],\n",
              "       [0.36824877],\n",
              "       [0.33813421],\n",
              "       [0.11096563],\n",
              "       [0.3086743 ],\n",
              "       [0.21636661],\n",
              "       [0.26808511],\n",
              "       [0.21047463],\n",
              "       [0.46055646],\n",
              "       [0.21898527],\n",
              "       [0.6386252 ],\n",
              "       [0.22312111],\n",
              "       [0.34206219],\n",
              "       [0.33551555],\n",
              "       [0.27345336],\n",
              "       [0.32307692],\n",
              "       [0.46317512],\n",
              "       [0.13846154],\n",
              "       [0.31391162],\n",
              "       [0.13453355],\n",
              "       [0.24320786],\n",
              "       [0.29492635],\n",
              "       [0.3787234 ],\n",
              "       [0.35319149],\n",
              "       [0.19345336],\n",
              "       [0.34238953],\n",
              "       [0.48674304],\n",
              "       [0.58291326],\n",
              "       [0.28844517],\n",
              "       [0.17171849],\n",
              "       [0.19135843],\n",
              "       [0.42454992],\n",
              "       [0.40392799],\n",
              "       [0.26677578],\n",
              "       [0.57813421],\n",
              "       [0.46369885],\n",
              "       [0.43240589],\n",
              "       [0.29361702],\n",
              "       [0.26743044],\n",
              "       [0.37283142],\n",
              "       [0.22487725],\n",
              "       [0.39472995],\n",
              "       [0.28052373],\n",
              "       [0.42821604],\n",
              "       [0.35581015],\n",
              "       [0.2296563 ],\n",
              "       [0.13394435],\n",
              "       [0.22487725],\n",
              "       [0.2608838 ],\n",
              "       [0.27397709],\n",
              "       [0.29689034],\n",
              "       [0.33944354],\n",
              "       [0.33944354],\n",
              "       [0.21178396],\n",
              "       [0.37545008],\n",
              "       [0.20523732],\n",
              "       [0.22815057],\n",
              "       [0.30016367],\n",
              "       [0.39836334],\n",
              "       [0.19541735],\n",
              "       [0.40883797],\n",
              "       [0.08373159],\n",
              "       [0.07446809],\n",
              "       [0.31653028],\n",
              "       [0.40163666],\n",
              "       [0.50756137],\n",
              "       [0.29368249],\n",
              "       [0.41407529],\n",
              "       [0.36873977],\n",
              "       [0.35908347],\n",
              "       [0.05793781],\n",
              "       [0.46382979],\n",
              "       [0.29361702],\n",
              "       [0.14959083],\n",
              "       [0.23469722],\n",
              "       [0.19214403],\n",
              "       [0.15109656],\n",
              "       [0.16530278],\n",
              "       [0.27731588],\n",
              "       [0.22487725],\n",
              "       [0.31803601],\n",
              "       [0.3492635 ],\n",
              "       [0.21833061],\n",
              "       [0.39266776],\n",
              "       [0.30310966],\n",
              "       [0.17342062],\n",
              "       [0.47364975],\n",
              "       [0.26743044],\n",
              "       [0.3198036 ],\n",
              "       [0.40130933],\n",
              "       [0.52929624],\n",
              "       [0.3198036 ],\n",
              "       [0.33617021],\n",
              "       [0.11031097],\n",
              "       [0.24860884],\n",
              "       [0.23797054],\n",
              "       [0.35581015],\n",
              "       [0.25630115],\n",
              "       [0.35253682],\n",
              "       [0.42454992],\n",
              "       [0.41865794],\n",
              "       [0.42690671],\n",
              "       [0.38245499],\n",
              "       [0.39279869],\n",
              "       [0.25662848],\n",
              "       [0.09067103],\n",
              "       [0.28379705],\n",
              "       [0.39266776],\n",
              "       [0.26972177],\n",
              "       [0.24988543],\n",
              "       [0.51292962],\n",
              "       [0.28379705],\n",
              "       [0.37545008],\n",
              "       [0.3198036 ],\n",
              "       [0.35725041],\n",
              "       [0.28517185],\n",
              "       [0.57643208],\n",
              "       [0.34271686],\n",
              "       [0.39371522],\n",
              "       [0.18232406],\n",
              "       [0.22873977],\n",
              "       [0.37407529],\n",
              "       [0.46841244],\n",
              "       [0.22880524],\n",
              "       [0.29787234],\n",
              "       [0.31653028],\n",
              "       [0.18559738],\n",
              "       [0.29237316],\n",
              "       [0.27332242],\n",
              "       [0.23967267],\n",
              "       [0.36235679],\n",
              "       [0.4801964 ],\n",
              "       [0.33289689],\n",
              "       [0.25466448],\n",
              "       [0.39509002],\n",
              "       [0.08412439],\n",
              "       [0.27725041],\n",
              "       [0.2903437 ],\n",
              "       [0.4801964 ],\n",
              "       [0.11031097],\n",
              "       [0.25433715],\n",
              "       [0.26415712],\n",
              "       [0.35682488],\n",
              "       [0.30507365],\n",
              "       [0.22880524],\n",
              "       [0.29374795],\n",
              "       [0.4212766 ],\n",
              "       [0.37545008],\n",
              "       [0.45708674],\n",
              "       [0.18353519],\n",
              "       [0.53391162],\n",
              "       [0.2191162 ],\n",
              "       [0.22815057],\n",
              "       [0.20163666],\n",
              "       [0.23469722],\n",
              "       [0.43273322],\n",
              "       [0.33001637],\n",
              "       [0.47364975],\n",
              "       [0.4507365 ],\n",
              "       [0.55548282],\n",
              "       [0.35908347],\n",
              "       [0.3114239 ],\n",
              "       [0.0801964 ],\n",
              "       [0.58166939],\n",
              "       [0.32772504],\n",
              "       [0.61708674],\n",
              "       [0.83679214],\n",
              "       [0.34553191],\n",
              "       [0.32229133],\n",
              "       [0.60779051],\n",
              "       [0.53256956],\n",
              "       [0.34566285],\n",
              "       [0.2314239 ],\n",
              "       [0.43436989],\n",
              "       [0.43436989],\n",
              "       [0.20523732],\n",
              "       [0.06775777],\n",
              "       [0.19541735],\n",
              "       [0.14599018],\n",
              "       [0.45564648],\n",
              "       [0.44091653],\n",
              "       [0.39869067],\n",
              "       [0.21145663],\n",
              "       [0.35581015],\n",
              "       [0.18232406],\n",
              "       [0.21263502],\n",
              "       [0.32130933],\n",
              "       [0.77873273],\n",
              "       [0.77873273],\n",
              "       [0.3610748 ],\n",
              "       [0.3319653 ],\n",
              "       [0.33617021],\n",
              "       [0.60458265],\n",
              "       [0.44334075],\n",
              "       [0.35574468],\n",
              "       [0.12373159],\n",
              "       [0.30998363],\n",
              "       [0.30671031],\n",
              "       [0.42454992],\n",
              "       [0.58134206],\n",
              "       [0.26651391],\n",
              "       [0.60407578],\n",
              "       [0.30791833],\n",
              "       [0.13705827],\n",
              "       [0.56230998],\n",
              "       [0.16996465],\n",
              "       [0.50282537],\n",
              "       [0.4370126 ],\n",
              "       [0.57876318],\n",
              "       [0.59774763],\n",
              "       [0.43109656],\n",
              "       [0.22815057],\n",
              "       [0.3114239 ],\n",
              "       [0.41047463],\n",
              "       [0.47332242],\n",
              "       [0.11685761],\n",
              "       [0.36740295],\n",
              "       [0.62432586],\n",
              "       [0.28260573],\n",
              "       [0.40818331],\n",
              "       [0.26743044],\n",
              "       [0.22815057],\n",
              "       [0.32437152],\n",
              "       [0.21679296],\n",
              "       [0.4496689 ],\n",
              "       [0.37119984],\n",
              "       [0.5192144 ],\n",
              "       [0.29031097],\n",
              "       [0.58693944],\n",
              "       [0.55440262],\n",
              "       [0.60448445],\n",
              "       [0.35253682],\n",
              "       [0.25963993],\n",
              "       [0.58572831],\n",
              "       [0.4407856 ],\n",
              "       [0.5307365 ],\n",
              "       [0.51306056],\n",
              "       [0.39181669],\n",
              "       [0.41296236],\n",
              "       [0.14631751],\n",
              "       [0.41472995],\n",
              "       [0.32248773],\n",
              "       [0.32248773],\n",
              "       [0.44013093],\n",
              "       [0.48618658],\n",
              "       [0.42454992],\n",
              "       [0.38199673],\n",
              "       [0.39463175],\n",
              "       [0.43613748],\n",
              "       [0.19489362],\n",
              "       [0.32314239],\n",
              "       [0.49001637],\n",
              "       [0.6501473 ],\n",
              "       [0.33348609],\n",
              "       [0.3691653 ],\n",
              "       [0.28533552],\n",
              "       [0.35727791],\n",
              "       [0.77482815],\n",
              "       [0.34432079],\n",
              "       [0.64435352],\n",
              "       [0.39207856],\n",
              "       [0.43659574],\n",
              "       [0.32271686],\n",
              "       [0.17250409],\n",
              "       [0.45220949],\n",
              "       [0.44746318],\n",
              "       [0.2811784 ],\n",
              "       [0.24582651],\n",
              "       [0.40425532],\n",
              "       [0.27584288],\n",
              "       [0.34271686],\n",
              "       [0.42373159],\n",
              "       [1.        ],\n",
              "       [0.63731588],\n",
              "       [0.76824877],\n",
              "       [0.69338789],\n",
              "       [0.42700491],\n",
              "       [0.36563011],\n",
              "       [0.36114566],\n",
              "       [0.38527005],\n",
              "       [0.63227496],\n",
              "       [0.40615385],\n",
              "       [0.3787234 ],\n",
              "       [0.69393552],\n",
              "       [0.33250409],\n",
              "       [0.28707038],\n",
              "       [0.37937807],\n",
              "       [0.26664484],\n",
              "       [0.40484452],\n",
              "       [0.09001637],\n",
              "       [0.22225859],\n",
              "       [0.43404255],\n",
              "       [0.56530278],\n",
              "       [0.46055646],\n",
              "       [0.34599018],\n",
              "       [0.16268412],\n",
              "       [0.41472995],\n",
              "       [0.75711948],\n",
              "       [0.23469722],\n",
              "       [0.3787234 ],\n",
              "       [0.41963993],\n",
              "       [0.23037643],\n",
              "       [0.03698854],\n",
              "       [0.33829345],\n",
              "       [0.32373159],\n",
              "       [0.24590245],\n",
              "       [0.5294036 ],\n",
              "       [0.52560671],\n",
              "       [0.17774141],\n",
              "       [0.31325696],\n",
              "       [0.26415712],\n",
              "       [0.43306056],\n",
              "       [0.36487169],\n",
              "       [0.18788871],\n",
              "       [0.39489362],\n",
              "       [0.26219313],\n",
              "       [0.53878887],\n",
              "       [0.2608838 ],\n",
              "       [0.17086743],\n",
              "       [0.10540098],\n",
              "       [0.3319653 ],\n",
              "       [0.2686838 ],\n",
              "       [0.45564648],\n",
              "       [0.35842881],\n",
              "       [0.58494272],\n",
              "       [0.12340426],\n",
              "       [0.15646481],\n",
              "       [0.22193126],\n",
              "       [0.3797054 ],\n",
              "       [0.66710311],\n",
              "       [0.29427169],\n",
              "       [0.40294599],\n",
              "       [0.43068445],\n",
              "       [0.22978723],\n",
              "       [0.46612209],\n",
              "       [0.45346579],\n",
              "       [0.42688756],\n",
              "       [0.25237316],\n",
              "       [0.4496689 ],\n",
              "       [0.33486088],\n",
              "       [0.93649755],\n",
              "       [0.2796072 ],\n",
              "       [0.35581015],\n",
              "       [0.33617021],\n",
              "       [0.57119476],\n",
              "       [0.17577741],\n",
              "       [0.36202946],\n",
              "       [0.53826301],\n",
              "       [0.36792144],\n",
              "       [0.50535663],\n",
              "       [0.38132488],\n",
              "       [0.298527  ],\n",
              "       [0.51090016],\n",
              "       [0.14042553],\n",
              "       [0.17021277],\n",
              "       [0.48805237],\n",
              "       [0.42651391],\n",
              "       [0.05400982],\n",
              "       [0.27248069],\n",
              "       [0.50535663],\n",
              "       [0.30736498],\n",
              "       [0.52180982],\n",
              "       [0.30667758]])"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(config={\"hyper\": \"parameter\"})\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "39NoNiEM-dHG",
        "outputId": "7b976219-13a6-4275-8dd6-4093f0ece7cf"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport wandb\\nfrom wandb.keras import WandbCallback\\n\\nwandb.init(config={\"hyper\": \"parameter\"})\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blood_model_2():\n",
        "    #clear_session()\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    input_len=170\n",
        "\n",
        "    print(input_len)\n",
        "    output_size=24\n",
        "    drop_frac0=0.0 \n",
        "    drop_frac1=0.0\n",
        "\n",
        "\n",
        "\n",
        "    input1=Input(shape=(input_len,))\n",
        "\n",
        "    #flatt=Flatten()(lstm1)\n",
        "\n",
        "    non=42\n",
        "    #initializer = tf.keras.initializers.LecunNormal()\n",
        "    #initializer=tf.keras.initializers.LecunUniform()\n",
        "    #initializer=tf.keras.initializers.HeUniform(    seed=None)\n",
        "    #initializer= tf.keras.initializers.RandomNormal(    mean=3.0, stddev=0.05, seed=None)\n",
        "\n",
        "    initializer=\"HeNormal\"\n",
        "    d1=Dense(1000,activation=\"sigmoid\",kernel_initializer=initializer)(input1)\n",
        "    d1=Dense(100,activation=\"sigmoid\",kernel_initializer=initializer)(d1)\n",
        "    #d1=Dense(100,activation=\"selu\",kernel_initializer=initializer)(d1)\n",
        "\n",
        "\n",
        "    #softmax\n",
        "    pred=Dense(1,activation=\"sigmoid\",)(d1)\n",
        "\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=input1, outputs=pred)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
        "\n",
        "\n",
        "    lossfn = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "\n",
        "    model.compile(loss=\"CategoricalCrossentropy\",\n",
        "        optimizer=opt,\n",
        "        metrics=[\"Accuracy\"])\n",
        "    return(model)"
      ],
      "metadata": {
        "id": "2f2U85aQmiaZ"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LtOGK6OIZ9Pu"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndb5XDN9nwsC",
        "outputId": "c586cbce-af74-417e-cff9-3f73f4e2f1f2"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.7)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.1)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msipoczlaszlo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(project=\"BM_Norm_3\", entity=\"sipoczlaszlo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421,
          "referenced_widgets": [
            "5008f884d889486096e5bf4d56d9c237",
            "06c3c56069b64754811c97cbb92584eb",
            "6990d76a960147ddb4af0afe7e200913",
            "00c81acd50e74486a0baa78ef4d57e22",
            "abedb1c0c2664ebc9513de5f50d57536",
            "a857d064a5bf44f7b8cc13dc43d1c3df",
            "8dc1b96c51d1485494e4c8745c92211a",
            "8f454602a294478686705b6a73b0620e"
          ]
        },
        "id": "3Rn8vSnroH4R",
        "outputId": "3b5a41ab-a03a-4fa1-b26a-e578806322bd"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:2gdvv5sa) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 11795... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "5008f884d889486096e5bf4d56d9c237",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 9.00MB of 9.00MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MSE</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_MSE</td><td>▃▁▂▁▂▂▁▁▂▁▁▃▁█▁▂▃▁▂▃▂▁▂▃▂▂▂▄▃▃▃▄▃▃▂▃▃▂▃▃</td></tr><tr><td>val_loss</td><td>▃▁▂▁▂▂▁▁▂▁▁▃▁█▁▂▃▁▂▃▂▁▂▃▂▂▂▄▃▃▃▄▃▃▂▃▃▂▃▃</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MSE</td><td>0.87777</td></tr><tr><td>best_epoch</td><td>20</td></tr><tr><td>best_val_loss</td><td>1.13807</td></tr><tr><td>epoch</td><td>48</td></tr><tr><td>loss</td><td>0.87777</td></tr><tr><td>lr</td><td>0.001</td></tr><tr><td>val_MSE</td><td>1.24743</td></tr><tr><td>val_loss</td><td>1.24743</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">ethereal-star-3</strong>: <a href=\"https://wandb.ai/sipoczlaszlo/BM_Norm_3/runs/2gdvv5sa\" target=\"_blank\">https://wandb.ai/sipoczlaszlo/BM_Norm_3/runs/2gdvv5sa</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211215_170232-2gdvv5sa/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:2gdvv5sa). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/sipoczlaszlo/BM_Norm_3/runs/114pmb53\" target=\"_blank\">cosmic-paper-4</a></strong> to <a href=\"https://wandb.ai/sipoczlaszlo/BM_Norm_3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7fdb52a64710>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sipoczlaszlo/BM_Norm_3/runs/114pmb53?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blood_model_regression():\n",
        "    #clear_session()\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    input_len=170\n",
        "\n",
        "    print(input_len)\n",
        "    output_size=24\n",
        "    drop_frac0=0.16 \n",
        "    drop_frac1=0.0\n",
        "\n",
        "\n",
        "\n",
        "    input1=Input(shape=(input_len,))\n",
        "\n",
        "    #flatt=Flatten()(lstm1)\n",
        "\n",
        "    non=42\n",
        "    #initializer = tf.keras.initializers.LecunNormal()\n",
        "    #initializer=tf.keras.initializers.LecunUniform()\n",
        "    #initializer=tf.keras.initializers.HeUniform(    seed=None)\n",
        "    #initializer= tf.keras.initializers.RandomNormal(    mean=3.0, stddev=0.05, seed=None)\n",
        "\n",
        "    initializer=\"HeNormal\"\n",
        "    initializer2=\"GlorotNormal\"\n",
        "    \n",
        "    d1=Dense(5512,activation=\"relu\",kernel_initializer=initializer)(input1)\n",
        "    d1=Dropout(drop_frac0)(d1)\n",
        "    \n",
        "    d1=Dense(256,activation=\"relu\",kernel_initializer=initializer)(d1)\n",
        "    #d1=Dropout(drop_frac1)(d1)\n",
        "    #d1=Dense(128,activation=\"tanh\",kernel_initializer=initializer2)(d1)\n",
        "    # d1=Dropout(drop_frac1)(d1) \n",
        "\n",
        "    #softmax\n",
        "    pred=Dense(1, kernel_initializer=initializer)(d1)\n",
        "\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=input1, outputs=pred)\n",
        "\n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=0.0000001)\n",
        "\n",
        "\n",
        "    lossfn = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "\n",
        "    model.compile(loss=\"MSE\",\n",
        "        optimizer=opt,\n",
        "        metrics=[\"MSE\"])\n",
        "    return(model)"
      ],
      "metadata": {
        "id": "f3cQpWHeJ4IJ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__DNN_MODE__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "rsAoivPNaBOU",
        "outputId": "4072dacd-d756-4540-b0a0-70ae7e8c3e06"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'regression'"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"classification\":\n",
        "    model_name=\"classification_\"\n",
        "    def scheduler(epoch, lr):\n",
        "        return 0.001\n",
        "\n",
        "    callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "    callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                            verbose=1, save_best_only=True, mode='min')]\n",
        "\n"
      ],
      "metadata": {
        "id": "DEVcX9Z5mwnz"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"regression\":\n",
        "    model_name=\"regression_\"\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch<50:\n",
        "            return 0.001\n",
        "        elif epoch <100:\n",
        "            return 0.0005\n",
        "        elif epoch <150:\n",
        "            return 0.0001\n",
        "        return 0.00005    \n",
        "\n",
        "\n",
        "    callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "    callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                            verbose=1, save_best_only=True, mode='min')]\n",
        "\n"
      ],
      "metadata": {
        "id": "zSCDNBu4aH9P"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"classification\":\n",
        "    new_model_2=blood_model_2()"
      ],
      "metadata": {
        "id": "yakOYRb_m07P"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"regression\":\n",
        "    new_model_2=blood_model_regression()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IM-jEkwbG2A",
        "outputId": "44877d39-c715-4a1f-c5fe-ce99ee275cac"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5"
      ],
      "metadata": {
        "id": "9-rSra7K3wVB"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Us0FJewhDKc",
        "outputId": "a97dcbbc-da59-4f77-ba78-aca0c7ae475d"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.36453647,  0.38885105,  0.39872672, ...,  1.82445842,\n",
              "         1.673298  ,  1.92809129],\n",
              "       [ 0.43579244,  0.39694511,  0.40961367, ...,  0.57719086,\n",
              "         0.45474503,  0.32195174],\n",
              "       [-0.9863988 , -0.96168426, -0.90536406, ...,  0.21397963,\n",
              "         0.08306335,  0.19148561],\n",
              "       ...,\n",
              "       [-1.07183495, -1.01721066, -1.05299893, ..., -1.18797608,\n",
              "        -1.15115283, -1.14202762],\n",
              "       [ 1.12166936,  1.0612296 ,  1.09288581, ..., -0.09962229,\n",
              "        -0.02788948, -0.05773341],\n",
              "       [-1.44973193, -1.46461933, -1.4970304 , ..., -1.13059767,\n",
              "        -1.05839207, -1.05836683]])"
            ]
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "    if __DNN_MODE__==\"regression\":\n",
        "        new_model_2.summary()\n",
        "        history=new_model_2.fit(X,\n",
        "          y,\n",
        "          epochs=200, \n",
        "          batch_size=3,\n",
        "          validation_split=0.1,\n",
        "          verbose=1,\n",
        "          callbacks=[callbacks,WandbCallback()],\n",
        "          shuffle=True\n",
        "          \n",
        "          )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHWVim-0vPxT",
        "outputId": "2f273794-d305-4286-988d-36c938fbdcf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 170)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 5512)              942552    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 5512)              0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 256)               1411328   \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 257       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,354,137\n",
            "Trainable params: 2,354,137\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/200\n",
            "  2/163 [..............................] - ETA: 22:05 - loss: 2.9213 - MSE: 2.9213WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0052s vs `on_train_batch_end` time: 1.3744s). Check your callbacks.\n",
            "156/163 [===========================>..] - ETA: 0s - loss: 0.9620 - MSE: 0.9620\n",
            "Epoch 00001: val_loss improved from inf to 0.15636, saving model to regression__0.92754_0.15636_.hdf5\n",
            "163/163 [==============================] - 16s 90ms/step - loss: 0.9275 - MSE: 0.9275 - val_loss: 0.1564 - val_MSE: 0.1564 - lr: 0.0010\n",
            "Epoch 2/200\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0900 - MSE: 0.0900\n",
            "Epoch 00002: val_loss improved from 0.15636 to 0.05488, saving model to regression__0.09003_0.05488_.hdf5\n",
            "163/163 [==============================] - 5s 33ms/step - loss: 0.0900 - MSE: 0.0900 - val_loss: 0.0549 - val_MSE: 0.0549 - lr: 0.0010\n",
            "Epoch 3/200\n",
            "159/163 [============================>.] - ETA: 0s - loss: 0.0752 - MSE: 0.0752\n",
            "Epoch 00003: val_loss improved from 0.05488 to 0.05353, saving model to regression__0.07646_0.05353_.hdf5\n",
            "163/163 [==============================] - 7s 43ms/step - loss: 0.0765 - MSE: 0.0765 - val_loss: 0.0535 - val_MSE: 0.0535 - lr: 0.0010\n",
            "Epoch 4/200\n",
            "156/163 [===========================>..] - ETA: 0s - loss: 0.0573 - MSE: 0.0573\n",
            "Epoch 00004: val_loss improved from 0.05353 to 0.03694, saving model to regression__0.05721_0.03694_.hdf5\n",
            "163/163 [==============================] - 10s 65ms/step - loss: 0.0572 - MSE: 0.0572 - val_loss: 0.0369 - val_MSE: 0.0369 - lr: 0.0010\n",
            "Epoch 5/200\n",
            "161/163 [============================>.] - ETA: 0s - loss: 0.0462 - MSE: 0.0462\n",
            "Epoch 00005: val_loss improved from 0.03694 to 0.03632, saving model to regression__0.04622_0.03632_.hdf5\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0462 - MSE: 0.0462 - val_loss: 0.0363 - val_MSE: 0.0363 - lr: 0.0010\n",
            "Epoch 6/200\n",
            "162/163 [============================>.] - ETA: 0s - loss: 0.0392 - MSE: 0.0392\n",
            "Epoch 00006: val_loss did not improve from 0.03632\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.0393 - MSE: 0.0393 - val_loss: 0.0396 - val_MSE: 0.0396 - lr: 0.0010\n",
            "Epoch 7/200\n",
            "162/163 [============================>.] - ETA: 0s - loss: 0.0400 - MSE: 0.0400\n",
            "Epoch 00007: val_loss did not improve from 0.03632\n",
            "163/163 [==============================] - 4s 25ms/step - loss: 0.0400 - MSE: 0.0400 - val_loss: 0.0451 - val_MSE: 0.0451 - lr: 0.0010\n",
            "Epoch 8/200\n",
            "159/163 [============================>.] - ETA: 0s - loss: 0.0379 - MSE: 0.0379\n",
            "Epoch 00008: val_loss improved from 0.03632 to 0.03577, saving model to regression__0.03775_0.03577_.hdf5\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0377 - MSE: 0.0377 - val_loss: 0.0358 - val_MSE: 0.0358 - lr: 0.0010\n",
            "Epoch 9/200\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0365 - MSE: 0.0365\n",
            "Epoch 00009: val_loss did not improve from 0.03577\n",
            "163/163 [==============================] - 4s 22ms/step - loss: 0.0365 - MSE: 0.0365 - val_loss: 0.0403 - val_MSE: 0.0403 - lr: 0.0010\n",
            "Epoch 10/200\n",
            "153/163 [===========================>..] - ETA: 0s - loss: 0.0349 - MSE: 0.0349\n",
            "Epoch 00010: val_loss did not improve from 0.03577\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0341 - MSE: 0.0341 - val_loss: 0.0439 - val_MSE: 0.0439 - lr: 0.0010\n",
            "Epoch 11/200\n",
            "157/163 [===========================>..] - ETA: 0s - loss: 0.0320 - MSE: 0.0320\n",
            "Epoch 00011: val_loss did not improve from 0.03577\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0332 - MSE: 0.0332 - val_loss: 0.0390 - val_MSE: 0.0390 - lr: 0.0010\n",
            "Epoch 12/200\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0322 - MSE: 0.0322\n",
            "Epoch 00012: val_loss did not improve from 0.03577\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0322 - MSE: 0.0322 - val_loss: 0.0419 - val_MSE: 0.0419 - lr: 0.0010\n",
            "Epoch 13/200\n",
            "156/163 [===========================>..] - ETA: 0s - loss: 0.0287 - MSE: 0.0287\n",
            "Epoch 00013: val_loss improved from 0.03577 to 0.03429, saving model to regression__0.02901_0.03429_.hdf5\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0290 - MSE: 0.0290 - val_loss: 0.0343 - val_MSE: 0.0343 - lr: 0.0010\n",
            "Epoch 14/200\n",
            "162/163 [============================>.] - ETA: 0s - loss: 0.0300 - MSE: 0.0300\n",
            "Epoch 00014: val_loss improved from 0.03429 to 0.02928, saving model to regression__0.02995_0.02928_.hdf5\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0299 - MSE: 0.0299 - val_loss: 0.0293 - val_MSE: 0.0293 - lr: 0.0010\n",
            "Epoch 15/200\n",
            "155/163 [===========================>..] - ETA: 0s - loss: 0.0278 - MSE: 0.0278\n",
            "Epoch 00015: val_loss did not improve from 0.02928\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0272 - MSE: 0.0272 - val_loss: 0.0356 - val_MSE: 0.0356 - lr: 0.0010\n",
            "Epoch 16/200\n",
            "158/163 [============================>.] - ETA: 0s - loss: 0.0265 - MSE: 0.0265\n",
            "Epoch 00016: val_loss did not improve from 0.02928\n",
            "163/163 [==============================] - 10s 61ms/step - loss: 0.0264 - MSE: 0.0264 - val_loss: 0.0316 - val_MSE: 0.0316 - lr: 0.0010\n",
            "Epoch 17/200\n",
            "152/163 [==========================>...] - ETA: 0s - loss: 0.0253 - MSE: 0.0253\n",
            "Epoch 00017: val_loss did not improve from 0.02928\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 0.0252 - MSE: 0.0252 - val_loss: 0.0313 - val_MSE: 0.0313 - lr: 0.0010\n",
            "Epoch 18/200\n",
            "157/163 [===========================>..] - ETA: 0s - loss: 0.0246 - MSE: 0.0246\n",
            "Epoch 00018: val_loss did not improve from 0.02928\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0248 - MSE: 0.0248 - val_loss: 0.0314 - val_MSE: 0.0314 - lr: 0.0010\n",
            "Epoch 19/200\n",
            "153/163 [===========================>..] - ETA: 0s - loss: 0.0239 - MSE: 0.0239\n",
            "Epoch 00019: val_loss improved from 0.02928 to 0.02746, saving model to regression__0.02474_0.02746_.hdf5\n",
            "163/163 [==============================] - 4s 22ms/step - loss: 0.0247 - MSE: 0.0247 - val_loss: 0.0275 - val_MSE: 0.0275 - lr: 0.0010\n",
            "Epoch 20/200\n",
            "154/163 [===========================>..] - ETA: 0s - loss: 0.0234 - MSE: 0.0234\n",
            "Epoch 00020: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0231 - MSE: 0.0231 - val_loss: 0.0379 - val_MSE: 0.0379 - lr: 0.0010\n",
            "Epoch 21/200\n",
            "152/163 [==========================>...] - ETA: 0s - loss: 0.0233 - MSE: 0.0233\n",
            "Epoch 00021: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 6s 37ms/step - loss: 0.0230 - MSE: 0.0230 - val_loss: 0.0306 - val_MSE: 0.0306 - lr: 0.0010\n",
            "Epoch 22/200\n",
            "160/163 [============================>.] - ETA: 0s - loss: 0.0240 - MSE: 0.0240\n",
            "Epoch 00022: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 25ms/step - loss: 0.0241 - MSE: 0.0241 - val_loss: 0.0332 - val_MSE: 0.0332 - lr: 0.0010\n",
            "Epoch 23/200\n",
            "155/163 [===========================>..] - ETA: 0s - loss: 0.0247 - MSE: 0.0247\n",
            "Epoch 00023: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 23ms/step - loss: 0.0242 - MSE: 0.0242 - val_loss: 0.0305 - val_MSE: 0.0305 - lr: 0.0010\n",
            "Epoch 24/200\n",
            "152/163 [==========================>...] - ETA: 0s - loss: 0.0225 - MSE: 0.0225\n",
            "Epoch 00024: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0231 - MSE: 0.0231 - val_loss: 0.0304 - val_MSE: 0.0304 - lr: 0.0010\n",
            "Epoch 25/200\n",
            "160/163 [============================>.] - ETA: 0s - loss: 0.0233 - MSE: 0.0233\n",
            "Epoch 00025: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0234 - MSE: 0.0234 - val_loss: 0.0336 - val_MSE: 0.0336 - lr: 0.0010\n",
            "Epoch 26/200\n",
            "161/163 [============================>.] - ETA: 0s - loss: 0.0232 - MSE: 0.0232\n",
            "Epoch 00026: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0232 - MSE: 0.0232 - val_loss: 0.0279 - val_MSE: 0.0279 - lr: 0.0010\n",
            "Epoch 27/200\n",
            "155/163 [===========================>..] - ETA: 0s - loss: 0.0215 - MSE: 0.0215\n",
            "Epoch 00027: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 0.0212 - MSE: 0.0212 - val_loss: 0.0290 - val_MSE: 0.0290 - lr: 0.0010\n",
            "Epoch 28/200\n",
            "157/163 [===========================>..] - ETA: 0s - loss: 0.0228 - MSE: 0.0228\n",
            "Epoch 00028: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 17ms/step - loss: 0.0223 - MSE: 0.0223 - val_loss: 0.0283 - val_MSE: 0.0283 - lr: 0.0010\n",
            "Epoch 29/200\n",
            "160/163 [============================>.] - ETA: 0s - loss: 0.0229 - MSE: 0.0229\n",
            "Epoch 00029: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 23ms/step - loss: 0.0231 - MSE: 0.0231 - val_loss: 0.0278 - val_MSE: 0.0278 - lr: 0.0010\n",
            "Epoch 30/200\n",
            "157/163 [===========================>..] - ETA: 0s - loss: 0.0220 - MSE: 0.0220\n",
            "Epoch 00030: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 23ms/step - loss: 0.0221 - MSE: 0.0221 - val_loss: 0.0305 - val_MSE: 0.0305 - lr: 0.0010\n",
            "Epoch 31/200\n",
            "160/163 [============================>.] - ETA: 0s - loss: 0.0225 - MSE: 0.0225\n",
            "Epoch 00031: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0224 - MSE: 0.0224 - val_loss: 0.0329 - val_MSE: 0.0329 - lr: 0.0010\n",
            "Epoch 32/200\n",
            "163/163 [==============================] - ETA: 0s - loss: 0.0221 - MSE: 0.0221\n",
            "Epoch 00032: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0221 - MSE: 0.0221 - val_loss: 0.0295 - val_MSE: 0.0295 - lr: 0.0010\n",
            "Epoch 33/200\n",
            "162/163 [============================>.] - ETA: 0s - loss: 0.0218 - MSE: 0.0218\n",
            "Epoch 00033: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 23ms/step - loss: 0.0218 - MSE: 0.0218 - val_loss: 0.0275 - val_MSE: 0.0275 - lr: 0.0010\n",
            "Epoch 34/200\n",
            "158/163 [============================>.] - ETA: 0s - loss: 0.0216 - MSE: 0.0216\n",
            "Epoch 00034: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 23ms/step - loss: 0.0220 - MSE: 0.0220 - val_loss: 0.0276 - val_MSE: 0.0276 - lr: 0.0010\n",
            "Epoch 35/200\n",
            "159/163 [============================>.] - ETA: 0s - loss: 0.0228 - MSE: 0.0228\n",
            "Epoch 00035: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 5s 29ms/step - loss: 0.0228 - MSE: 0.0228 - val_loss: 0.0288 - val_MSE: 0.0288 - lr: 0.0010\n",
            "Epoch 36/200\n",
            "154/163 [===========================>..] - ETA: 0s - loss: 0.0210 - MSE: 0.0210\n",
            "Epoch 00036: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 0.0210 - MSE: 0.0210 - val_loss: 0.0295 - val_MSE: 0.0295 - lr: 0.0010\n",
            "Epoch 37/200\n",
            "158/163 [============================>.] - ETA: 0s - loss: 0.0217 - MSE: 0.0217\n",
            "Epoch 00037: val_loss did not improve from 0.02746\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0214 - MSE: 0.0214 - val_loss: 0.0302 - val_MSE: 0.0302 - lr: 0.0010\n",
            "Epoch 38/200\n",
            "152/163 [==========================>...] - ETA: 0s - loss: 0.0220 - MSE: 0.0220\n",
            "Epoch 00038: val_loss improved from 0.02746 to 0.02721, saving model to regression__0.02240_0.02721_.hdf5\n",
            "163/163 [==============================] - 6s 40ms/step - loss: 0.0224 - MSE: 0.0224 - val_loss: 0.0272 - val_MSE: 0.0272 - lr: 0.0010\n",
            "Epoch 39/200\n",
            "154/163 [===========================>..] - ETA: 0s - loss: 0.0209 - MSE: 0.0209\n",
            "Epoch 00039: val_loss did not improve from 0.02721\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0209 - MSE: 0.0209 - val_loss: 0.0296 - val_MSE: 0.0296 - lr: 0.0010\n",
            "Epoch 40/200\n",
            "158/163 [============================>.] - ETA: 0s - loss: 0.0203 - MSE: 0.0203\n",
            "Epoch 00040: val_loss did not improve from 0.02721\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 0.0203 - MSE: 0.0203 - val_loss: 0.0302 - val_MSE: 0.0302 - lr: 0.0010\n",
            "Epoch 41/200\n",
            "156/163 [===========================>..] - ETA: 0s - loss: 0.0203 - MSE: 0.0203\n",
            "Epoch 00041: val_loss did not improve from 0.02721\n",
            "163/163 [==============================] - 7s 43ms/step - loss: 0.0206 - MSE: 0.0206 - val_loss: 0.0309 - val_MSE: 0.0309 - lr: 0.0010\n",
            "Epoch 42/200\n",
            "160/163 [============================>.] - ETA: 0s - loss: 0.0192 - MSE: 0.0192\n",
            "Epoch 00042: val_loss did not improve from 0.02721\n",
            "163/163 [==============================] - 4s 27ms/step - loss: 0.0190 - MSE: 0.0190 - val_loss: 0.0285 - val_MSE: 0.0285 - lr: 0.0010\n",
            "Epoch 43/200\n",
            "152/163 [==========================>...] - ETA: 0s - loss: 0.0209 - MSE: 0.0209\n",
            "Epoch 00043: val_loss did not improve from 0.02721\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0210 - MSE: 0.0210 - val_loss: 0.0298 - val_MSE: 0.0298 - lr: 0.0010\n",
            "Epoch 44/200\n",
            "153/163 [===========================>..] - ETA: 0s - loss: 0.0198 - MSE: 0.0198\n",
            "Epoch 00044: val_loss improved from 0.02721 to 0.02588, saving model to regression__0.02030_0.02588_.hdf5\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0203 - MSE: 0.0203 - val_loss: 0.0259 - val_MSE: 0.0259 - lr: 0.0010\n",
            "Epoch 45/200\n",
            "154/163 [===========================>..] - ETA: 0s - loss: 0.0198 - MSE: 0.0198\n",
            "Epoch 00045: val_loss did not improve from 0.02588\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0201 - MSE: 0.0201 - val_loss: 0.0270 - val_MSE: 0.0270 - lr: 0.0010\n",
            "Epoch 46/200\n",
            "160/163 [============================>.] - ETA: 0s - loss: 0.0201 - MSE: 0.0201\n",
            "Epoch 00046: val_loss did not improve from 0.02588\n",
            "163/163 [==============================] - 1s 5ms/step - loss: 0.0199 - MSE: 0.0199 - val_loss: 0.0260 - val_MSE: 0.0260 - lr: 0.0010\n",
            "Epoch 47/200\n",
            "159/163 [============================>.] - ETA: 0s - loss: 0.0207 - MSE: 0.0207\n",
            "Epoch 00047: val_loss did not improve from 0.02588\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0204 - MSE: 0.0204 - val_loss: 0.0275 - val_MSE: 0.0275 - lr: 0.0010\n",
            "Epoch 48/200\n",
            "162/163 [============================>.] - ETA: 0s - loss: 0.0202 - MSE: 0.0202\n",
            "Epoch 00048: val_loss improved from 0.02588 to 0.02537, saving model to regression__0.02013_0.02537_.hdf5\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0201 - MSE: 0.0201 - val_loss: 0.0254 - val_MSE: 0.0254 - lr: 0.0010\n",
            "Epoch 49/200\n",
            "155/163 [===========================>..] - ETA: 0s - loss: 0.0199 - MSE: 0.0199\n",
            "Epoch 00049: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0199 - MSE: 0.0199 - val_loss: 0.0260 - val_MSE: 0.0260 - lr: 0.0010\n",
            "Epoch 50/200\n",
            "161/163 [============================>.] - ETA: 0s - loss: 0.0197 - MSE: 0.0197\n",
            "Epoch 00050: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0197 - MSE: 0.0197 - val_loss: 0.0263 - val_MSE: 0.0263 - lr: 0.0010\n",
            "Epoch 51/200\n",
            "158/163 [============================>.] - ETA: 0s - loss: 0.0200 - MSE: 0.0200\n",
            "Epoch 00051: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0202 - MSE: 0.0202 - val_loss: 0.0268 - val_MSE: 0.0268 - lr: 5.0000e-04\n",
            "Epoch 52/200\n",
            "159/163 [============================>.] - ETA: 0s - loss: 0.0202 - MSE: 0.0202\n",
            "Epoch 00052: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0204 - MSE: 0.0204 - val_loss: 0.0262 - val_MSE: 0.0262 - lr: 5.0000e-04\n",
            "Epoch 53/200\n",
            "158/163 [============================>.] - ETA: 0s - loss: 0.0204 - MSE: 0.0204\n",
            "Epoch 00053: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 3s 21ms/step - loss: 0.0203 - MSE: 0.0203 - val_loss: 0.0258 - val_MSE: 0.0258 - lr: 5.0000e-04\n",
            "Epoch 54/200\n",
            "162/163 [============================>.] - ETA: 0s - loss: 0.0201 - MSE: 0.0201\n",
            "Epoch 00054: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 4s 24ms/step - loss: 0.0201 - MSE: 0.0201 - val_loss: 0.0265 - val_MSE: 0.0265 - lr: 5.0000e-04\n",
            "Epoch 55/200\n",
            "155/163 [===========================>..] - ETA: 0s - loss: 0.0203 - MSE: 0.0203\n",
            "Epoch 00055: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0199 - MSE: 0.0199 - val_loss: 0.0267 - val_MSE: 0.0267 - lr: 5.0000e-04\n",
            "Epoch 56/200\n",
            "157/163 [===========================>..] - ETA: 0s - loss: 0.0199 - MSE: 0.0199\n",
            "Epoch 00056: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 3s 20ms/step - loss: 0.0197 - MSE: 0.0197 - val_loss: 0.0263 - val_MSE: 0.0263 - lr: 5.0000e-04\n",
            "Epoch 57/200\n",
            "157/163 [===========================>..] - ETA: 0s - loss: 0.0199 - MSE: 0.0199\n",
            "Epoch 00057: val_loss did not improve from 0.02537\n",
            "163/163 [==============================] - 4s 26ms/step - loss: 0.0198 - MSE: 0.0198 - val_loss: 0.0262 - val_MSE: 0.0262 - lr: 5.0000e-04\n",
            "Epoch 58/200\n",
            " 93/163 [================>.............] - ETA: 0s - loss: 0.0210 - MSE: 0.0210"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ihqdnY6Vm2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_model_2.load_weights(\"./_col_0.00098_0.00173_.xhdf5\")"
      ],
      "metadata": {
        "id": "6lS9W3aAu2v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eSVDad6L_Fgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_2.load_weights(\"./regression__0.04629_0.03222_.hdf5\")"
      ],
      "metadata": {
        "id": "BKodgIskMiCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=new_model_2.predict(X)\n",
        "result_df=pd.DataFrame()\n",
        "y_pred_col=[i[0] for i in y_pred]\n",
        "y_col=[i[0] for i in y]\n",
        "\n",
        "result_df[\"y\"]=y_col\n",
        "result_df[\"y_pred\"]=y_pred_col"
      ],
      "metadata": {
        "id": "4YcPfNhoDkVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(result_df, x=\"y\", y=\"y_pred\",width=600, height=400 )\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "8JvY2HWxIQQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "hGqdbRn7lYIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "\n",
        "\n",
        "clf = RandomForestRegressor(max_depth=70, random_state=0,n_estimators=300)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "randomforest_predict=clf.predict(X_train)\n"
      ],
      "metadata": {
        "id": "CBgYvOPdECYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {mean_absolute_error(y_train,randomforest_predict)}\")"
      ],
      "metadata": {
        "id": "fxJVYoR0EJ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomforest_predict=clf.predict(X_test)\n",
        "print(f\"Accuracy: {mean_absolute_error(y_test,randomforest_predict)}\")"
      ],
      "metadata": {
        "id": "ski4ZlVBExqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3cjQyvPz8Tz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "4HFebOdZ_4Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LazyClassifier(verbose=1,ignore_warnings=True, custom_metric=None)\n",
        "models,predictions = clf.fit(X[:-100], X[-100:], y[:-100], y[-100:])"
      ],
      "metadata": {
        "id": "Kls-KXivMt5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(models)"
      ],
      "metadata": {
        "id": "JDWbhHRM_r_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yo=[i[0] for i in y_]\n",
        "yo"
      ],
      "metadata": {
        "id": "QuoQjjVD_Bwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "apnbaoAkC09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "reg = LazyRegressor(verbose=2, ignore_warnings=False, custom_metric=None)\n",
        "models,predictions = clf.fit(X[:-100], X[-100:], y[:-100], y[-100:])"
      ],
      "metadata": {
        "id": "6KPqXIiH-Ycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v4OdEx1rEAqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(models)"
      ],
      "metadata": {
        "id": "9Wn6o2lY80q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_counter=1"
      ],
      "metadata": {
        "id": "sHoy7pnWM84V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X[:-1*x_counter]\n",
        "X_test=X[-1*x_counter:]\n",
        "y_train=yo[:-1*x_counter]\n",
        "y_test =yo[-1*x_counter:]\n"
      ],
      "metadata": {
        "id": "3tl2wZDeEWsD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}