{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Blood_ldl_model.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8405c12ce35441d9a090bd729fad7b72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "VBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "VBoxView",
            "_dom_classes": [],
            "_model_name": "VBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_656d1a42a3ae46c39884c60d5950bb73",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_78ae1bcb951e4fb18f57ccb5317763d4",
              "IPY_MODEL_382f3ad2d7554e399dce3f9a13787f94"
            ]
          }
        },
        "656d1a42a3ae46c39884c60d5950bb73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "78ae1bcb951e4fb18f57ccb5317763d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "LabelModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "LabelView",
            "style": "IPY_MODEL_b7b5b493f043401e970fecf1a6e91b99",
            "_dom_classes": [],
            "description": "",
            "_model_name": "LabelModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 0.86MB of 0.86MB uploaded (0.00MB deduped)\r",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_77abdfb340ae49c8bca47fa5874929bc"
          }
        },
        "382f3ad2d7554e399dce3f9a13787f94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_f94bb599dc1a44f480cb59eea492b34c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0dd560ef7bf4e5e9e2b52c621ab0541"
          }
        },
        "b7b5b493f043401e970fecf1a6e91b99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "77abdfb340ae49c8bca47fa5874929bc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f94bb599dc1a44f480cb59eea492b34c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0dd560ef7bf4e5e9e2b52c621ab0541": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install lazypredict"
      ],
      "metadata": {
        "id": "lkC-pw0r-iXW"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__PROJECT_SOURCE__=\"COLAB\""
      ],
      "metadata": {
        "id": "k_VTudIT_T2U"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNkYLOFrij5B",
        "outputId": "88dcd452-c720-4fa0-9637-19b1462a2638"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "if __PROJECT_SOURCE__==\"COLAB\":\n",
        "    # Import PyDrive and associated libraries.\n",
        "    # This only needs to be done once per notebook.\n",
        "    from pydrive.auth import GoogleAuth\n",
        "    from pydrive.drive import GoogleDrive\n",
        "    from google.colab import auth\n",
        "    from oauth2client.client import GoogleCredentials\n",
        "\n",
        "    # Authenticate and create the PyDrive client.\n",
        "    # This only needs to be done once per notebook.\n",
        "    auth.authenticate_user()\n",
        "    gauth = GoogleAuth()\n",
        "    gauth.credentials = GoogleCredentials.get_application_default()\n",
        "    drive = GoogleDrive(gauth)\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "    fname_dir=\"/content/blood/\"\n",
        "    fname_url=\"/content/drive/MyDrive/blood/rework/*agg.csv\"\n",
        "    fname=fname_url.split(\"/\")[-1]\n",
        "elif __PROJECT_SOURCE__==\"LOCAL\":\n",
        "    fname_dir=\"sdfsdfsd\" #working dir\n",
        "    fname_url=\"/content/drive/MyDrive/blood/rework/*agg.csv\" #data source dir\n",
        "    \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "m7yB5l-7i7dS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "Z3v1BBbJjBqa"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir $fname_dir"
      ],
      "metadata": {
        "id": "zU1ZwzodjSD4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bc75070-27da-4ef0-8494-336791aeaee7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/blood/’: File exists\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cp $fname_url $fname_dir"
      ],
      "metadata": {
        "id": "vXyl_8ELjgrT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __PROJECT_SOURCE__==\"COLAB\":\n",
        "    drive.flush_and_unmount()\n",
        "    print('Unmount Google Drive :-(')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-Y7z49_kBNx",
        "outputId": "6f495b5e-3cf5-4b03-eea1-5c4afd806e36"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unmount Google Drive :-(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feldolgozás"
      ],
      "metadata": {
        "id": "JCOsDGPtxztL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy1mbPDRizA2",
        "outputId": "e32346c8-de3b-4f94-b098-eace8af68d21"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '*.hdf5': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "LjJtfapexzDo"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "F6TPT1jWknJD"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fnames_list=[\"ldl1_agg.csv\",\"ldl2_agg.csv\"]"
      ],
      "metadata": {
        "id": "5MmhW6aekVHb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_name1=fname_dir+fnames_list[0]\n",
        "print(file_name1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5YCcXLaMkrlf",
        "outputId": "11e43c34-8a22-4b4f-dfb8-8be58861f50a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blood/ldl1_agg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_name2=fname_dir+fnames_list[1]\n",
        "print(file_name2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fiMNf4HPBAnV",
        "outputId": "e0629897-1e45-4c2b-8ecc-e502ea690e0d"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/blood/ldl2_agg.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "TkP2-bFrx4ra"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg1= pd.read_csv(file_name1)\n",
        "df_agg1.describe()\n",
        "df_agg1.drop(df_agg1[df_agg1.absorbance0 < 0].index, inplace=True) # kill the negative elements\n"
      ],
      "metadata": {
        "id": "mPW32Ow0k9BQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg1.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "VSTvJ9RvzrI8",
        "outputId": "abb8f452-a34b-4040-c0e2-24847f003d7c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0.535147</td>\n",
              "      <td>0.537960</td>\n",
              "      <td>0.541943</td>\n",
              "      <td>0.548029</td>\n",
              "      <td>0.553548</td>\n",
              "      <td>0.560042</td>\n",
              "      <td>0.566466</td>\n",
              "      <td>0.574085</td>\n",
              "      <td>0.586266</td>\n",
              "      <td>0.599614</td>\n",
              "      <td>0.615089</td>\n",
              "      <td>0.626659</td>\n",
              "      <td>0.633565</td>\n",
              "      <td>0.637011</td>\n",
              "      <td>0.639238</td>\n",
              "      <td>0.638972</td>\n",
              "      <td>0.637331</td>\n",
              "      <td>0.635508</td>\n",
              "      <td>0.632157</td>\n",
              "      <td>0.628089</td>\n",
              "      <td>0.624585</td>\n",
              "      <td>0.621561</td>\n",
              "      <td>0.617845</td>\n",
              "      <td>0.614378</td>\n",
              "      <td>0.610104</td>\n",
              "      <td>0.607131</td>\n",
              "      <td>0.604818</td>\n",
              "      <td>0.602482</td>\n",
              "      <td>0.601290</td>\n",
              "      <td>0.599995</td>\n",
              "      <td>0.598909</td>\n",
              "      <td>0.599053</td>\n",
              "      <td>0.599842</td>\n",
              "      <td>0.601187</td>\n",
              "      <td>0.602825</td>\n",
              "      <td>0.606265</td>\n",
              "      <td>0.609677</td>\n",
              "      <td>0.614040</td>\n",
              "      <td>0.618327</td>\n",
              "      <td>...</td>\n",
              "      <td>1.688604</td>\n",
              "      <td>1.674469</td>\n",
              "      <td>1.660716</td>\n",
              "      <td>1.648262</td>\n",
              "      <td>1.630642</td>\n",
              "      <td>1.618590</td>\n",
              "      <td>1.606035</td>\n",
              "      <td>1.591950</td>\n",
              "      <td>1.580556</td>\n",
              "      <td>1.565986</td>\n",
              "      <td>1.555014</td>\n",
              "      <td>1.542866</td>\n",
              "      <td>1.533852</td>\n",
              "      <td>1.525125</td>\n",
              "      <td>1.514723</td>\n",
              "      <td>1.503877</td>\n",
              "      <td>1.495593</td>\n",
              "      <td>1.488977</td>\n",
              "      <td>1.479454</td>\n",
              "      <td>1.472006</td>\n",
              "      <td>1.465699</td>\n",
              "      <td>1.460134</td>\n",
              "      <td>1.458216</td>\n",
              "      <td>1.456837</td>\n",
              "      <td>1.456203</td>\n",
              "      <td>1.459403</td>\n",
              "      <td>1.460975</td>\n",
              "      <td>1.457055</td>\n",
              "      <td>1.457455</td>\n",
              "      <td>1.454225</td>\n",
              "      <td>1.447507</td>\n",
              "      <td>1.450268</td>\n",
              "      <td>1.451860</td>\n",
              "      <td>1.461692</td>\n",
              "      <td>1.471279</td>\n",
              "      <td>2440</td>\n",
              "      <td>39.057167</td>\n",
              "      <td>46.649500</td>\n",
              "      <td>105.6</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>0.538019</td>\n",
              "      <td>0.538283</td>\n",
              "      <td>0.542379</td>\n",
              "      <td>0.545535</td>\n",
              "      <td>0.549656</td>\n",
              "      <td>0.555612</td>\n",
              "      <td>0.563389</td>\n",
              "      <td>0.570042</td>\n",
              "      <td>0.579115</td>\n",
              "      <td>0.590913</td>\n",
              "      <td>0.604809</td>\n",
              "      <td>0.620944</td>\n",
              "      <td>0.628157</td>\n",
              "      <td>0.630772</td>\n",
              "      <td>0.631071</td>\n",
              "      <td>0.630588</td>\n",
              "      <td>0.628943</td>\n",
              "      <td>0.627554</td>\n",
              "      <td>0.625168</td>\n",
              "      <td>0.623034</td>\n",
              "      <td>0.620265</td>\n",
              "      <td>0.617370</td>\n",
              "      <td>0.614614</td>\n",
              "      <td>0.611324</td>\n",
              "      <td>0.608916</td>\n",
              "      <td>0.606037</td>\n",
              "      <td>0.603702</td>\n",
              "      <td>0.602188</td>\n",
              "      <td>0.600821</td>\n",
              "      <td>0.599539</td>\n",
              "      <td>0.598978</td>\n",
              "      <td>0.599364</td>\n",
              "      <td>0.599753</td>\n",
              "      <td>0.601123</td>\n",
              "      <td>0.603251</td>\n",
              "      <td>0.605764</td>\n",
              "      <td>0.608760</td>\n",
              "      <td>0.612281</td>\n",
              "      <td>0.615855</td>\n",
              "      <td>...</td>\n",
              "      <td>1.564822</td>\n",
              "      <td>1.550711</td>\n",
              "      <td>1.534420</td>\n",
              "      <td>1.521360</td>\n",
              "      <td>1.509736</td>\n",
              "      <td>1.496078</td>\n",
              "      <td>1.484581</td>\n",
              "      <td>1.472423</td>\n",
              "      <td>1.458472</td>\n",
              "      <td>1.450467</td>\n",
              "      <td>1.436744</td>\n",
              "      <td>1.426040</td>\n",
              "      <td>1.417529</td>\n",
              "      <td>1.408495</td>\n",
              "      <td>1.397023</td>\n",
              "      <td>1.391276</td>\n",
              "      <td>1.383204</td>\n",
              "      <td>1.374296</td>\n",
              "      <td>1.368845</td>\n",
              "      <td>1.361276</td>\n",
              "      <td>1.354829</td>\n",
              "      <td>1.349905</td>\n",
              "      <td>1.344804</td>\n",
              "      <td>1.343582</td>\n",
              "      <td>1.340867</td>\n",
              "      <td>1.341139</td>\n",
              "      <td>1.342966</td>\n",
              "      <td>1.342546</td>\n",
              "      <td>1.342661</td>\n",
              "      <td>1.343233</td>\n",
              "      <td>1.339520</td>\n",
              "      <td>1.331402</td>\n",
              "      <td>1.329552</td>\n",
              "      <td>1.334918</td>\n",
              "      <td>1.308238</td>\n",
              "      <td>2476</td>\n",
              "      <td>38.457833</td>\n",
              "      <td>51.680833</td>\n",
              "      <td>72.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>0.480685</td>\n",
              "      <td>0.484010</td>\n",
              "      <td>0.489706</td>\n",
              "      <td>0.494084</td>\n",
              "      <td>0.500947</td>\n",
              "      <td>0.507894</td>\n",
              "      <td>0.513360</td>\n",
              "      <td>0.520308</td>\n",
              "      <td>0.530952</td>\n",
              "      <td>0.543218</td>\n",
              "      <td>0.557018</td>\n",
              "      <td>0.567722</td>\n",
              "      <td>0.574330</td>\n",
              "      <td>0.577556</td>\n",
              "      <td>0.578793</td>\n",
              "      <td>0.578903</td>\n",
              "      <td>0.577392</td>\n",
              "      <td>0.575224</td>\n",
              "      <td>0.572716</td>\n",
              "      <td>0.568465</td>\n",
              "      <td>0.565585</td>\n",
              "      <td>0.562637</td>\n",
              "      <td>0.559241</td>\n",
              "      <td>0.556354</td>\n",
              "      <td>0.552418</td>\n",
              "      <td>0.549454</td>\n",
              "      <td>0.547636</td>\n",
              "      <td>0.545892</td>\n",
              "      <td>0.544410</td>\n",
              "      <td>0.543401</td>\n",
              "      <td>0.542475</td>\n",
              "      <td>0.543075</td>\n",
              "      <td>0.543687</td>\n",
              "      <td>0.544753</td>\n",
              "      <td>0.546528</td>\n",
              "      <td>0.549802</td>\n",
              "      <td>0.553025</td>\n",
              "      <td>0.556793</td>\n",
              "      <td>0.560706</td>\n",
              "      <td>...</td>\n",
              "      <td>1.539243</td>\n",
              "      <td>1.526498</td>\n",
              "      <td>1.512998</td>\n",
              "      <td>1.500887</td>\n",
              "      <td>1.486508</td>\n",
              "      <td>1.474466</td>\n",
              "      <td>1.461664</td>\n",
              "      <td>1.452243</td>\n",
              "      <td>1.439495</td>\n",
              "      <td>1.427213</td>\n",
              "      <td>1.416068</td>\n",
              "      <td>1.406622</td>\n",
              "      <td>1.398762</td>\n",
              "      <td>1.389438</td>\n",
              "      <td>1.379374</td>\n",
              "      <td>1.372073</td>\n",
              "      <td>1.363920</td>\n",
              "      <td>1.357083</td>\n",
              "      <td>1.351891</td>\n",
              "      <td>1.343235</td>\n",
              "      <td>1.339888</td>\n",
              "      <td>1.334037</td>\n",
              "      <td>1.332106</td>\n",
              "      <td>1.329624</td>\n",
              "      <td>1.332323</td>\n",
              "      <td>1.330829</td>\n",
              "      <td>1.334422</td>\n",
              "      <td>1.328862</td>\n",
              "      <td>1.324434</td>\n",
              "      <td>1.315813</td>\n",
              "      <td>1.300146</td>\n",
              "      <td>1.295544</td>\n",
              "      <td>1.293936</td>\n",
              "      <td>1.296249</td>\n",
              "      <td>1.294994</td>\n",
              "      <td>2478</td>\n",
              "      <td>42.028167</td>\n",
              "      <td>43.518667</td>\n",
              "      <td>68.4</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>0.505587</td>\n",
              "      <td>0.510463</td>\n",
              "      <td>0.515027</td>\n",
              "      <td>0.520545</td>\n",
              "      <td>0.528644</td>\n",
              "      <td>0.539653</td>\n",
              "      <td>0.547931</td>\n",
              "      <td>0.553654</td>\n",
              "      <td>0.560278</td>\n",
              "      <td>0.571240</td>\n",
              "      <td>0.582939</td>\n",
              "      <td>0.597353</td>\n",
              "      <td>0.603132</td>\n",
              "      <td>0.606054</td>\n",
              "      <td>0.607849</td>\n",
              "      <td>0.608324</td>\n",
              "      <td>0.607437</td>\n",
              "      <td>0.605870</td>\n",
              "      <td>0.602386</td>\n",
              "      <td>0.599726</td>\n",
              "      <td>0.596480</td>\n",
              "      <td>0.593364</td>\n",
              "      <td>0.590350</td>\n",
              "      <td>0.587280</td>\n",
              "      <td>0.584094</td>\n",
              "      <td>0.580810</td>\n",
              "      <td>0.578204</td>\n",
              "      <td>0.576421</td>\n",
              "      <td>0.574798</td>\n",
              "      <td>0.572748</td>\n",
              "      <td>0.572369</td>\n",
              "      <td>0.572139</td>\n",
              "      <td>0.572084</td>\n",
              "      <td>0.572998</td>\n",
              "      <td>0.574243</td>\n",
              "      <td>0.576048</td>\n",
              "      <td>0.578608</td>\n",
              "      <td>0.582545</td>\n",
              "      <td>0.586093</td>\n",
              "      <td>...</td>\n",
              "      <td>1.453705</td>\n",
              "      <td>1.442506</td>\n",
              "      <td>1.431383</td>\n",
              "      <td>1.420372</td>\n",
              "      <td>1.409361</td>\n",
              "      <td>1.398739</td>\n",
              "      <td>1.384815</td>\n",
              "      <td>1.375346</td>\n",
              "      <td>1.364956</td>\n",
              "      <td>1.358352</td>\n",
              "      <td>1.346600</td>\n",
              "      <td>1.337193</td>\n",
              "      <td>1.329186</td>\n",
              "      <td>1.319507</td>\n",
              "      <td>1.312434</td>\n",
              "      <td>1.305864</td>\n",
              "      <td>1.297765</td>\n",
              "      <td>1.292396</td>\n",
              "      <td>1.287311</td>\n",
              "      <td>1.279727</td>\n",
              "      <td>1.274297</td>\n",
              "      <td>1.272475</td>\n",
              "      <td>1.267522</td>\n",
              "      <td>1.265180</td>\n",
              "      <td>1.264382</td>\n",
              "      <td>1.263661</td>\n",
              "      <td>1.261225</td>\n",
              "      <td>1.255700</td>\n",
              "      <td>1.249614</td>\n",
              "      <td>1.239259</td>\n",
              "      <td>1.224037</td>\n",
              "      <td>1.213512</td>\n",
              "      <td>1.204171</td>\n",
              "      <td>1.216543</td>\n",
              "      <td>1.208677</td>\n",
              "      <td>2559</td>\n",
              "      <td>43.543333</td>\n",
              "      <td>23.219333</td>\n",
              "      <td>106.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>0.546671</td>\n",
              "      <td>0.548270</td>\n",
              "      <td>0.551964</td>\n",
              "      <td>0.555424</td>\n",
              "      <td>0.560574</td>\n",
              "      <td>0.566887</td>\n",
              "      <td>0.572948</td>\n",
              "      <td>0.580261</td>\n",
              "      <td>0.591718</td>\n",
              "      <td>0.605186</td>\n",
              "      <td>0.620562</td>\n",
              "      <td>0.632075</td>\n",
              "      <td>0.639916</td>\n",
              "      <td>0.643679</td>\n",
              "      <td>0.645930</td>\n",
              "      <td>0.645582</td>\n",
              "      <td>0.644002</td>\n",
              "      <td>0.641777</td>\n",
              "      <td>0.638956</td>\n",
              "      <td>0.634722</td>\n",
              "      <td>0.631684</td>\n",
              "      <td>0.627920</td>\n",
              "      <td>0.624216</td>\n",
              "      <td>0.620549</td>\n",
              "      <td>0.615449</td>\n",
              "      <td>0.612441</td>\n",
              "      <td>0.609487</td>\n",
              "      <td>0.606799</td>\n",
              "      <td>0.604819</td>\n",
              "      <td>0.602992</td>\n",
              "      <td>0.601829</td>\n",
              "      <td>0.600981</td>\n",
              "      <td>0.601903</td>\n",
              "      <td>0.602502</td>\n",
              "      <td>0.604280</td>\n",
              "      <td>0.606917</td>\n",
              "      <td>0.610150</td>\n",
              "      <td>0.614290</td>\n",
              "      <td>0.618448</td>\n",
              "      <td>...</td>\n",
              "      <td>1.752160</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.729738</td>\n",
              "      <td>1.718851</td>\n",
              "      <td>1.703801</td>\n",
              "      <td>1.691461</td>\n",
              "      <td>1.676763</td>\n",
              "      <td>1.665650</td>\n",
              "      <td>1.656921</td>\n",
              "      <td>1.639813</td>\n",
              "      <td>1.628050</td>\n",
              "      <td>1.619605</td>\n",
              "      <td>1.606320</td>\n",
              "      <td>1.599001</td>\n",
              "      <td>1.585844</td>\n",
              "      <td>1.576560</td>\n",
              "      <td>1.566763</td>\n",
              "      <td>1.559222</td>\n",
              "      <td>1.548711</td>\n",
              "      <td>1.540025</td>\n",
              "      <td>1.533041</td>\n",
              "      <td>1.530444</td>\n",
              "      <td>1.526929</td>\n",
              "      <td>1.524400</td>\n",
              "      <td>1.525873</td>\n",
              "      <td>1.531209</td>\n",
              "      <td>1.535637</td>\n",
              "      <td>1.539162</td>\n",
              "      <td>1.538592</td>\n",
              "      <td>1.541236</td>\n",
              "      <td>1.543308</td>\n",
              "      <td>1.550344</td>\n",
              "      <td>1.545398</td>\n",
              "      <td>1.560044</td>\n",
              "      <td>1.571627</td>\n",
              "      <td>2581</td>\n",
              "      <td>38.824833</td>\n",
              "      <td>45.364167</td>\n",
              "      <td>31.2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0  absorbance0  ...  cholesterol_ldl_value  cholesterol_ldl_human\n",
              "0           0     0.535147  ...                  105.6                      1\n",
              "1           1     0.538019  ...                   72.4                      1\n",
              "2           2     0.480685  ...                   68.4                      1\n",
              "3           3     0.505587  ...                  106.0                      1\n",
              "4           4     0.546671  ...                   31.2                      0\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg2= pd.read_csv(file_name2)\n",
        "df_agg2.describe()\n",
        "df_agg2.drop(df_agg2[df_agg2.absorbance0 < 0].index, inplace=True) # kill the negative elements\n"
      ],
      "metadata": {
        "id": "VbOP7ZtiBKBM"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg=pd.concat([df_agg1,df_agg2], ignore_index=True)"
      ],
      "metadata": {
        "id": "6hFjkYViA-Di"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg.tail()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 273
        },
        "id": "Y5xTiFTT0r-n",
        "outputId": "3710cf5e-3b46-4b0f-8164-14e1e43e04e1"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>537</th>\n",
              "      <td>405</td>\n",
              "      <td>0.559508</td>\n",
              "      <td>0.557981</td>\n",
              "      <td>0.562073</td>\n",
              "      <td>0.566272</td>\n",
              "      <td>0.571244</td>\n",
              "      <td>0.577397</td>\n",
              "      <td>0.583213</td>\n",
              "      <td>0.590163</td>\n",
              "      <td>0.598694</td>\n",
              "      <td>0.610475</td>\n",
              "      <td>0.623472</td>\n",
              "      <td>0.636641</td>\n",
              "      <td>0.642579</td>\n",
              "      <td>0.645626</td>\n",
              "      <td>0.647103</td>\n",
              "      <td>0.645668</td>\n",
              "      <td>0.643687</td>\n",
              "      <td>0.640753</td>\n",
              "      <td>0.637975</td>\n",
              "      <td>0.635578</td>\n",
              "      <td>0.633089</td>\n",
              "      <td>0.630470</td>\n",
              "      <td>0.627600</td>\n",
              "      <td>0.624106</td>\n",
              "      <td>0.622028</td>\n",
              "      <td>0.619228</td>\n",
              "      <td>0.617372</td>\n",
              "      <td>0.616006</td>\n",
              "      <td>0.614953</td>\n",
              "      <td>0.614089</td>\n",
              "      <td>0.614401</td>\n",
              "      <td>0.614494</td>\n",
              "      <td>0.615416</td>\n",
              "      <td>0.617037</td>\n",
              "      <td>0.619308</td>\n",
              "      <td>0.623084</td>\n",
              "      <td>0.626258</td>\n",
              "      <td>0.629867</td>\n",
              "      <td>0.634516</td>\n",
              "      <td>...</td>\n",
              "      <td>1.440180</td>\n",
              "      <td>1.429945</td>\n",
              "      <td>1.421697</td>\n",
              "      <td>1.412292</td>\n",
              "      <td>1.403772</td>\n",
              "      <td>1.395936</td>\n",
              "      <td>1.385628</td>\n",
              "      <td>1.377189</td>\n",
              "      <td>1.370285</td>\n",
              "      <td>1.363224</td>\n",
              "      <td>1.354198</td>\n",
              "      <td>1.347067</td>\n",
              "      <td>1.339531</td>\n",
              "      <td>1.332175</td>\n",
              "      <td>1.326958</td>\n",
              "      <td>1.320780</td>\n",
              "      <td>1.315768</td>\n",
              "      <td>1.311262</td>\n",
              "      <td>1.304993</td>\n",
              "      <td>1.299778</td>\n",
              "      <td>1.296999</td>\n",
              "      <td>1.292156</td>\n",
              "      <td>1.289398</td>\n",
              "      <td>1.285569</td>\n",
              "      <td>1.286972</td>\n",
              "      <td>1.283523</td>\n",
              "      <td>1.281386</td>\n",
              "      <td>1.279686</td>\n",
              "      <td>1.274868</td>\n",
              "      <td>1.271164</td>\n",
              "      <td>1.265842</td>\n",
              "      <td>1.263005</td>\n",
              "      <td>1.266337</td>\n",
              "      <td>1.297181</td>\n",
              "      <td>1.266201</td>\n",
              "      <td>11886</td>\n",
              "      <td>36.536500</td>\n",
              "      <td>65.203000</td>\n",
              "      <td>88.54285</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>538</th>\n",
              "      <td>406</td>\n",
              "      <td>0.587427</td>\n",
              "      <td>0.585250</td>\n",
              "      <td>0.589764</td>\n",
              "      <td>0.594061</td>\n",
              "      <td>0.599806</td>\n",
              "      <td>0.604902</td>\n",
              "      <td>0.609408</td>\n",
              "      <td>0.614733</td>\n",
              "      <td>0.620852</td>\n",
              "      <td>0.630668</td>\n",
              "      <td>0.641564</td>\n",
              "      <td>0.653106</td>\n",
              "      <td>0.657908</td>\n",
              "      <td>0.659866</td>\n",
              "      <td>0.660317</td>\n",
              "      <td>0.658655</td>\n",
              "      <td>0.655788</td>\n",
              "      <td>0.652237</td>\n",
              "      <td>0.648927</td>\n",
              "      <td>0.646154</td>\n",
              "      <td>0.643198</td>\n",
              "      <td>0.640632</td>\n",
              "      <td>0.637537</td>\n",
              "      <td>0.633689</td>\n",
              "      <td>0.631356</td>\n",
              "      <td>0.628085</td>\n",
              "      <td>0.625785</td>\n",
              "      <td>0.624119</td>\n",
              "      <td>0.622667</td>\n",
              "      <td>0.621206</td>\n",
              "      <td>0.620691</td>\n",
              "      <td>0.620383</td>\n",
              "      <td>0.620737</td>\n",
              "      <td>0.621599</td>\n",
              "      <td>0.623025</td>\n",
              "      <td>0.625937</td>\n",
              "      <td>0.628400</td>\n",
              "      <td>0.631338</td>\n",
              "      <td>0.635659</td>\n",
              "      <td>...</td>\n",
              "      <td>1.463203</td>\n",
              "      <td>1.454463</td>\n",
              "      <td>1.445494</td>\n",
              "      <td>1.436059</td>\n",
              "      <td>1.426811</td>\n",
              "      <td>1.417521</td>\n",
              "      <td>1.406092</td>\n",
              "      <td>1.397415</td>\n",
              "      <td>1.388936</td>\n",
              "      <td>1.383776</td>\n",
              "      <td>1.372381</td>\n",
              "      <td>1.364249</td>\n",
              "      <td>1.355797</td>\n",
              "      <td>1.347775</td>\n",
              "      <td>1.341057</td>\n",
              "      <td>1.333965</td>\n",
              "      <td>1.328754</td>\n",
              "      <td>1.321832</td>\n",
              "      <td>1.315572</td>\n",
              "      <td>1.309635</td>\n",
              "      <td>1.303772</td>\n",
              "      <td>1.299180</td>\n",
              "      <td>1.295138</td>\n",
              "      <td>1.292522</td>\n",
              "      <td>1.289935</td>\n",
              "      <td>1.287213</td>\n",
              "      <td>1.282831</td>\n",
              "      <td>1.277070</td>\n",
              "      <td>1.270260</td>\n",
              "      <td>1.253691</td>\n",
              "      <td>1.242504</td>\n",
              "      <td>1.236326</td>\n",
              "      <td>1.236166</td>\n",
              "      <td>1.250518</td>\n",
              "      <td>1.236380</td>\n",
              "      <td>11889</td>\n",
              "      <td>39.054667</td>\n",
              "      <td>56.012500</td>\n",
              "      <td>159.68645</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>539</th>\n",
              "      <td>407</td>\n",
              "      <td>0.477241</td>\n",
              "      <td>0.481792</td>\n",
              "      <td>0.483792</td>\n",
              "      <td>0.489861</td>\n",
              "      <td>0.497037</td>\n",
              "      <td>0.504735</td>\n",
              "      <td>0.509637</td>\n",
              "      <td>0.514202</td>\n",
              "      <td>0.520144</td>\n",
              "      <td>0.530000</td>\n",
              "      <td>0.541345</td>\n",
              "      <td>0.550381</td>\n",
              "      <td>0.555803</td>\n",
              "      <td>0.558563</td>\n",
              "      <td>0.559343</td>\n",
              "      <td>0.558502</td>\n",
              "      <td>0.556676</td>\n",
              "      <td>0.554130</td>\n",
              "      <td>0.551494</td>\n",
              "      <td>0.547095</td>\n",
              "      <td>0.543851</td>\n",
              "      <td>0.540543</td>\n",
              "      <td>0.537108</td>\n",
              "      <td>0.533737</td>\n",
              "      <td>0.530775</td>\n",
              "      <td>0.527208</td>\n",
              "      <td>0.524530</td>\n",
              "      <td>0.523219</td>\n",
              "      <td>0.521271</td>\n",
              "      <td>0.520013</td>\n",
              "      <td>0.519327</td>\n",
              "      <td>0.518718</td>\n",
              "      <td>0.518963</td>\n",
              "      <td>0.519878</td>\n",
              "      <td>0.520868</td>\n",
              "      <td>0.522727</td>\n",
              "      <td>0.525264</td>\n",
              "      <td>0.528919</td>\n",
              "      <td>0.532656</td>\n",
              "      <td>...</td>\n",
              "      <td>1.494378</td>\n",
              "      <td>1.478303</td>\n",
              "      <td>1.463623</td>\n",
              "      <td>1.448563</td>\n",
              "      <td>1.434307</td>\n",
              "      <td>1.419192</td>\n",
              "      <td>1.402043</td>\n",
              "      <td>1.388613</td>\n",
              "      <td>1.375551</td>\n",
              "      <td>1.366222</td>\n",
              "      <td>1.351601</td>\n",
              "      <td>1.340437</td>\n",
              "      <td>1.327134</td>\n",
              "      <td>1.318319</td>\n",
              "      <td>1.309765</td>\n",
              "      <td>1.300136</td>\n",
              "      <td>1.292696</td>\n",
              "      <td>1.283508</td>\n",
              "      <td>1.275061</td>\n",
              "      <td>1.269340</td>\n",
              "      <td>1.263197</td>\n",
              "      <td>1.258333</td>\n",
              "      <td>1.253829</td>\n",
              "      <td>1.251677</td>\n",
              "      <td>1.249435</td>\n",
              "      <td>1.246182</td>\n",
              "      <td>1.243464</td>\n",
              "      <td>1.237783</td>\n",
              "      <td>1.227517</td>\n",
              "      <td>1.206936</td>\n",
              "      <td>1.179302</td>\n",
              "      <td>1.162252</td>\n",
              "      <td>1.156460</td>\n",
              "      <td>1.167845</td>\n",
              "      <td>1.159627</td>\n",
              "      <td>11890</td>\n",
              "      <td>46.116000</td>\n",
              "      <td>46.849000</td>\n",
              "      <td>99.20000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>540</th>\n",
              "      <td>408</td>\n",
              "      <td>0.565670</td>\n",
              "      <td>0.564820</td>\n",
              "      <td>0.569749</td>\n",
              "      <td>0.573770</td>\n",
              "      <td>0.580790</td>\n",
              "      <td>0.586531</td>\n",
              "      <td>0.591289</td>\n",
              "      <td>0.596792</td>\n",
              "      <td>0.603492</td>\n",
              "      <td>0.613606</td>\n",
              "      <td>0.625469</td>\n",
              "      <td>0.637730</td>\n",
              "      <td>0.642756</td>\n",
              "      <td>0.645300</td>\n",
              "      <td>0.646095</td>\n",
              "      <td>0.644386</td>\n",
              "      <td>0.641853</td>\n",
              "      <td>0.638425</td>\n",
              "      <td>0.635619</td>\n",
              "      <td>0.633162</td>\n",
              "      <td>0.630523</td>\n",
              "      <td>0.627955</td>\n",
              "      <td>0.624878</td>\n",
              "      <td>0.621162</td>\n",
              "      <td>0.618793</td>\n",
              "      <td>0.615519</td>\n",
              "      <td>0.613485</td>\n",
              "      <td>0.611918</td>\n",
              "      <td>0.610617</td>\n",
              "      <td>0.609168</td>\n",
              "      <td>0.608868</td>\n",
              "      <td>0.608688</td>\n",
              "      <td>0.609372</td>\n",
              "      <td>0.610378</td>\n",
              "      <td>0.612109</td>\n",
              "      <td>0.615408</td>\n",
              "      <td>0.618294</td>\n",
              "      <td>0.621513</td>\n",
              "      <td>0.626012</td>\n",
              "      <td>...</td>\n",
              "      <td>1.506006</td>\n",
              "      <td>1.496877</td>\n",
              "      <td>1.486883</td>\n",
              "      <td>1.475934</td>\n",
              "      <td>1.466819</td>\n",
              "      <td>1.456462</td>\n",
              "      <td>1.445260</td>\n",
              "      <td>1.435506</td>\n",
              "      <td>1.426113</td>\n",
              "      <td>1.419729</td>\n",
              "      <td>1.409409</td>\n",
              "      <td>1.402097</td>\n",
              "      <td>1.391310</td>\n",
              "      <td>1.384826</td>\n",
              "      <td>1.377594</td>\n",
              "      <td>1.371748</td>\n",
              "      <td>1.366276</td>\n",
              "      <td>1.358685</td>\n",
              "      <td>1.354177</td>\n",
              "      <td>1.348492</td>\n",
              "      <td>1.342599</td>\n",
              "      <td>1.337098</td>\n",
              "      <td>1.332178</td>\n",
              "      <td>1.329692</td>\n",
              "      <td>1.329856</td>\n",
              "      <td>1.327782</td>\n",
              "      <td>1.323786</td>\n",
              "      <td>1.317777</td>\n",
              "      <td>1.305306</td>\n",
              "      <td>1.290804</td>\n",
              "      <td>1.276198</td>\n",
              "      <td>1.268350</td>\n",
              "      <td>1.263184</td>\n",
              "      <td>1.284706</td>\n",
              "      <td>1.269695</td>\n",
              "      <td>11891</td>\n",
              "      <td>40.384833</td>\n",
              "      <td>51.917167</td>\n",
              "      <td>164.71290</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>541</th>\n",
              "      <td>409</td>\n",
              "      <td>0.462006</td>\n",
              "      <td>0.463919</td>\n",
              "      <td>0.466006</td>\n",
              "      <td>0.470398</td>\n",
              "      <td>0.474160</td>\n",
              "      <td>0.479535</td>\n",
              "      <td>0.484806</td>\n",
              "      <td>0.490237</td>\n",
              "      <td>0.499680</td>\n",
              "      <td>0.509926</td>\n",
              "      <td>0.522268</td>\n",
              "      <td>0.532148</td>\n",
              "      <td>0.538059</td>\n",
              "      <td>0.542097</td>\n",
              "      <td>0.543162</td>\n",
              "      <td>0.542517</td>\n",
              "      <td>0.541707</td>\n",
              "      <td>0.539483</td>\n",
              "      <td>0.537062</td>\n",
              "      <td>0.533966</td>\n",
              "      <td>0.531556</td>\n",
              "      <td>0.529130</td>\n",
              "      <td>0.525741</td>\n",
              "      <td>0.521571</td>\n",
              "      <td>0.516842</td>\n",
              "      <td>0.513043</td>\n",
              "      <td>0.510353</td>\n",
              "      <td>0.507669</td>\n",
              "      <td>0.505915</td>\n",
              "      <td>0.503540</td>\n",
              "      <td>0.502311</td>\n",
              "      <td>0.502070</td>\n",
              "      <td>0.501865</td>\n",
              "      <td>0.502888</td>\n",
              "      <td>0.503878</td>\n",
              "      <td>0.506279</td>\n",
              "      <td>0.508641</td>\n",
              "      <td>0.511671</td>\n",
              "      <td>0.515179</td>\n",
              "      <td>...</td>\n",
              "      <td>1.332708</td>\n",
              "      <td>1.319521</td>\n",
              "      <td>1.308515</td>\n",
              "      <td>1.298146</td>\n",
              "      <td>1.288379</td>\n",
              "      <td>1.277194</td>\n",
              "      <td>1.267351</td>\n",
              "      <td>1.254786</td>\n",
              "      <td>1.246378</td>\n",
              "      <td>1.239163</td>\n",
              "      <td>1.228400</td>\n",
              "      <td>1.219838</td>\n",
              "      <td>1.210519</td>\n",
              "      <td>1.203041</td>\n",
              "      <td>1.196190</td>\n",
              "      <td>1.189647</td>\n",
              "      <td>1.183361</td>\n",
              "      <td>1.174566</td>\n",
              "      <td>1.170406</td>\n",
              "      <td>1.165284</td>\n",
              "      <td>1.159469</td>\n",
              "      <td>1.155536</td>\n",
              "      <td>1.153178</td>\n",
              "      <td>1.149640</td>\n",
              "      <td>1.149195</td>\n",
              "      <td>1.147773</td>\n",
              "      <td>1.148173</td>\n",
              "      <td>1.147194</td>\n",
              "      <td>1.146067</td>\n",
              "      <td>1.146250</td>\n",
              "      <td>1.149830</td>\n",
              "      <td>1.153864</td>\n",
              "      <td>1.162086</td>\n",
              "      <td>1.177496</td>\n",
              "      <td>1.168120</td>\n",
              "      <td>11966</td>\n",
              "      <td>36.485833</td>\n",
              "      <td>50.900167</td>\n",
              "      <td>98.99000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     Unnamed: 0  absorbance0  ...  cholesterol_ldl_value  cholesterol_ldl_human\n",
              "537         405     0.559508  ...               88.54285                      1\n",
              "538         406     0.587427  ...              159.68645                      3\n",
              "539         407     0.477241  ...               99.20000                      1\n",
              "540         408     0.565670  ...              164.71290                      3\n",
              "541         409     0.462006  ...               98.99000                      1\n",
              "\n",
              "[5 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_agg.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        },
        "id": "NzXlFWOu0W2K",
        "outputId": "936dccd7-6dd6-4219-bc28-b55d1c8ed027"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>absorbance0</th>\n",
              "      <th>absorbance1</th>\n",
              "      <th>absorbance2</th>\n",
              "      <th>absorbance3</th>\n",
              "      <th>absorbance4</th>\n",
              "      <th>absorbance5</th>\n",
              "      <th>absorbance6</th>\n",
              "      <th>absorbance7</th>\n",
              "      <th>absorbance8</th>\n",
              "      <th>absorbance9</th>\n",
              "      <th>absorbance10</th>\n",
              "      <th>absorbance11</th>\n",
              "      <th>absorbance12</th>\n",
              "      <th>absorbance13</th>\n",
              "      <th>absorbance14</th>\n",
              "      <th>absorbance15</th>\n",
              "      <th>absorbance16</th>\n",
              "      <th>absorbance17</th>\n",
              "      <th>absorbance18</th>\n",
              "      <th>absorbance19</th>\n",
              "      <th>absorbance20</th>\n",
              "      <th>absorbance21</th>\n",
              "      <th>absorbance22</th>\n",
              "      <th>absorbance23</th>\n",
              "      <th>absorbance24</th>\n",
              "      <th>absorbance25</th>\n",
              "      <th>absorbance26</th>\n",
              "      <th>absorbance27</th>\n",
              "      <th>absorbance28</th>\n",
              "      <th>absorbance29</th>\n",
              "      <th>absorbance30</th>\n",
              "      <th>absorbance31</th>\n",
              "      <th>absorbance32</th>\n",
              "      <th>absorbance33</th>\n",
              "      <th>absorbance34</th>\n",
              "      <th>absorbance35</th>\n",
              "      <th>absorbance36</th>\n",
              "      <th>absorbance37</th>\n",
              "      <th>absorbance38</th>\n",
              "      <th>...</th>\n",
              "      <th>absorbance135</th>\n",
              "      <th>absorbance136</th>\n",
              "      <th>absorbance137</th>\n",
              "      <th>absorbance138</th>\n",
              "      <th>absorbance139</th>\n",
              "      <th>absorbance140</th>\n",
              "      <th>absorbance141</th>\n",
              "      <th>absorbance142</th>\n",
              "      <th>absorbance143</th>\n",
              "      <th>absorbance144</th>\n",
              "      <th>absorbance145</th>\n",
              "      <th>absorbance146</th>\n",
              "      <th>absorbance147</th>\n",
              "      <th>absorbance148</th>\n",
              "      <th>absorbance149</th>\n",
              "      <th>absorbance150</th>\n",
              "      <th>absorbance151</th>\n",
              "      <th>absorbance152</th>\n",
              "      <th>absorbance153</th>\n",
              "      <th>absorbance154</th>\n",
              "      <th>absorbance155</th>\n",
              "      <th>absorbance156</th>\n",
              "      <th>absorbance157</th>\n",
              "      <th>absorbance158</th>\n",
              "      <th>absorbance159</th>\n",
              "      <th>absorbance160</th>\n",
              "      <th>absorbance161</th>\n",
              "      <th>absorbance162</th>\n",
              "      <th>absorbance163</th>\n",
              "      <th>absorbance164</th>\n",
              "      <th>absorbance165</th>\n",
              "      <th>absorbance166</th>\n",
              "      <th>absorbance167</th>\n",
              "      <th>absorbance168</th>\n",
              "      <th>absorbance169</th>\n",
              "      <th>donation_id</th>\n",
              "      <th>temperature</th>\n",
              "      <th>humidity</th>\n",
              "      <th>cholesterol_ldl_value</th>\n",
              "      <th>cholesterol_ldl_human</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "      <td>542.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>170.702952</td>\n",
              "      <td>0.520451</td>\n",
              "      <td>0.522427</td>\n",
              "      <td>0.525972</td>\n",
              "      <td>0.530680</td>\n",
              "      <td>0.536854</td>\n",
              "      <td>0.543551</td>\n",
              "      <td>0.549648</td>\n",
              "      <td>0.555232</td>\n",
              "      <td>0.563119</td>\n",
              "      <td>0.574407</td>\n",
              "      <td>0.587313</td>\n",
              "      <td>0.598591</td>\n",
              "      <td>0.605160</td>\n",
              "      <td>0.608182</td>\n",
              "      <td>0.609431</td>\n",
              "      <td>0.609150</td>\n",
              "      <td>0.607543</td>\n",
              "      <td>0.605090</td>\n",
              "      <td>0.602129</td>\n",
              "      <td>0.598775</td>\n",
              "      <td>0.595773</td>\n",
              "      <td>0.592707</td>\n",
              "      <td>0.589487</td>\n",
              "      <td>0.585683</td>\n",
              "      <td>0.582415</td>\n",
              "      <td>0.579220</td>\n",
              "      <td>0.576781</td>\n",
              "      <td>0.574689</td>\n",
              "      <td>0.572943</td>\n",
              "      <td>0.571480</td>\n",
              "      <td>0.570644</td>\n",
              "      <td>0.570247</td>\n",
              "      <td>0.570480</td>\n",
              "      <td>0.571400</td>\n",
              "      <td>0.572933</td>\n",
              "      <td>0.575169</td>\n",
              "      <td>0.577869</td>\n",
              "      <td>0.581283</td>\n",
              "      <td>0.585061</td>\n",
              "      <td>...</td>\n",
              "      <td>1.497814</td>\n",
              "      <td>1.485553</td>\n",
              "      <td>1.474109</td>\n",
              "      <td>1.463112</td>\n",
              "      <td>1.450626</td>\n",
              "      <td>1.439371</td>\n",
              "      <td>1.428135</td>\n",
              "      <td>1.417230</td>\n",
              "      <td>1.406798</td>\n",
              "      <td>1.398556</td>\n",
              "      <td>1.385953</td>\n",
              "      <td>1.377019</td>\n",
              "      <td>1.368533</td>\n",
              "      <td>1.360171</td>\n",
              "      <td>1.351819</td>\n",
              "      <td>1.344645</td>\n",
              "      <td>1.337181</td>\n",
              "      <td>1.330049</td>\n",
              "      <td>1.323455</td>\n",
              "      <td>1.316835</td>\n",
              "      <td>1.311906</td>\n",
              "      <td>1.307739</td>\n",
              "      <td>1.304195</td>\n",
              "      <td>1.301982</td>\n",
              "      <td>1.301178</td>\n",
              "      <td>1.300802</td>\n",
              "      <td>1.300394</td>\n",
              "      <td>1.299197</td>\n",
              "      <td>1.295539</td>\n",
              "      <td>1.289453</td>\n",
              "      <td>1.282270</td>\n",
              "      <td>1.275940</td>\n",
              "      <td>1.272953</td>\n",
              "      <td>1.287608</td>\n",
              "      <td>1.275556</td>\n",
              "      <td>6927.533210</td>\n",
              "      <td>42.111418</td>\n",
              "      <td>39.222955</td>\n",
              "      <td>111.436592</td>\n",
              "      <td>1.485240</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>120.507914</td>\n",
              "      <td>0.040351</td>\n",
              "      <td>0.039984</td>\n",
              "      <td>0.040093</td>\n",
              "      <td>0.040035</td>\n",
              "      <td>0.040144</td>\n",
              "      <td>0.040521</td>\n",
              "      <td>0.040435</td>\n",
              "      <td>0.040376</td>\n",
              "      <td>0.040171</td>\n",
              "      <td>0.039981</td>\n",
              "      <td>0.039891</td>\n",
              "      <td>0.039888</td>\n",
              "      <td>0.039833</td>\n",
              "      <td>0.039825</td>\n",
              "      <td>0.039900</td>\n",
              "      <td>0.039866</td>\n",
              "      <td>0.039767</td>\n",
              "      <td>0.039718</td>\n",
              "      <td>0.039647</td>\n",
              "      <td>0.039748</td>\n",
              "      <td>0.039703</td>\n",
              "      <td>0.039702</td>\n",
              "      <td>0.039726</td>\n",
              "      <td>0.039846</td>\n",
              "      <td>0.040008</td>\n",
              "      <td>0.040042</td>\n",
              "      <td>0.040114</td>\n",
              "      <td>0.040156</td>\n",
              "      <td>0.040191</td>\n",
              "      <td>0.040191</td>\n",
              "      <td>0.040187</td>\n",
              "      <td>0.040152</td>\n",
              "      <td>0.040178</td>\n",
              "      <td>0.040161</td>\n",
              "      <td>0.040140</td>\n",
              "      <td>0.040145</td>\n",
              "      <td>0.040169</td>\n",
              "      <td>0.040266</td>\n",
              "      <td>0.040271</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091700</td>\n",
              "      <td>0.090305</td>\n",
              "      <td>0.088924</td>\n",
              "      <td>0.087603</td>\n",
              "      <td>0.085971</td>\n",
              "      <td>0.084668</td>\n",
              "      <td>0.083410</td>\n",
              "      <td>0.082486</td>\n",
              "      <td>0.081525</td>\n",
              "      <td>0.080697</td>\n",
              "      <td>0.079354</td>\n",
              "      <td>0.078591</td>\n",
              "      <td>0.077906</td>\n",
              "      <td>0.077287</td>\n",
              "      <td>0.076565</td>\n",
              "      <td>0.076015</td>\n",
              "      <td>0.075402</td>\n",
              "      <td>0.074832</td>\n",
              "      <td>0.074297</td>\n",
              "      <td>0.073843</td>\n",
              "      <td>0.073528</td>\n",
              "      <td>0.073310</td>\n",
              "      <td>0.073180</td>\n",
              "      <td>0.073283</td>\n",
              "      <td>0.073591</td>\n",
              "      <td>0.074264</td>\n",
              "      <td>0.075230</td>\n",
              "      <td>0.076718</td>\n",
              "      <td>0.079323</td>\n",
              "      <td>0.083310</td>\n",
              "      <td>0.088549</td>\n",
              "      <td>0.093928</td>\n",
              "      <td>0.098151</td>\n",
              "      <td>0.104133</td>\n",
              "      <td>0.101605</td>\n",
              "      <td>3094.955454</td>\n",
              "      <td>3.420590</td>\n",
              "      <td>8.948404</td>\n",
              "      <td>44.306810</td>\n",
              "      <td>1.031287</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.400606</td>\n",
              "      <td>0.403625</td>\n",
              "      <td>0.407266</td>\n",
              "      <td>0.411518</td>\n",
              "      <td>0.416126</td>\n",
              "      <td>0.421518</td>\n",
              "      <td>0.426629</td>\n",
              "      <td>0.433139</td>\n",
              "      <td>0.444069</td>\n",
              "      <td>0.456182</td>\n",
              "      <td>0.469249</td>\n",
              "      <td>0.480239</td>\n",
              "      <td>0.486473</td>\n",
              "      <td>0.489661</td>\n",
              "      <td>0.490777</td>\n",
              "      <td>0.490226</td>\n",
              "      <td>0.488814</td>\n",
              "      <td>0.486476</td>\n",
              "      <td>0.483668</td>\n",
              "      <td>0.479653</td>\n",
              "      <td>0.476146</td>\n",
              "      <td>0.473155</td>\n",
              "      <td>0.469834</td>\n",
              "      <td>0.466424</td>\n",
              "      <td>0.462388</td>\n",
              "      <td>0.459335</td>\n",
              "      <td>0.457297</td>\n",
              "      <td>0.455538</td>\n",
              "      <td>0.453853</td>\n",
              "      <td>0.452486</td>\n",
              "      <td>0.451719</td>\n",
              "      <td>0.451624</td>\n",
              "      <td>0.452026</td>\n",
              "      <td>0.453133</td>\n",
              "      <td>0.454851</td>\n",
              "      <td>0.457279</td>\n",
              "      <td>0.460600</td>\n",
              "      <td>0.463820</td>\n",
              "      <td>0.467916</td>\n",
              "      <td>...</td>\n",
              "      <td>1.202957</td>\n",
              "      <td>1.193580</td>\n",
              "      <td>1.184336</td>\n",
              "      <td>1.175013</td>\n",
              "      <td>1.163368</td>\n",
              "      <td>1.154944</td>\n",
              "      <td>1.145616</td>\n",
              "      <td>1.137301</td>\n",
              "      <td>1.128859</td>\n",
              "      <td>1.118666</td>\n",
              "      <td>1.110864</td>\n",
              "      <td>1.102883</td>\n",
              "      <td>1.096296</td>\n",
              "      <td>1.089810</td>\n",
              "      <td>1.080864</td>\n",
              "      <td>1.075002</td>\n",
              "      <td>1.068992</td>\n",
              "      <td>1.063488</td>\n",
              "      <td>1.057937</td>\n",
              "      <td>1.052125</td>\n",
              "      <td>1.047685</td>\n",
              "      <td>1.044301</td>\n",
              "      <td>1.041746</td>\n",
              "      <td>1.039873</td>\n",
              "      <td>1.038416</td>\n",
              "      <td>1.039063</td>\n",
              "      <td>1.037324</td>\n",
              "      <td>1.036100</td>\n",
              "      <td>1.032403</td>\n",
              "      <td>1.026585</td>\n",
              "      <td>1.020585</td>\n",
              "      <td>1.004983</td>\n",
              "      <td>0.981338</td>\n",
              "      <td>0.969416</td>\n",
              "      <td>0.950952</td>\n",
              "      <td>1974.000000</td>\n",
              "      <td>31.384333</td>\n",
              "      <td>16.136667</td>\n",
              "      <td>5.300000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>68.000000</td>\n",
              "      <td>0.492162</td>\n",
              "      <td>0.494457</td>\n",
              "      <td>0.498396</td>\n",
              "      <td>0.502493</td>\n",
              "      <td>0.508495</td>\n",
              "      <td>0.513840</td>\n",
              "      <td>0.520323</td>\n",
              "      <td>0.526687</td>\n",
              "      <td>0.535248</td>\n",
              "      <td>0.547601</td>\n",
              "      <td>0.561213</td>\n",
              "      <td>0.572464</td>\n",
              "      <td>0.579220</td>\n",
              "      <td>0.582228</td>\n",
              "      <td>0.583382</td>\n",
              "      <td>0.583044</td>\n",
              "      <td>0.581269</td>\n",
              "      <td>0.578919</td>\n",
              "      <td>0.576212</td>\n",
              "      <td>0.572606</td>\n",
              "      <td>0.569006</td>\n",
              "      <td>0.565879</td>\n",
              "      <td>0.562660</td>\n",
              "      <td>0.558955</td>\n",
              "      <td>0.555664</td>\n",
              "      <td>0.552372</td>\n",
              "      <td>0.549645</td>\n",
              "      <td>0.547153</td>\n",
              "      <td>0.545535</td>\n",
              "      <td>0.543872</td>\n",
              "      <td>0.543126</td>\n",
              "      <td>0.542971</td>\n",
              "      <td>0.543221</td>\n",
              "      <td>0.544237</td>\n",
              "      <td>0.545786</td>\n",
              "      <td>0.548325</td>\n",
              "      <td>0.551150</td>\n",
              "      <td>0.554563</td>\n",
              "      <td>0.558216</td>\n",
              "      <td>...</td>\n",
              "      <td>1.438055</td>\n",
              "      <td>1.427390</td>\n",
              "      <td>1.418313</td>\n",
              "      <td>1.408198</td>\n",
              "      <td>1.397676</td>\n",
              "      <td>1.386605</td>\n",
              "      <td>1.375685</td>\n",
              "      <td>1.365357</td>\n",
              "      <td>1.355396</td>\n",
              "      <td>1.347791</td>\n",
              "      <td>1.337279</td>\n",
              "      <td>1.328478</td>\n",
              "      <td>1.320503</td>\n",
              "      <td>1.312188</td>\n",
              "      <td>1.303734</td>\n",
              "      <td>1.297646</td>\n",
              "      <td>1.291290</td>\n",
              "      <td>1.283525</td>\n",
              "      <td>1.277472</td>\n",
              "      <td>1.271808</td>\n",
              "      <td>1.266912</td>\n",
              "      <td>1.263202</td>\n",
              "      <td>1.259202</td>\n",
              "      <td>1.257043</td>\n",
              "      <td>1.255547</td>\n",
              "      <td>1.254069</td>\n",
              "      <td>1.251106</td>\n",
              "      <td>1.250801</td>\n",
              "      <td>1.245398</td>\n",
              "      <td>1.236956</td>\n",
              "      <td>1.228441</td>\n",
              "      <td>1.216640</td>\n",
              "      <td>1.214438</td>\n",
              "      <td>1.219523</td>\n",
              "      <td>1.210930</td>\n",
              "      <td>4150.500000</td>\n",
              "      <td>39.420750</td>\n",
              "      <td>33.587333</td>\n",
              "      <td>81.347500</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>136.500000</td>\n",
              "      <td>0.517637</td>\n",
              "      <td>0.519890</td>\n",
              "      <td>0.524229</td>\n",
              "      <td>0.528808</td>\n",
              "      <td>0.534595</td>\n",
              "      <td>0.541708</td>\n",
              "      <td>0.547380</td>\n",
              "      <td>0.553991</td>\n",
              "      <td>0.561483</td>\n",
              "      <td>0.572200</td>\n",
              "      <td>0.585533</td>\n",
              "      <td>0.597325</td>\n",
              "      <td>0.603621</td>\n",
              "      <td>0.607155</td>\n",
              "      <td>0.608601</td>\n",
              "      <td>0.608395</td>\n",
              "      <td>0.606871</td>\n",
              "      <td>0.604356</td>\n",
              "      <td>0.601704</td>\n",
              "      <td>0.598210</td>\n",
              "      <td>0.594825</td>\n",
              "      <td>0.591652</td>\n",
              "      <td>0.588568</td>\n",
              "      <td>0.584530</td>\n",
              "      <td>0.581085</td>\n",
              "      <td>0.577706</td>\n",
              "      <td>0.575218</td>\n",
              "      <td>0.573276</td>\n",
              "      <td>0.571611</td>\n",
              "      <td>0.570549</td>\n",
              "      <td>0.569648</td>\n",
              "      <td>0.569021</td>\n",
              "      <td>0.568656</td>\n",
              "      <td>0.569177</td>\n",
              "      <td>0.570510</td>\n",
              "      <td>0.572768</td>\n",
              "      <td>0.575309</td>\n",
              "      <td>0.578820</td>\n",
              "      <td>0.582439</td>\n",
              "      <td>...</td>\n",
              "      <td>1.509993</td>\n",
              "      <td>1.496886</td>\n",
              "      <td>1.484712</td>\n",
              "      <td>1.472726</td>\n",
              "      <td>1.459901</td>\n",
              "      <td>1.447849</td>\n",
              "      <td>1.434799</td>\n",
              "      <td>1.422255</td>\n",
              "      <td>1.411197</td>\n",
              "      <td>1.402132</td>\n",
              "      <td>1.389828</td>\n",
              "      <td>1.380030</td>\n",
              "      <td>1.371024</td>\n",
              "      <td>1.361727</td>\n",
              "      <td>1.351912</td>\n",
              "      <td>1.344241</td>\n",
              "      <td>1.337637</td>\n",
              "      <td>1.330476</td>\n",
              "      <td>1.324922</td>\n",
              "      <td>1.318474</td>\n",
              "      <td>1.313972</td>\n",
              "      <td>1.309751</td>\n",
              "      <td>1.306991</td>\n",
              "      <td>1.304351</td>\n",
              "      <td>1.303537</td>\n",
              "      <td>1.303173</td>\n",
              "      <td>1.302108</td>\n",
              "      <td>1.300589</td>\n",
              "      <td>1.296182</td>\n",
              "      <td>1.291905</td>\n",
              "      <td>1.282205</td>\n",
              "      <td>1.273889</td>\n",
              "      <td>1.269692</td>\n",
              "      <td>1.286520</td>\n",
              "      <td>1.271123</td>\n",
              "      <td>7086.500000</td>\n",
              "      <td>41.806083</td>\n",
              "      <td>38.467583</td>\n",
              "      <td>109.850000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>273.750000</td>\n",
              "      <td>0.545690</td>\n",
              "      <td>0.547908</td>\n",
              "      <td>0.552079</td>\n",
              "      <td>0.557400</td>\n",
              "      <td>0.564152</td>\n",
              "      <td>0.571383</td>\n",
              "      <td>0.577585</td>\n",
              "      <td>0.582518</td>\n",
              "      <td>0.590188</td>\n",
              "      <td>0.600986</td>\n",
              "      <td>0.614931</td>\n",
              "      <td>0.626134</td>\n",
              "      <td>0.632340</td>\n",
              "      <td>0.635539</td>\n",
              "      <td>0.636588</td>\n",
              "      <td>0.635807</td>\n",
              "      <td>0.634326</td>\n",
              "      <td>0.632133</td>\n",
              "      <td>0.629180</td>\n",
              "      <td>0.625997</td>\n",
              "      <td>0.623065</td>\n",
              "      <td>0.619732</td>\n",
              "      <td>0.616301</td>\n",
              "      <td>0.611912</td>\n",
              "      <td>0.609308</td>\n",
              "      <td>0.606632</td>\n",
              "      <td>0.604324</td>\n",
              "      <td>0.602069</td>\n",
              "      <td>0.600773</td>\n",
              "      <td>0.599572</td>\n",
              "      <td>0.598980</td>\n",
              "      <td>0.599007</td>\n",
              "      <td>0.599201</td>\n",
              "      <td>0.600384</td>\n",
              "      <td>0.602131</td>\n",
              "      <td>0.604517</td>\n",
              "      <td>0.607264</td>\n",
              "      <td>0.610573</td>\n",
              "      <td>0.614266</td>\n",
              "      <td>...</td>\n",
              "      <td>1.559759</td>\n",
              "      <td>1.547204</td>\n",
              "      <td>1.533789</td>\n",
              "      <td>1.521686</td>\n",
              "      <td>1.508628</td>\n",
              "      <td>1.496775</td>\n",
              "      <td>1.483442</td>\n",
              "      <td>1.472454</td>\n",
              "      <td>1.460157</td>\n",
              "      <td>1.452229</td>\n",
              "      <td>1.438763</td>\n",
              "      <td>1.429058</td>\n",
              "      <td>1.419599</td>\n",
              "      <td>1.409979</td>\n",
              "      <td>1.400365</td>\n",
              "      <td>1.392393</td>\n",
              "      <td>1.383955</td>\n",
              "      <td>1.376497</td>\n",
              "      <td>1.369636</td>\n",
              "      <td>1.363443</td>\n",
              "      <td>1.357990</td>\n",
              "      <td>1.353691</td>\n",
              "      <td>1.350482</td>\n",
              "      <td>1.348698</td>\n",
              "      <td>1.347153</td>\n",
              "      <td>1.347032</td>\n",
              "      <td>1.346518</td>\n",
              "      <td>1.345891</td>\n",
              "      <td>1.342652</td>\n",
              "      <td>1.341573</td>\n",
              "      <td>1.338421</td>\n",
              "      <td>1.332192</td>\n",
              "      <td>1.332948</td>\n",
              "      <td>1.351667</td>\n",
              "      <td>1.339838</td>\n",
              "      <td>9618.500000</td>\n",
              "      <td>44.381750</td>\n",
              "      <td>45.301333</td>\n",
              "      <td>136.480000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>409.000000</td>\n",
              "      <td>0.696677</td>\n",
              "      <td>0.696001</td>\n",
              "      <td>0.697815</td>\n",
              "      <td>0.700696</td>\n",
              "      <td>0.704963</td>\n",
              "      <td>0.709175</td>\n",
              "      <td>0.716203</td>\n",
              "      <td>0.722194</td>\n",
              "      <td>0.731920</td>\n",
              "      <td>0.744236</td>\n",
              "      <td>0.757309</td>\n",
              "      <td>0.771393</td>\n",
              "      <td>0.778315</td>\n",
              "      <td>0.782179</td>\n",
              "      <td>0.782835</td>\n",
              "      <td>0.782078</td>\n",
              "      <td>0.779667</td>\n",
              "      <td>0.776754</td>\n",
              "      <td>0.774371</td>\n",
              "      <td>0.771715</td>\n",
              "      <td>0.769261</td>\n",
              "      <td>0.766669</td>\n",
              "      <td>0.763244</td>\n",
              "      <td>0.759328</td>\n",
              "      <td>0.756597</td>\n",
              "      <td>0.752855</td>\n",
              "      <td>0.750512</td>\n",
              "      <td>0.748430</td>\n",
              "      <td>0.746127</td>\n",
              "      <td>0.743960</td>\n",
              "      <td>0.742321</td>\n",
              "      <td>0.741800</td>\n",
              "      <td>0.741419</td>\n",
              "      <td>0.741988</td>\n",
              "      <td>0.743058</td>\n",
              "      <td>0.745721</td>\n",
              "      <td>0.748387</td>\n",
              "      <td>0.751435</td>\n",
              "      <td>0.755251</td>\n",
              "      <td>...</td>\n",
              "      <td>1.752160</td>\n",
              "      <td>1.742859</td>\n",
              "      <td>1.729738</td>\n",
              "      <td>1.718851</td>\n",
              "      <td>1.703801</td>\n",
              "      <td>1.691461</td>\n",
              "      <td>1.676763</td>\n",
              "      <td>1.665650</td>\n",
              "      <td>1.656921</td>\n",
              "      <td>1.639813</td>\n",
              "      <td>1.628050</td>\n",
              "      <td>1.619605</td>\n",
              "      <td>1.606320</td>\n",
              "      <td>1.599001</td>\n",
              "      <td>1.585844</td>\n",
              "      <td>1.576560</td>\n",
              "      <td>1.566763</td>\n",
              "      <td>1.559222</td>\n",
              "      <td>1.548711</td>\n",
              "      <td>1.540025</td>\n",
              "      <td>1.533886</td>\n",
              "      <td>1.530444</td>\n",
              "      <td>1.530373</td>\n",
              "      <td>1.528527</td>\n",
              "      <td>1.529409</td>\n",
              "      <td>1.531209</td>\n",
              "      <td>1.535637</td>\n",
              "      <td>1.539162</td>\n",
              "      <td>1.539344</td>\n",
              "      <td>1.541236</td>\n",
              "      <td>1.543308</td>\n",
              "      <td>1.550344</td>\n",
              "      <td>1.552939</td>\n",
              "      <td>1.599580</td>\n",
              "      <td>1.575339</td>\n",
              "      <td>11966.000000</td>\n",
              "      <td>52.678500</td>\n",
              "      <td>65.659833</td>\n",
              "      <td>310.800000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows × 176 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       Unnamed: 0  absorbance0  ...  cholesterol_ldl_value  cholesterol_ldl_human\n",
              "count  542.000000   542.000000  ...             542.000000             542.000000\n",
              "mean   170.702952     0.520451  ...             111.436592               1.485240\n",
              "std    120.507914     0.040351  ...              44.306810               1.031287\n",
              "min      0.000000     0.400606  ...               5.300000               0.000000\n",
              "25%     68.000000     0.492162  ...              81.347500               1.000000\n",
              "50%    136.500000     0.517637  ...             109.850000               1.000000\n",
              "75%    273.750000     0.545690  ...             136.480000               3.000000\n",
              "max    409.000000     0.696677  ...             310.800000               3.000000\n",
              "\n",
              "[8 rows x 176 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from keras.layers import InputLayer, Dense, LSTM, Input, Dropout,Embedding, Flatten,LayerNormalization\n",
        "from keras.models import Sequential, Model\n",
        "\n",
        "import keras.optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.backend import clear_session\n",
        "from tensorflow.keras.losses import mean_absolute_percentage_error, huber,kld,mse\n",
        "\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n"
      ],
      "metadata": {
        "id": "YTJL576ZmB3U"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_columns=df_agg.columns[1:-5]\n",
        "y_columns=df_agg.columns[-1] # the category is at the end of columns\n",
        "X_=df_agg[X_columns]\n",
        "y_=df_agg[y_columns]"
      ],
      "metadata": {
        "id": "tyAOeKvNmIFk"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FP82OUkI-Ban",
        "outputId": "4216a606-7227-490b-f102-d5022625302c"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Index(['absorbance0', 'absorbance1', 'absorbance2', 'absorbance3',\n",
              "       'absorbance4', 'absorbance5', 'absorbance6', 'absorbance7',\n",
              "       'absorbance8', 'absorbance9',\n",
              "       ...\n",
              "       'absorbance160', 'absorbance161', 'absorbance162', 'absorbance163',\n",
              "       'absorbance164', 'absorbance165', 'absorbance166', 'absorbance167',\n",
              "       'absorbance168', 'absorbance169'],\n",
              "      dtype='object', length=170)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_.replace(3,2, inplace=True) # the generated file contains  0,1,3 as the category label :-( "
      ],
      "metadata": {
        "id": "x-ZQDORU8dSB"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#y_.replace(2,0, inplace=True)"
      ],
      "metadata": {
        "id": "ktI0xxn_8_1c"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_=np.array(y_)"
      ],
      "metadata": {
        "id": "T1yh_hqpqEt4"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_=y_.reshape(-1,1)"
      ],
      "metadata": {
        "id": "Mstvf1lXqSEe"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_columns"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "ugaWW0KOq90O",
        "outputId": "e02e7840-71ca-4d01-f309-fbdb57993a09"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'cholesterol_ldl_human'"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "X_columns=df_agg.columns[1:-5]\n",
        "y_columns=df_agg.columns[-1] # the category is at the end of columns\n",
        "X_=df_agg[X_columns]\n",
        "y_=df_agg[y_columns]\n",
        "\n",
        "y_.replace(3,2, inplace=True) # the generated file contains  0,1,3 as the category label :-( \n",
        "\n",
        "y_=np.array(y_)\n",
        "y_=y_.reshape(-1,1)\n",
        "\n",
        "\n",
        "\n",
        "__X_SCALER__=\"normal\"\n",
        "\n",
        "__DNN_MODE__=\"regression\"\n",
        "\n",
        "__Y_SCALER__=\"normal\"\n",
        "\n",
        "if __DNN_MODE__==\"regression\":\n",
        "    y_columns=df_agg.columns[-2] # the category is at the  -2  column\n",
        "    y_=df_agg[y_columns]\n",
        "    y_=np.array(y_)\n",
        "    y_=y_.reshape(-1,1)\n",
        "\n",
        "\n",
        "elif __DNN_MODE__==\"classification\":\n",
        "    y_columns=df_agg.columns[-1] # the category is at the end of columns\n",
        "    y_=df_agg[y_columns]\n",
        "    y_.replace(3,2, inplace=True) # the generated file contains  0,1,3 as the category label :-( \n",
        "    y_=np.array(y_)\n",
        "    y_=y_.reshape(-1,1)\n",
        "\n",
        "y=y_\n",
        "print(y[0:10])\n",
        "\n",
        "if __X_SCALER__==\"normal\":\n",
        "    X_normalizer = StandardScaler() \n",
        "    X=X_normalizer.fit_transform(X_)\n",
        "\n",
        "if __Y_SCALER__==\"normal\":\n",
        "    y_normalizer = StandardScaler() \n",
        "    y=y_normalizer.fit_transform(y_,)\n",
        "    print(f\"Itt: {y}\")\n",
        "\n",
        "if __X_SCALER__==\"minmax\":\n",
        "    X_minmax = MinMaxScaler() \n",
        "    X=X_minmax.fit_transform(X_)\n",
        "\n",
        "if __Y_SCALER__==\"minmax\":\n",
        "    y_minmax = MinMaxScaler() \n",
        "    y=y_minmax.fit_transform(y_)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "BQ__bM5BpIaE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3f52d41d-5e2a-4ca7-8834-5f81ef1a35f6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[105.6]\n",
            " [ 72.4]\n",
            " [ 68.4]\n",
            " [106. ]\n",
            " [ 31.2]\n",
            " [ 40.6]\n",
            " [ 82.6]\n",
            " [ 32.7]\n",
            " [ 64.2]\n",
            " [151. ]]\n",
            "Itt: [[-0.13185294]\n",
            " [-0.88186563]\n",
            " [-0.97222861]\n",
            " [-0.12281665]\n",
            " [-1.81260428]\n",
            " [-1.60025128]\n",
            " [-0.65144005]\n",
            " [-1.77871816]\n",
            " [-1.06710973]\n",
            " [ 0.89376682]\n",
            " [-0.37583298]\n",
            " [-0.74180302]\n",
            " [-0.01438108]\n",
            " [ 0.83051274]\n",
            " [-0.65144005]\n",
            " [ 0.42387935]\n",
            " [ 2.26276589]\n",
            " [-1.33819866]\n",
            " [-0.45715965]\n",
            " [-1.66802352]\n",
            " [ 0.22959896]\n",
            " [ 0.0669456 ]\n",
            " [ 0.07146375]\n",
            " [ 0.15279043]\n",
            " [-0.75083932]\n",
            " [-0.19510703]\n",
            " [ 1.73866064]\n",
            " [ 0.595569  ]\n",
            " [ 0.50972418]\n",
            " [ 1.21907353]\n",
            " [-0.36679668]\n",
            " [-0.97674676]\n",
            " [ 1.04286573]\n",
            " [ 1.04286573]\n",
            " [-0.84572044]\n",
            " [-0.72824858]\n",
            " [ 0.81244014]\n",
            " [ 0.17086302]\n",
            " [ 0.93443016]\n",
            " [ 0.3524926 ]\n",
            " [-0.3035426 ]\n",
            " [-1.50175564]\n",
            " [-1.09015229]\n",
            " [-0.10881038]\n",
            " [-0.58411963]\n",
            " [-0.03064641]\n",
            " [-0.5619807 ]\n",
            " [-0.4494788 ]\n",
            " [ 0.24496066]\n",
            " [-1.44663423]\n",
            " [ 0.13291057]\n",
            " [-0.19962517]\n",
            " [-0.01618834]\n",
            " [-1.03051273]\n",
            " [-2.13700736]\n",
            " [-0.14540739]\n",
            " [ 0.07959642]\n",
            " [ 0.6927092 ]\n",
            " [-0.86831119]\n",
            " [-0.59722226]\n",
            " [ 0.60234623]\n",
            " [ 0.69722735]\n",
            " [ 0.22688807]\n",
            " [ 0.77629495]\n",
            " [-0.83713596]\n",
            " [ 1.62118877]\n",
            " [ 0.02176411]\n",
            " [-0.37989931]\n",
            " [-1.41003722]\n",
            " [-0.1060995 ]\n",
            " [ 0.76364414]\n",
            " [ 0.27929859]\n",
            " [ 0.62380743]\n",
            " [-0.98126491]\n",
            " [-0.06633979]\n",
            " [-0.06046619]\n",
            " [-0.1182985 ]\n",
            " [-0.43908706]\n",
            " [-0.3035426 ]\n",
            " [ 0.9872925 ]\n",
            " [-0.00986293]\n",
            " [-1.12765293]\n",
            " [ 0.6226779 ]\n",
            " [ 1.18744649]\n",
            " [ 0.11754887]\n",
            " [-0.43185802]\n",
            " [ 0.0579093 ]\n",
            " [ 0.3280946 ]\n",
            " [-1.62781199]\n",
            " [ 0.61228615]\n",
            " [ 1.11244522]\n",
            " [ 1.70567815]\n",
            " [-0.08847872]\n",
            " [ 0.17041121]\n",
            " [-0.44812336]\n",
            " [ 0.67463661]\n",
            " [ 0.07417464]\n",
            " [-0.18200439]\n",
            " [-1.58217869]\n",
            " [ 0.25580422]\n",
            " [ 0.23321348]\n",
            " [-1.11861663]\n",
            " [ 0.03712582]\n",
            " [-0.50505203]\n",
            " [-0.29179541]\n",
            " [ 0.03983671]\n",
            " [ 1.70838904]\n",
            " [ 1.04241392]\n",
            " [ 0.10309079]\n",
            " [-0.1002259 ]\n",
            " [-0.93608342]\n",
            " [-1.07162788]\n",
            " [ 0.93894831]\n",
            " [ 0.23411711]\n",
            " [ 1.10973433]\n",
            " [-0.4040714 ]\n",
            " [-0.71017598]\n",
            " [-1.50808105]\n",
            " [ 0.26348507]\n",
            " [ 0.4193612 ]\n",
            " [ 0.91635757]\n",
            " [-0.84572044]\n",
            " [-0.43230983]\n",
            " [-0.39390557]\n",
            " [-1.78097723]\n",
            " [-0.93608342]\n",
            " [ 0.91635757]\n",
            " [ 1.88775955]\n",
            " [-1.34723496]\n",
            " [-1.77194094]\n",
            " [-2.39770454]\n",
            " [-2.39770454]\n",
            " [-1.24331753]\n",
            " [-1.79679075]\n",
            " [ 0.42387935]\n",
            " [-0.76891192]\n",
            " [ 0.38321601]\n",
            " [ 0.45098824]\n",
            " [ 1.00220239]\n",
            " [ 2.39831035]\n",
            " [ 0.17538117]\n",
            " [ 0.53231492]\n",
            " [ 0.0850182 ]\n",
            " [-0.1182985 ]\n",
            " [ 0.56846011]\n",
            " [ 0.39677046]\n",
            " [ 0.20249006]\n",
            " [ 0.32899823]\n",
            " [-1.49633386]\n",
            " [-0.33516964]\n",
            " [ 1.22359168]\n",
            " [ 2.25824774]\n",
            " [ 0.01724597]\n",
            " [-0.92252897]\n",
            " [ 0.33803453]\n",
            " [-0.98578306]\n",
            " [-1.12132752]\n",
            " [-0.17703443]\n",
            " [ 0.84858533]\n",
            " [ 0.7988857 ]\n",
            " [-0.02793552]\n",
            " [ 0.80340385]\n",
            " [ 0.27929859]\n",
            " [-0.32161519]\n",
            " [ 0.42387935]\n",
            " [ 0.10760894]\n",
            " [-0.85927489]\n",
            " [-1.32916236]\n",
            " [ 1.20100094]\n",
            " [-0.38486927]\n",
            " [-0.02341737]\n",
            " [-0.89090193]\n",
            " [-0.08667146]\n",
            " [-0.81861155]\n",
            " [ 1.66637026]\n",
            " [-1.41952533]\n",
            " [ 0.22508081]\n",
            " [ 2.00974956]\n",
            " [-0.04148997]\n",
            " [-0.78246636]\n",
            " [ 0.14375413]\n",
            " [-0.06408071]\n",
            " [-1.63187833]\n",
            " [-0.26739741]\n",
            " [-0.90445638]\n",
            " [-0.54752263]\n",
            " [-0.94511972]\n",
            " [ 0.7808131 ]\n",
            " [-0.88638378]\n",
            " [ 2.00974956]\n",
            " [-0.85784038]\n",
            " [-0.03697182]\n",
            " [-0.08215331]\n",
            " [-0.51047381]\n",
            " [-0.16799813]\n",
            " [ 0.7988857 ]\n",
            " [-1.44211608]\n",
            " [-0.23125222]\n",
            " [-1.46922497]\n",
            " [-0.71921228]\n",
            " [-0.36227853]\n",
            " [ 0.21604451]\n",
            " [ 0.03983671]\n",
            " [-1.06259158]\n",
            " [-0.03471275]\n",
            " [ 0.96153905]\n",
            " [ 1.6252551 ]\n",
            " [-0.4070082 ]\n",
            " [-1.21259412]\n",
            " [-1.07704966]\n",
            " [ 0.53231492]\n",
            " [ 0.38999324]\n",
            " [-0.55655892]\n",
            " [ 1.59227262]\n",
            " [ 0.80250022]\n",
            " [ 0.58653271]\n",
            " [-0.37131483]\n",
            " [-0.55204078]\n",
            " [ 0.17538117]\n",
            " [-0.84572044]\n",
            " [ 0.32651325]\n",
            " [-0.4616778 ]\n",
            " [ 0.55761656]\n",
            " [ 0.0579093 ]\n",
            " [-0.81273796]\n",
            " [-1.4732913 ]\n",
            " [-0.84572044]\n",
            " [-0.59722226]\n",
            " [-0.50685929]\n",
            " [-0.34872408]\n",
            " [-0.05504441]\n",
            " [-0.05504441]\n",
            " [-0.93608342]\n",
            " [ 0.19345377]\n",
            " [-0.98126491]\n",
            " [-0.8231297 ]\n",
            " [-0.32613334]\n",
            " [ 0.35158897]\n",
            " [-1.04903714]\n",
            " [ 0.42387935]\n",
            " [-1.81983331]\n",
            " [-1.88376512]\n",
            " [-0.21317962]\n",
            " [ 0.37417972]\n",
            " [ 1.10521618]\n",
            " [-0.37086301]\n",
            " [ 0.46002454]\n",
            " [ 0.14714274]\n",
            " [ 0.08050005]\n",
            " [-1.99784837]\n",
            " [ 0.80340385]\n",
            " [-0.37131483]\n",
            " [-1.36530755]\n",
            " [-0.77794821]\n",
            " [-1.07162788]\n",
            " [-1.35491581]\n",
            " [-1.25687198]\n",
            " [-0.48381673]\n",
            " [-0.84572044]\n",
            " [-0.20278788]\n",
            " [ 0.01272782]\n",
            " [-0.89090193]\n",
            " [ 0.31228108]\n",
            " [-0.30580167]\n",
            " [-1.20084694]\n",
            " [ 0.87117608]\n",
            " [-0.55204078]\n",
            " [-0.19058888]\n",
            " [ 0.37192064]\n",
            " [ 1.25521872]\n",
            " [-0.19058888]\n",
            " [-0.07763516]\n",
            " [-1.63639647]\n",
            " [-0.68193755]\n",
            " [-0.75535747]\n",
            " [ 0.0579093 ]\n",
            " [-0.6288493 ]\n",
            " [ 0.03531856]\n",
            " [ 0.53231492]\n",
            " [ 0.49165158]\n",
            " [ 0.54858026]\n",
            " [ 0.24179796]\n",
            " [ 0.31318471]\n",
            " [-0.62659023]\n",
            " [-1.77194094]\n",
            " [-0.43908706]\n",
            " [ 0.31228108]\n",
            " [-0.53622726]\n",
            " [-0.67312716]\n",
            " [ 1.142265  ]\n",
            " [-0.43908706]\n",
            " [ 0.19345377]\n",
            " [-0.19058888]\n",
            " [ 0.06784923]\n",
            " [-0.42959895]\n",
            " [ 1.58052543]\n",
            " [-0.03245367]\n",
            " [ 0.31951012]\n",
            " [-1.13940011]\n",
            " [-0.81906337]\n",
            " [ 0.18396565]\n",
            " [ 0.83503089]\n",
            " [-0.81861155]\n",
            " [-0.34194686]\n",
            " [-0.21317962]\n",
            " [-1.11680937]\n",
            " [-0.37989931]\n",
            " [-0.51137744]\n",
            " [-0.74361028]\n",
            " [ 0.10309079]\n",
            " [ 0.91635757]\n",
            " [-0.1002259 ]\n",
            " [-0.64014468]\n",
            " [ 0.32899823]\n",
            " [-1.81712242]\n",
            " [-0.48426855]\n",
            " [-0.39390557]\n",
            " [ 0.91635757]\n",
            " [-1.63639647]\n",
            " [-0.64240375]\n",
            " [-0.57463152]\n",
            " [ 0.06491243]\n",
            " [-0.29224722]\n",
            " [-0.81861155]\n",
            " [-0.3704112 ]\n",
            " [ 0.50972418]\n",
            " [ 0.19345377]\n",
            " [ 0.75686691]\n",
            " [-1.13104154]\n",
            " [ 1.28707167]\n",
            " [-0.88548015]\n",
            " [-0.8231297 ]\n",
            " [-1.00611472]\n",
            " [-0.77794821]\n",
            " [ 0.58879178]\n",
            " [-0.12010576]\n",
            " [ 0.87117608]\n",
            " [ 0.71304087]\n",
            " [ 1.43594467]\n",
            " [ 0.08050005]\n",
            " [-0.24842118]\n",
            " [-1.84423132]\n",
            " [ 1.61667062]\n",
            " [-0.13591928]\n",
            " [ 1.86110247]\n",
            " [ 3.37739319]\n",
            " [-0.01302563]\n",
            " [-0.17341991]\n",
            " [ 1.79694476]\n",
            " [ 1.27780946]\n",
            " [-0.012122  ]\n",
            " [-0.80053896]\n",
            " [ 0.60008715]\n",
            " [ 0.60008715]\n",
            " [-0.98126491]\n",
            " [-1.93007614]\n",
            " [-1.04903714]\n",
            " [-1.39015737]\n",
            " [ 0.74692699]\n",
            " [ 0.64526864]\n",
            " [ 0.35384805]\n",
            " [-0.93834249]\n",
            " [ 0.0579093 ]\n",
            " [-1.13940011]\n",
            " [-0.93020983]\n",
            " [-0.18019713]\n",
            " [ 2.97669778]\n",
            " [ 2.97669778]\n",
            " [ 0.09424313]\n",
            " [-0.10665523]\n",
            " [-0.07763516]\n",
            " [ 1.77480583]\n",
            " [ 0.66199935]\n",
            " [ 0.05745749]\n",
            " [-1.54377443]\n",
            " [-0.25836111]\n",
            " [-0.28095185]\n",
            " [ 0.53231492]\n",
            " [ 1.61441155]\n",
            " [-0.55836618]\n",
            " [ 1.77130765]\n",
            " [-0.27261474]\n",
            " [-1.45180073]\n",
            " [ 1.48306219]\n",
            " [-1.22469824]\n",
            " [ 1.07253077]\n",
            " [ 0.61832579]\n",
            " [ 1.59661343]\n",
            " [ 1.72763409]\n",
            " [ 0.57749641]\n",
            " [-0.8231297 ]\n",
            " [-0.24842118]\n",
            " [ 0.43517472]\n",
            " [ 0.868917  ]\n",
            " [-1.59121499]\n",
            " [ 0.13791668]\n",
            " [ 1.91106303]\n",
            " [-0.44730896]\n",
            " [ 0.4193612 ]\n",
            " [-0.55204078]\n",
            " [-0.8231297 ]\n",
            " [-0.15906349]\n",
            " [-0.90151393]\n",
            " [ 0.7056729 ]\n",
            " [ 0.16412082]\n",
            " [ 1.18563923]\n",
            " [-0.39413148]\n",
            " [ 1.65304172]\n",
            " [ 1.42848973]\n",
            " [ 1.7741281 ]\n",
            " [ 0.03531856]\n",
            " [-0.60580675]\n",
            " [ 1.64468314]\n",
            " [ 0.64436501]\n",
            " [ 1.26515865]\n",
            " [ 1.14316863]\n",
            " [ 0.30640749]\n",
            " [ 0.45234369]\n",
            " [-1.38789829]\n",
            " [ 0.46454269]\n",
            " [-0.17206447]\n",
            " [-0.17206447]\n",
            " [ 0.63984686]\n",
            " [ 0.95769863]\n",
            " [ 0.53231492]\n",
            " [ 0.23863525]\n",
            " [ 0.32583552]\n",
            " [ 0.61228615]\n",
            " [-1.05265166]\n",
            " [-0.16754632]\n",
            " [ 0.9841298 ]\n",
            " [ 2.08926898]\n",
            " [-0.09615957]\n",
            " [ 0.15007954]\n",
            " [-0.42846941]\n",
            " [ 0.06803899]\n",
            " [ 2.94975041]\n",
            " [-0.02138421]\n",
            " [ 2.04928336]\n",
            " [ 0.30821474]\n",
            " [ 0.61544886]\n",
            " [-0.17048311]\n",
            " [-1.20717234]\n",
            " [ 0.72320671]\n",
            " [ 0.69045013]\n",
            " [-0.45715965]\n",
            " [-0.70113968]\n",
            " [ 0.39225231]\n",
            " [-0.49398256]\n",
            " [-0.03245367]\n",
            " [ 0.52666724]\n",
            " [ 4.50376767]\n",
            " [ 2.00071326]\n",
            " [ 2.90434301]\n",
            " [ 2.3876927 ]\n",
            " [ 0.54925798]\n",
            " [ 0.12568154]\n",
            " [ 0.09473222]\n",
            " [ 0.261226  ]\n",
            " [ 1.96592352]\n",
            " [ 0.40535494]\n",
            " [ 0.21604451]\n",
            " [ 2.39147214]\n",
            " [-0.10293679]\n",
            " [-0.41649631]\n",
            " [ 0.22056266]\n",
            " [-0.55746255]\n",
            " [ 0.39631865]\n",
            " [-1.77645909]\n",
            " [-0.86379304]\n",
            " [ 0.59782808]\n",
            " [ 1.5037169 ]\n",
            " [ 0.7808131 ]\n",
            " [-0.00986293]\n",
            " [-1.27494458]\n",
            " [ 0.46454269]\n",
            " [ 2.82753448]\n",
            " [-0.77794821]\n",
            " [ 0.21604451]\n",
            " [ 0.49842881]\n",
            " [-0.807768  ]\n",
            " [-2.14242913]\n",
            " [-0.06298167]\n",
            " [-0.16347998]\n",
            " [-0.70061558]\n",
            " [ 1.2559597 ]\n",
            " [ 1.22975556]\n",
            " [-1.17102715]\n",
            " [-0.23577036]\n",
            " [-0.57463152]\n",
            " [ 0.59105086]\n",
            " [ 0.12044726]\n",
            " [-1.10099585]\n",
            " [ 0.32764278]\n",
            " [-0.58818597]\n",
            " [ 1.32073188]\n",
            " [-0.59722226]\n",
            " [-1.21846772]\n",
            " [-1.67028259]\n",
            " [-0.10665523]\n",
            " [-0.54339078]\n",
            " [ 0.74692699]\n",
            " [ 0.0759819 ]\n",
            " [ 1.63926136]\n",
            " [-1.5460335 ]\n",
            " [-1.31786699]\n",
            " [-0.86605211]\n",
            " [ 0.22282173]\n",
            " [ 2.20628903]\n",
            " [-0.36679668]\n",
            " [ 0.38321601]\n",
            " [ 0.57465224]\n",
            " [-0.81183433]\n",
            " [ 0.81922414]\n",
            " [ 0.73187703]\n",
            " [ 0.5484481 ]\n",
            " [-0.6559582 ]\n",
            " [ 0.7056729 ]\n",
            " [-0.08667146]\n",
            " [ 4.06550724]\n",
            " [-0.46800321]\n",
            " [ 0.0579093 ]\n",
            " [-0.07763516]\n",
            " [ 1.54438024]\n",
            " [-1.1845816 ]\n",
            " [ 0.10083172]\n",
            " [ 1.31710267]\n",
            " [ 0.14149506]\n",
            " [ 1.09000019]\n",
            " [ 0.2339985 ]\n",
            " [-0.33742871]\n",
            " [ 1.12825874]\n",
            " [-1.42856163]\n",
            " [-1.22298586]\n",
            " [ 0.97057535]\n",
            " [ 0.54586937]\n",
            " [-2.02495727]\n",
            " [-0.51718665]\n",
            " [ 1.09000019]\n",
            " [-0.2764337 ]\n",
            " [ 1.20355143]\n",
            " [-0.28117776]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_normalizer.get_params()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOpMpaVeFrrX",
        "outputId": "b1b8d772-e3a0-4d2f-91ca-4a7929103eaa"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'copy': True, 'with_mean': True, 'with_std': True}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C-ALi-6SrjHS",
        "outputId": "6d12c820-1582-4a36-ba71-2f1f30d70869"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.13185294],\n",
              "       [-0.88186563],\n",
              "       [-0.97222861],\n",
              "       [-0.12281665],\n",
              "       [-1.81260428],\n",
              "       [-1.60025128],\n",
              "       [-0.65144005],\n",
              "       [-1.77871816],\n",
              "       [-1.06710973],\n",
              "       [ 0.89376682],\n",
              "       [-0.37583298],\n",
              "       [-0.74180302],\n",
              "       [-0.01438108],\n",
              "       [ 0.83051274],\n",
              "       [-0.65144005],\n",
              "       [ 0.42387935],\n",
              "       [ 2.26276589],\n",
              "       [-1.33819866],\n",
              "       [-0.45715965],\n",
              "       [-1.66802352],\n",
              "       [ 0.22959896],\n",
              "       [ 0.0669456 ],\n",
              "       [ 0.07146375],\n",
              "       [ 0.15279043],\n",
              "       [-0.75083932],\n",
              "       [-0.19510703],\n",
              "       [ 1.73866064],\n",
              "       [ 0.595569  ],\n",
              "       [ 0.50972418],\n",
              "       [ 1.21907353],\n",
              "       [-0.36679668],\n",
              "       [-0.97674676],\n",
              "       [ 1.04286573],\n",
              "       [ 1.04286573],\n",
              "       [-0.84572044],\n",
              "       [-0.72824858],\n",
              "       [ 0.81244014],\n",
              "       [ 0.17086302],\n",
              "       [ 0.93443016],\n",
              "       [ 0.3524926 ],\n",
              "       [-0.3035426 ],\n",
              "       [-1.50175564],\n",
              "       [-1.09015229],\n",
              "       [-0.10881038],\n",
              "       [-0.58411963],\n",
              "       [-0.03064641],\n",
              "       [-0.5619807 ],\n",
              "       [-0.4494788 ],\n",
              "       [ 0.24496066],\n",
              "       [-1.44663423],\n",
              "       [ 0.13291057],\n",
              "       [-0.19962517],\n",
              "       [-0.01618834],\n",
              "       [-1.03051273],\n",
              "       [-2.13700736],\n",
              "       [-0.14540739],\n",
              "       [ 0.07959642],\n",
              "       [ 0.6927092 ],\n",
              "       [-0.86831119],\n",
              "       [-0.59722226],\n",
              "       [ 0.60234623],\n",
              "       [ 0.69722735],\n",
              "       [ 0.22688807],\n",
              "       [ 0.77629495],\n",
              "       [-0.83713596],\n",
              "       [ 1.62118877],\n",
              "       [ 0.02176411],\n",
              "       [-0.37989931],\n",
              "       [-1.41003722],\n",
              "       [-0.1060995 ],\n",
              "       [ 0.76364414],\n",
              "       [ 0.27929859],\n",
              "       [ 0.62380743],\n",
              "       [-0.98126491],\n",
              "       [-0.06633979],\n",
              "       [-0.06046619],\n",
              "       [-0.1182985 ],\n",
              "       [-0.43908706],\n",
              "       [-0.3035426 ],\n",
              "       [ 0.9872925 ],\n",
              "       [-0.00986293],\n",
              "       [-1.12765293],\n",
              "       [ 0.6226779 ],\n",
              "       [ 1.18744649],\n",
              "       [ 0.11754887],\n",
              "       [-0.43185802],\n",
              "       [ 0.0579093 ],\n",
              "       [ 0.3280946 ],\n",
              "       [-1.62781199],\n",
              "       [ 0.61228615],\n",
              "       [ 1.11244522],\n",
              "       [ 1.70567815],\n",
              "       [-0.08847872],\n",
              "       [ 0.17041121],\n",
              "       [-0.44812336],\n",
              "       [ 0.67463661],\n",
              "       [ 0.07417464],\n",
              "       [-0.18200439],\n",
              "       [-1.58217869],\n",
              "       [ 0.25580422],\n",
              "       [ 0.23321348],\n",
              "       [-1.11861663],\n",
              "       [ 0.03712582],\n",
              "       [-0.50505203],\n",
              "       [-0.29179541],\n",
              "       [ 0.03983671],\n",
              "       [ 1.70838904],\n",
              "       [ 1.04241392],\n",
              "       [ 0.10309079],\n",
              "       [-0.1002259 ],\n",
              "       [-0.93608342],\n",
              "       [-1.07162788],\n",
              "       [ 0.93894831],\n",
              "       [ 0.23411711],\n",
              "       [ 1.10973433],\n",
              "       [-0.4040714 ],\n",
              "       [-0.71017598],\n",
              "       [-1.50808105],\n",
              "       [ 0.26348507],\n",
              "       [ 0.4193612 ],\n",
              "       [ 0.91635757],\n",
              "       [-0.84572044],\n",
              "       [-0.43230983],\n",
              "       [-0.39390557],\n",
              "       [-1.78097723],\n",
              "       [-0.93608342],\n",
              "       [ 0.91635757],\n",
              "       [ 1.88775955],\n",
              "       [-1.34723496],\n",
              "       [-1.77194094],\n",
              "       [-2.39770454],\n",
              "       [-2.39770454],\n",
              "       [-1.24331753],\n",
              "       [-1.79679075],\n",
              "       [ 0.42387935],\n",
              "       [-0.76891192],\n",
              "       [ 0.38321601],\n",
              "       [ 0.45098824],\n",
              "       [ 1.00220239],\n",
              "       [ 2.39831035],\n",
              "       [ 0.17538117],\n",
              "       [ 0.53231492],\n",
              "       [ 0.0850182 ],\n",
              "       [-0.1182985 ],\n",
              "       [ 0.56846011],\n",
              "       [ 0.39677046],\n",
              "       [ 0.20249006],\n",
              "       [ 0.32899823],\n",
              "       [-1.49633386],\n",
              "       [-0.33516964],\n",
              "       [ 1.22359168],\n",
              "       [ 2.25824774],\n",
              "       [ 0.01724597],\n",
              "       [-0.92252897],\n",
              "       [ 0.33803453],\n",
              "       [-0.98578306],\n",
              "       [-1.12132752],\n",
              "       [-0.17703443],\n",
              "       [ 0.84858533],\n",
              "       [ 0.7988857 ],\n",
              "       [-0.02793552],\n",
              "       [ 0.80340385],\n",
              "       [ 0.27929859],\n",
              "       [-0.32161519],\n",
              "       [ 0.42387935],\n",
              "       [ 0.10760894],\n",
              "       [-0.85927489],\n",
              "       [-1.32916236],\n",
              "       [ 1.20100094],\n",
              "       [-0.38486927],\n",
              "       [-0.02341737],\n",
              "       [-0.89090193],\n",
              "       [-0.08667146],\n",
              "       [-0.81861155],\n",
              "       [ 1.66637026],\n",
              "       [-1.41952533],\n",
              "       [ 0.22508081],\n",
              "       [ 2.00974956],\n",
              "       [-0.04148997],\n",
              "       [-0.78246636],\n",
              "       [ 0.14375413],\n",
              "       [-0.06408071],\n",
              "       [-1.63187833],\n",
              "       [-0.26739741],\n",
              "       [-0.90445638],\n",
              "       [-0.54752263],\n",
              "       [-0.94511972],\n",
              "       [ 0.7808131 ],\n",
              "       [-0.88638378],\n",
              "       [ 2.00974956],\n",
              "       [-0.85784038],\n",
              "       [-0.03697182],\n",
              "       [-0.08215331],\n",
              "       [-0.51047381],\n",
              "       [-0.16799813],\n",
              "       [ 0.7988857 ],\n",
              "       [-1.44211608],\n",
              "       [-0.23125222],\n",
              "       [-1.46922497],\n",
              "       [-0.71921228],\n",
              "       [-0.36227853],\n",
              "       [ 0.21604451],\n",
              "       [ 0.03983671],\n",
              "       [-1.06259158],\n",
              "       [-0.03471275],\n",
              "       [ 0.96153905],\n",
              "       [ 1.6252551 ],\n",
              "       [-0.4070082 ],\n",
              "       [-1.21259412],\n",
              "       [-1.07704966],\n",
              "       [ 0.53231492],\n",
              "       [ 0.38999324],\n",
              "       [-0.55655892],\n",
              "       [ 1.59227262],\n",
              "       [ 0.80250022],\n",
              "       [ 0.58653271],\n",
              "       [-0.37131483],\n",
              "       [-0.55204078],\n",
              "       [ 0.17538117],\n",
              "       [-0.84572044],\n",
              "       [ 0.32651325],\n",
              "       [-0.4616778 ],\n",
              "       [ 0.55761656],\n",
              "       [ 0.0579093 ],\n",
              "       [-0.81273796],\n",
              "       [-1.4732913 ],\n",
              "       [-0.84572044],\n",
              "       [-0.59722226],\n",
              "       [-0.50685929],\n",
              "       [-0.34872408],\n",
              "       [-0.05504441],\n",
              "       [-0.05504441],\n",
              "       [-0.93608342],\n",
              "       [ 0.19345377],\n",
              "       [-0.98126491],\n",
              "       [-0.8231297 ],\n",
              "       [-0.32613334],\n",
              "       [ 0.35158897],\n",
              "       [-1.04903714],\n",
              "       [ 0.42387935],\n",
              "       [-1.81983331],\n",
              "       [-1.88376512],\n",
              "       [-0.21317962],\n",
              "       [ 0.37417972],\n",
              "       [ 1.10521618],\n",
              "       [-0.37086301],\n",
              "       [ 0.46002454],\n",
              "       [ 0.14714274],\n",
              "       [ 0.08050005],\n",
              "       [-1.99784837],\n",
              "       [ 0.80340385],\n",
              "       [-0.37131483],\n",
              "       [-1.36530755],\n",
              "       [-0.77794821],\n",
              "       [-1.07162788],\n",
              "       [-1.35491581],\n",
              "       [-1.25687198],\n",
              "       [-0.48381673],\n",
              "       [-0.84572044],\n",
              "       [-0.20278788],\n",
              "       [ 0.01272782],\n",
              "       [-0.89090193],\n",
              "       [ 0.31228108],\n",
              "       [-0.30580167],\n",
              "       [-1.20084694],\n",
              "       [ 0.87117608],\n",
              "       [-0.55204078],\n",
              "       [-0.19058888],\n",
              "       [ 0.37192064],\n",
              "       [ 1.25521872],\n",
              "       [-0.19058888],\n",
              "       [-0.07763516],\n",
              "       [-1.63639647],\n",
              "       [-0.68193755],\n",
              "       [-0.75535747],\n",
              "       [ 0.0579093 ],\n",
              "       [-0.6288493 ],\n",
              "       [ 0.03531856],\n",
              "       [ 0.53231492],\n",
              "       [ 0.49165158],\n",
              "       [ 0.54858026],\n",
              "       [ 0.24179796],\n",
              "       [ 0.31318471],\n",
              "       [-0.62659023],\n",
              "       [-1.77194094],\n",
              "       [-0.43908706],\n",
              "       [ 0.31228108],\n",
              "       [-0.53622726],\n",
              "       [-0.67312716],\n",
              "       [ 1.142265  ],\n",
              "       [-0.43908706],\n",
              "       [ 0.19345377],\n",
              "       [-0.19058888],\n",
              "       [ 0.06784923],\n",
              "       [-0.42959895],\n",
              "       [ 1.58052543],\n",
              "       [-0.03245367],\n",
              "       [ 0.31951012],\n",
              "       [-1.13940011],\n",
              "       [-0.81906337],\n",
              "       [ 0.18396565],\n",
              "       [ 0.83503089],\n",
              "       [-0.81861155],\n",
              "       [-0.34194686],\n",
              "       [-0.21317962],\n",
              "       [-1.11680937],\n",
              "       [-0.37989931],\n",
              "       [-0.51137744],\n",
              "       [-0.74361028],\n",
              "       [ 0.10309079],\n",
              "       [ 0.91635757],\n",
              "       [-0.1002259 ],\n",
              "       [-0.64014468],\n",
              "       [ 0.32899823],\n",
              "       [-1.81712242],\n",
              "       [-0.48426855],\n",
              "       [-0.39390557],\n",
              "       [ 0.91635757],\n",
              "       [-1.63639647],\n",
              "       [-0.64240375],\n",
              "       [-0.57463152],\n",
              "       [ 0.06491243],\n",
              "       [-0.29224722],\n",
              "       [-0.81861155],\n",
              "       [-0.3704112 ],\n",
              "       [ 0.50972418],\n",
              "       [ 0.19345377],\n",
              "       [ 0.75686691],\n",
              "       [-1.13104154],\n",
              "       [ 1.28707167],\n",
              "       [-0.88548015],\n",
              "       [-0.8231297 ],\n",
              "       [-1.00611472],\n",
              "       [-0.77794821],\n",
              "       [ 0.58879178],\n",
              "       [-0.12010576],\n",
              "       [ 0.87117608],\n",
              "       [ 0.71304087],\n",
              "       [ 1.43594467],\n",
              "       [ 0.08050005],\n",
              "       [-0.24842118],\n",
              "       [-1.84423132],\n",
              "       [ 1.61667062],\n",
              "       [-0.13591928],\n",
              "       [ 1.86110247],\n",
              "       [ 3.37739319],\n",
              "       [-0.01302563],\n",
              "       [-0.17341991],\n",
              "       [ 1.79694476],\n",
              "       [ 1.27780946],\n",
              "       [-0.012122  ],\n",
              "       [-0.80053896],\n",
              "       [ 0.60008715],\n",
              "       [ 0.60008715],\n",
              "       [-0.98126491],\n",
              "       [-1.93007614],\n",
              "       [-1.04903714],\n",
              "       [-1.39015737],\n",
              "       [ 0.74692699],\n",
              "       [ 0.64526864],\n",
              "       [ 0.35384805],\n",
              "       [-0.93834249],\n",
              "       [ 0.0579093 ],\n",
              "       [-1.13940011],\n",
              "       [-0.93020983],\n",
              "       [-0.18019713],\n",
              "       [ 2.97669778],\n",
              "       [ 2.97669778],\n",
              "       [ 0.09424313],\n",
              "       [-0.10665523],\n",
              "       [-0.07763516],\n",
              "       [ 1.77480583],\n",
              "       [ 0.66199935],\n",
              "       [ 0.05745749],\n",
              "       [-1.54377443],\n",
              "       [-0.25836111],\n",
              "       [-0.28095185],\n",
              "       [ 0.53231492],\n",
              "       [ 1.61441155],\n",
              "       [-0.55836618],\n",
              "       [ 1.77130765],\n",
              "       [-0.27261474],\n",
              "       [-1.45180073],\n",
              "       [ 1.48306219],\n",
              "       [-1.22469824],\n",
              "       [ 1.07253077],\n",
              "       [ 0.61832579],\n",
              "       [ 1.59661343],\n",
              "       [ 1.72763409],\n",
              "       [ 0.57749641],\n",
              "       [-0.8231297 ],\n",
              "       [-0.24842118],\n",
              "       [ 0.43517472],\n",
              "       [ 0.868917  ],\n",
              "       [-1.59121499],\n",
              "       [ 0.13791668],\n",
              "       [ 1.91106303],\n",
              "       [-0.44730896],\n",
              "       [ 0.4193612 ],\n",
              "       [-0.55204078],\n",
              "       [-0.8231297 ],\n",
              "       [-0.15906349],\n",
              "       [-0.90151393],\n",
              "       [ 0.7056729 ],\n",
              "       [ 0.16412082],\n",
              "       [ 1.18563923],\n",
              "       [-0.39413148],\n",
              "       [ 1.65304172],\n",
              "       [ 1.42848973],\n",
              "       [ 1.7741281 ],\n",
              "       [ 0.03531856],\n",
              "       [-0.60580675],\n",
              "       [ 1.64468314],\n",
              "       [ 0.64436501],\n",
              "       [ 1.26515865],\n",
              "       [ 1.14316863],\n",
              "       [ 0.30640749],\n",
              "       [ 0.45234369],\n",
              "       [-1.38789829],\n",
              "       [ 0.46454269],\n",
              "       [-0.17206447],\n",
              "       [-0.17206447],\n",
              "       [ 0.63984686],\n",
              "       [ 0.95769863],\n",
              "       [ 0.53231492],\n",
              "       [ 0.23863525],\n",
              "       [ 0.32583552],\n",
              "       [ 0.61228615],\n",
              "       [-1.05265166],\n",
              "       [-0.16754632],\n",
              "       [ 0.9841298 ],\n",
              "       [ 2.08926898],\n",
              "       [-0.09615957],\n",
              "       [ 0.15007954],\n",
              "       [-0.42846941],\n",
              "       [ 0.06803899],\n",
              "       [ 2.94975041],\n",
              "       [-0.02138421],\n",
              "       [ 2.04928336],\n",
              "       [ 0.30821474],\n",
              "       [ 0.61544886],\n",
              "       [-0.17048311],\n",
              "       [-1.20717234],\n",
              "       [ 0.72320671],\n",
              "       [ 0.69045013],\n",
              "       [-0.45715965],\n",
              "       [-0.70113968],\n",
              "       [ 0.39225231],\n",
              "       [-0.49398256],\n",
              "       [-0.03245367],\n",
              "       [ 0.52666724],\n",
              "       [ 4.50376767],\n",
              "       [ 2.00071326],\n",
              "       [ 2.90434301],\n",
              "       [ 2.3876927 ],\n",
              "       [ 0.54925798],\n",
              "       [ 0.12568154],\n",
              "       [ 0.09473222],\n",
              "       [ 0.261226  ],\n",
              "       [ 1.96592352],\n",
              "       [ 0.40535494],\n",
              "       [ 0.21604451],\n",
              "       [ 2.39147214],\n",
              "       [-0.10293679],\n",
              "       [-0.41649631],\n",
              "       [ 0.22056266],\n",
              "       [-0.55746255],\n",
              "       [ 0.39631865],\n",
              "       [-1.77645909],\n",
              "       [-0.86379304],\n",
              "       [ 0.59782808],\n",
              "       [ 1.5037169 ],\n",
              "       [ 0.7808131 ],\n",
              "       [-0.00986293],\n",
              "       [-1.27494458],\n",
              "       [ 0.46454269],\n",
              "       [ 2.82753448],\n",
              "       [-0.77794821],\n",
              "       [ 0.21604451],\n",
              "       [ 0.49842881],\n",
              "       [-0.807768  ],\n",
              "       [-2.14242913],\n",
              "       [-0.06298167],\n",
              "       [-0.16347998],\n",
              "       [-0.70061558],\n",
              "       [ 1.2559597 ],\n",
              "       [ 1.22975556],\n",
              "       [-1.17102715],\n",
              "       [-0.23577036],\n",
              "       [-0.57463152],\n",
              "       [ 0.59105086],\n",
              "       [ 0.12044726],\n",
              "       [-1.10099585],\n",
              "       [ 0.32764278],\n",
              "       [-0.58818597],\n",
              "       [ 1.32073188],\n",
              "       [-0.59722226],\n",
              "       [-1.21846772],\n",
              "       [-1.67028259],\n",
              "       [-0.10665523],\n",
              "       [-0.54339078],\n",
              "       [ 0.74692699],\n",
              "       [ 0.0759819 ],\n",
              "       [ 1.63926136],\n",
              "       [-1.5460335 ],\n",
              "       [-1.31786699],\n",
              "       [-0.86605211],\n",
              "       [ 0.22282173],\n",
              "       [ 2.20628903],\n",
              "       [-0.36679668],\n",
              "       [ 0.38321601],\n",
              "       [ 0.57465224],\n",
              "       [-0.81183433],\n",
              "       [ 0.81922414],\n",
              "       [ 0.73187703],\n",
              "       [ 0.5484481 ],\n",
              "       [-0.6559582 ],\n",
              "       [ 0.7056729 ],\n",
              "       [-0.08667146],\n",
              "       [ 4.06550724],\n",
              "       [-0.46800321],\n",
              "       [ 0.0579093 ],\n",
              "       [-0.07763516],\n",
              "       [ 1.54438024],\n",
              "       [-1.1845816 ],\n",
              "       [ 0.10083172],\n",
              "       [ 1.31710267],\n",
              "       [ 0.14149506],\n",
              "       [ 1.09000019],\n",
              "       [ 0.2339985 ],\n",
              "       [-0.33742871],\n",
              "       [ 1.12825874],\n",
              "       [-1.42856163],\n",
              "       [-1.22298586],\n",
              "       [ 0.97057535],\n",
              "       [ 0.54586937],\n",
              "       [-2.02495727],\n",
              "       [-0.51718665],\n",
              "       [ 1.09000019],\n",
              "       [-0.2764337 ],\n",
              "       [ 1.20355143],\n",
              "       [-0.28117776]])"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(config={\"hyper\": \"parameter\"})\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "39NoNiEM-dHG",
        "outputId": "06d8936e-75cd-4032-c548-8d2a38418229"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nimport wandb\\nfrom wandb.keras import WandbCallback\\n\\nwandb.init(config={\"hyper\": \"parameter\"})\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blood_model_2():\n",
        "    #clear_session()\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    input_len=170\n",
        "\n",
        "    print(input_len)\n",
        "    output_size=24\n",
        "    drop_frac0=0.0 \n",
        "    drop_frac1=0.0\n",
        "\n",
        "\n",
        "\n",
        "    input1=Input(shape=(input_len,))\n",
        "\n",
        "    #flatt=Flatten()(lstm1)\n",
        "\n",
        "    non=42\n",
        "    #initializer = tf.keras.initializers.LecunNormal()\n",
        "    #initializer=tf.keras.initializers.LecunUniform()\n",
        "    #initializer=tf.keras.initializers.HeUniform(    seed=None)\n",
        "    #initializer= tf.keras.initializers.RandomNormal(    mean=3.0, stddev=0.05, seed=None)\n",
        "\n",
        "    initializer=\"HeNormal\"\n",
        "    d1=Dense(1000,activation=\"sigmoid\",kernel_initializer=initializer)(input1)\n",
        "    d1=Dense(100,activation=\"sigmoid\",kernel_initializer=initializer)(d1)\n",
        "    #d1=Dense(100,activation=\"selu\",kernel_initializer=initializer)(d1)\n",
        "\n",
        "\n",
        "    #softmax\n",
        "    pred=Dense(1,activation=\"sigmoid\",)(d1)\n",
        "\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=input1, outputs=pred)\n",
        "\n",
        "    opt = tf.keras.optimizers.Adamax(learning_rate=0.001)\n",
        "\n",
        "\n",
        "    lossfn = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "\n",
        "    model.compile(loss=\"CategoricalCrossentropy\",\n",
        "        optimizer=opt,\n",
        "        metrics=[\"Accuracy\"])\n",
        "    return(model)"
      ],
      "metadata": {
        "id": "2f2U85aQmiaZ"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LtOGK6OIZ9Pu",
        "outputId": "29c9872b-a3a7-42b7-cec9-ccb6aad93105"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.13185294],\n",
              "       [-0.88186563],\n",
              "       [-0.97222861],\n",
              "       [-0.12281665],\n",
              "       [-1.81260428],\n",
              "       [-1.60025128],\n",
              "       [-0.65144005],\n",
              "       [-1.77871816],\n",
              "       [-1.06710973],\n",
              "       [ 0.89376682],\n",
              "       [-0.37583298],\n",
              "       [-0.74180302],\n",
              "       [-0.01438108],\n",
              "       [ 0.83051274],\n",
              "       [-0.65144005],\n",
              "       [ 0.42387935],\n",
              "       [ 2.26276589],\n",
              "       [-1.33819866],\n",
              "       [-0.45715965],\n",
              "       [-1.66802352],\n",
              "       [ 0.22959896],\n",
              "       [ 0.0669456 ],\n",
              "       [ 0.07146375],\n",
              "       [ 0.15279043],\n",
              "       [-0.75083932],\n",
              "       [-0.19510703],\n",
              "       [ 1.73866064],\n",
              "       [ 0.595569  ],\n",
              "       [ 0.50972418],\n",
              "       [ 1.21907353],\n",
              "       [-0.36679668],\n",
              "       [-0.97674676],\n",
              "       [ 1.04286573],\n",
              "       [ 1.04286573],\n",
              "       [-0.84572044],\n",
              "       [-0.72824858],\n",
              "       [ 0.81244014],\n",
              "       [ 0.17086302],\n",
              "       [ 0.93443016],\n",
              "       [ 0.3524926 ],\n",
              "       [-0.3035426 ],\n",
              "       [-1.50175564],\n",
              "       [-1.09015229],\n",
              "       [-0.10881038],\n",
              "       [-0.58411963],\n",
              "       [-0.03064641],\n",
              "       [-0.5619807 ],\n",
              "       [-0.4494788 ],\n",
              "       [ 0.24496066],\n",
              "       [-1.44663423],\n",
              "       [ 0.13291057],\n",
              "       [-0.19962517],\n",
              "       [-0.01618834],\n",
              "       [-1.03051273],\n",
              "       [-2.13700736],\n",
              "       [-0.14540739],\n",
              "       [ 0.07959642],\n",
              "       [ 0.6927092 ],\n",
              "       [-0.86831119],\n",
              "       [-0.59722226],\n",
              "       [ 0.60234623],\n",
              "       [ 0.69722735],\n",
              "       [ 0.22688807],\n",
              "       [ 0.77629495],\n",
              "       [-0.83713596],\n",
              "       [ 1.62118877],\n",
              "       [ 0.02176411],\n",
              "       [-0.37989931],\n",
              "       [-1.41003722],\n",
              "       [-0.1060995 ],\n",
              "       [ 0.76364414],\n",
              "       [ 0.27929859],\n",
              "       [ 0.62380743],\n",
              "       [-0.98126491],\n",
              "       [-0.06633979],\n",
              "       [-0.06046619],\n",
              "       [-0.1182985 ],\n",
              "       [-0.43908706],\n",
              "       [-0.3035426 ],\n",
              "       [ 0.9872925 ],\n",
              "       [-0.00986293],\n",
              "       [-1.12765293],\n",
              "       [ 0.6226779 ],\n",
              "       [ 1.18744649],\n",
              "       [ 0.11754887],\n",
              "       [-0.43185802],\n",
              "       [ 0.0579093 ],\n",
              "       [ 0.3280946 ],\n",
              "       [-1.62781199],\n",
              "       [ 0.61228615],\n",
              "       [ 1.11244522],\n",
              "       [ 1.70567815],\n",
              "       [-0.08847872],\n",
              "       [ 0.17041121],\n",
              "       [-0.44812336],\n",
              "       [ 0.67463661],\n",
              "       [ 0.07417464],\n",
              "       [-0.18200439],\n",
              "       [-1.58217869],\n",
              "       [ 0.25580422],\n",
              "       [ 0.23321348],\n",
              "       [-1.11861663],\n",
              "       [ 0.03712582],\n",
              "       [-0.50505203],\n",
              "       [-0.29179541],\n",
              "       [ 0.03983671],\n",
              "       [ 1.70838904],\n",
              "       [ 1.04241392],\n",
              "       [ 0.10309079],\n",
              "       [-0.1002259 ],\n",
              "       [-0.93608342],\n",
              "       [-1.07162788],\n",
              "       [ 0.93894831],\n",
              "       [ 0.23411711],\n",
              "       [ 1.10973433],\n",
              "       [-0.4040714 ],\n",
              "       [-0.71017598],\n",
              "       [-1.50808105],\n",
              "       [ 0.26348507],\n",
              "       [ 0.4193612 ],\n",
              "       [ 0.91635757],\n",
              "       [-0.84572044],\n",
              "       [-0.43230983],\n",
              "       [-0.39390557],\n",
              "       [-1.78097723],\n",
              "       [-0.93608342],\n",
              "       [ 0.91635757],\n",
              "       [ 1.88775955],\n",
              "       [-1.34723496],\n",
              "       [-1.77194094],\n",
              "       [-2.39770454],\n",
              "       [-2.39770454],\n",
              "       [-1.24331753],\n",
              "       [-1.79679075],\n",
              "       [ 0.42387935],\n",
              "       [-0.76891192],\n",
              "       [ 0.38321601],\n",
              "       [ 0.45098824],\n",
              "       [ 1.00220239],\n",
              "       [ 2.39831035],\n",
              "       [ 0.17538117],\n",
              "       [ 0.53231492],\n",
              "       [ 0.0850182 ],\n",
              "       [-0.1182985 ],\n",
              "       [ 0.56846011],\n",
              "       [ 0.39677046],\n",
              "       [ 0.20249006],\n",
              "       [ 0.32899823],\n",
              "       [-1.49633386],\n",
              "       [-0.33516964],\n",
              "       [ 1.22359168],\n",
              "       [ 2.25824774],\n",
              "       [ 0.01724597],\n",
              "       [-0.92252897],\n",
              "       [ 0.33803453],\n",
              "       [-0.98578306],\n",
              "       [-1.12132752],\n",
              "       [-0.17703443],\n",
              "       [ 0.84858533],\n",
              "       [ 0.7988857 ],\n",
              "       [-0.02793552],\n",
              "       [ 0.80340385],\n",
              "       [ 0.27929859],\n",
              "       [-0.32161519],\n",
              "       [ 0.42387935],\n",
              "       [ 0.10760894],\n",
              "       [-0.85927489],\n",
              "       [-1.32916236],\n",
              "       [ 1.20100094],\n",
              "       [-0.38486927],\n",
              "       [-0.02341737],\n",
              "       [-0.89090193],\n",
              "       [-0.08667146],\n",
              "       [-0.81861155],\n",
              "       [ 1.66637026],\n",
              "       [-1.41952533],\n",
              "       [ 0.22508081],\n",
              "       [ 2.00974956],\n",
              "       [-0.04148997],\n",
              "       [-0.78246636],\n",
              "       [ 0.14375413],\n",
              "       [-0.06408071],\n",
              "       [-1.63187833],\n",
              "       [-0.26739741],\n",
              "       [-0.90445638],\n",
              "       [-0.54752263],\n",
              "       [-0.94511972],\n",
              "       [ 0.7808131 ],\n",
              "       [-0.88638378],\n",
              "       [ 2.00974956],\n",
              "       [-0.85784038],\n",
              "       [-0.03697182],\n",
              "       [-0.08215331],\n",
              "       [-0.51047381],\n",
              "       [-0.16799813],\n",
              "       [ 0.7988857 ],\n",
              "       [-1.44211608],\n",
              "       [-0.23125222],\n",
              "       [-1.46922497],\n",
              "       [-0.71921228],\n",
              "       [-0.36227853],\n",
              "       [ 0.21604451],\n",
              "       [ 0.03983671],\n",
              "       [-1.06259158],\n",
              "       [-0.03471275],\n",
              "       [ 0.96153905],\n",
              "       [ 1.6252551 ],\n",
              "       [-0.4070082 ],\n",
              "       [-1.21259412],\n",
              "       [-1.07704966],\n",
              "       [ 0.53231492],\n",
              "       [ 0.38999324],\n",
              "       [-0.55655892],\n",
              "       [ 1.59227262],\n",
              "       [ 0.80250022],\n",
              "       [ 0.58653271],\n",
              "       [-0.37131483],\n",
              "       [-0.55204078],\n",
              "       [ 0.17538117],\n",
              "       [-0.84572044],\n",
              "       [ 0.32651325],\n",
              "       [-0.4616778 ],\n",
              "       [ 0.55761656],\n",
              "       [ 0.0579093 ],\n",
              "       [-0.81273796],\n",
              "       [-1.4732913 ],\n",
              "       [-0.84572044],\n",
              "       [-0.59722226],\n",
              "       [-0.50685929],\n",
              "       [-0.34872408],\n",
              "       [-0.05504441],\n",
              "       [-0.05504441],\n",
              "       [-0.93608342],\n",
              "       [ 0.19345377],\n",
              "       [-0.98126491],\n",
              "       [-0.8231297 ],\n",
              "       [-0.32613334],\n",
              "       [ 0.35158897],\n",
              "       [-1.04903714],\n",
              "       [ 0.42387935],\n",
              "       [-1.81983331],\n",
              "       [-1.88376512],\n",
              "       [-0.21317962],\n",
              "       [ 0.37417972],\n",
              "       [ 1.10521618],\n",
              "       [-0.37086301],\n",
              "       [ 0.46002454],\n",
              "       [ 0.14714274],\n",
              "       [ 0.08050005],\n",
              "       [-1.99784837],\n",
              "       [ 0.80340385],\n",
              "       [-0.37131483],\n",
              "       [-1.36530755],\n",
              "       [-0.77794821],\n",
              "       [-1.07162788],\n",
              "       [-1.35491581],\n",
              "       [-1.25687198],\n",
              "       [-0.48381673],\n",
              "       [-0.84572044],\n",
              "       [-0.20278788],\n",
              "       [ 0.01272782],\n",
              "       [-0.89090193],\n",
              "       [ 0.31228108],\n",
              "       [-0.30580167],\n",
              "       [-1.20084694],\n",
              "       [ 0.87117608],\n",
              "       [-0.55204078],\n",
              "       [-0.19058888],\n",
              "       [ 0.37192064],\n",
              "       [ 1.25521872],\n",
              "       [-0.19058888],\n",
              "       [-0.07763516],\n",
              "       [-1.63639647],\n",
              "       [-0.68193755],\n",
              "       [-0.75535747],\n",
              "       [ 0.0579093 ],\n",
              "       [-0.6288493 ],\n",
              "       [ 0.03531856],\n",
              "       [ 0.53231492],\n",
              "       [ 0.49165158],\n",
              "       [ 0.54858026],\n",
              "       [ 0.24179796],\n",
              "       [ 0.31318471],\n",
              "       [-0.62659023],\n",
              "       [-1.77194094],\n",
              "       [-0.43908706],\n",
              "       [ 0.31228108],\n",
              "       [-0.53622726],\n",
              "       [-0.67312716],\n",
              "       [ 1.142265  ],\n",
              "       [-0.43908706],\n",
              "       [ 0.19345377],\n",
              "       [-0.19058888],\n",
              "       [ 0.06784923],\n",
              "       [-0.42959895],\n",
              "       [ 1.58052543],\n",
              "       [-0.03245367],\n",
              "       [ 0.31951012],\n",
              "       [-1.13940011],\n",
              "       [-0.81906337],\n",
              "       [ 0.18396565],\n",
              "       [ 0.83503089],\n",
              "       [-0.81861155],\n",
              "       [-0.34194686],\n",
              "       [-0.21317962],\n",
              "       [-1.11680937],\n",
              "       [-0.37989931],\n",
              "       [-0.51137744],\n",
              "       [-0.74361028],\n",
              "       [ 0.10309079],\n",
              "       [ 0.91635757],\n",
              "       [-0.1002259 ],\n",
              "       [-0.64014468],\n",
              "       [ 0.32899823],\n",
              "       [-1.81712242],\n",
              "       [-0.48426855],\n",
              "       [-0.39390557],\n",
              "       [ 0.91635757],\n",
              "       [-1.63639647],\n",
              "       [-0.64240375],\n",
              "       [-0.57463152],\n",
              "       [ 0.06491243],\n",
              "       [-0.29224722],\n",
              "       [-0.81861155],\n",
              "       [-0.3704112 ],\n",
              "       [ 0.50972418],\n",
              "       [ 0.19345377],\n",
              "       [ 0.75686691],\n",
              "       [-1.13104154],\n",
              "       [ 1.28707167],\n",
              "       [-0.88548015],\n",
              "       [-0.8231297 ],\n",
              "       [-1.00611472],\n",
              "       [-0.77794821],\n",
              "       [ 0.58879178],\n",
              "       [-0.12010576],\n",
              "       [ 0.87117608],\n",
              "       [ 0.71304087],\n",
              "       [ 1.43594467],\n",
              "       [ 0.08050005],\n",
              "       [-0.24842118],\n",
              "       [-1.84423132],\n",
              "       [ 1.61667062],\n",
              "       [-0.13591928],\n",
              "       [ 1.86110247],\n",
              "       [ 3.37739319],\n",
              "       [-0.01302563],\n",
              "       [-0.17341991],\n",
              "       [ 1.79694476],\n",
              "       [ 1.27780946],\n",
              "       [-0.012122  ],\n",
              "       [-0.80053896],\n",
              "       [ 0.60008715],\n",
              "       [ 0.60008715],\n",
              "       [-0.98126491],\n",
              "       [-1.93007614],\n",
              "       [-1.04903714],\n",
              "       [-1.39015737],\n",
              "       [ 0.74692699],\n",
              "       [ 0.64526864],\n",
              "       [ 0.35384805],\n",
              "       [-0.93834249],\n",
              "       [ 0.0579093 ],\n",
              "       [-1.13940011],\n",
              "       [-0.93020983],\n",
              "       [-0.18019713],\n",
              "       [ 2.97669778],\n",
              "       [ 2.97669778],\n",
              "       [ 0.09424313],\n",
              "       [-0.10665523],\n",
              "       [-0.07763516],\n",
              "       [ 1.77480583],\n",
              "       [ 0.66199935],\n",
              "       [ 0.05745749],\n",
              "       [-1.54377443],\n",
              "       [-0.25836111],\n",
              "       [-0.28095185],\n",
              "       [ 0.53231492],\n",
              "       [ 1.61441155],\n",
              "       [-0.55836618],\n",
              "       [ 1.77130765],\n",
              "       [-0.27261474],\n",
              "       [-1.45180073],\n",
              "       [ 1.48306219],\n",
              "       [-1.22469824],\n",
              "       [ 1.07253077],\n",
              "       [ 0.61832579],\n",
              "       [ 1.59661343],\n",
              "       [ 1.72763409],\n",
              "       [ 0.57749641],\n",
              "       [-0.8231297 ],\n",
              "       [-0.24842118],\n",
              "       [ 0.43517472],\n",
              "       [ 0.868917  ],\n",
              "       [-1.59121499],\n",
              "       [ 0.13791668],\n",
              "       [ 1.91106303],\n",
              "       [-0.44730896],\n",
              "       [ 0.4193612 ],\n",
              "       [-0.55204078],\n",
              "       [-0.8231297 ],\n",
              "       [-0.15906349],\n",
              "       [-0.90151393],\n",
              "       [ 0.7056729 ],\n",
              "       [ 0.16412082],\n",
              "       [ 1.18563923],\n",
              "       [-0.39413148],\n",
              "       [ 1.65304172],\n",
              "       [ 1.42848973],\n",
              "       [ 1.7741281 ],\n",
              "       [ 0.03531856],\n",
              "       [-0.60580675],\n",
              "       [ 1.64468314],\n",
              "       [ 0.64436501],\n",
              "       [ 1.26515865],\n",
              "       [ 1.14316863],\n",
              "       [ 0.30640749],\n",
              "       [ 0.45234369],\n",
              "       [-1.38789829],\n",
              "       [ 0.46454269],\n",
              "       [-0.17206447],\n",
              "       [-0.17206447],\n",
              "       [ 0.63984686],\n",
              "       [ 0.95769863],\n",
              "       [ 0.53231492],\n",
              "       [ 0.23863525],\n",
              "       [ 0.32583552],\n",
              "       [ 0.61228615],\n",
              "       [-1.05265166],\n",
              "       [-0.16754632],\n",
              "       [ 0.9841298 ],\n",
              "       [ 2.08926898],\n",
              "       [-0.09615957],\n",
              "       [ 0.15007954],\n",
              "       [-0.42846941],\n",
              "       [ 0.06803899],\n",
              "       [ 2.94975041],\n",
              "       [-0.02138421],\n",
              "       [ 2.04928336],\n",
              "       [ 0.30821474],\n",
              "       [ 0.61544886],\n",
              "       [-0.17048311],\n",
              "       [-1.20717234],\n",
              "       [ 0.72320671],\n",
              "       [ 0.69045013],\n",
              "       [-0.45715965],\n",
              "       [-0.70113968],\n",
              "       [ 0.39225231],\n",
              "       [-0.49398256],\n",
              "       [-0.03245367],\n",
              "       [ 0.52666724],\n",
              "       [ 4.50376767],\n",
              "       [ 2.00071326],\n",
              "       [ 2.90434301],\n",
              "       [ 2.3876927 ],\n",
              "       [ 0.54925798],\n",
              "       [ 0.12568154],\n",
              "       [ 0.09473222],\n",
              "       [ 0.261226  ],\n",
              "       [ 1.96592352],\n",
              "       [ 0.40535494],\n",
              "       [ 0.21604451],\n",
              "       [ 2.39147214],\n",
              "       [-0.10293679],\n",
              "       [-0.41649631],\n",
              "       [ 0.22056266],\n",
              "       [-0.55746255],\n",
              "       [ 0.39631865],\n",
              "       [-1.77645909],\n",
              "       [-0.86379304],\n",
              "       [ 0.59782808],\n",
              "       [ 1.5037169 ],\n",
              "       [ 0.7808131 ],\n",
              "       [-0.00986293],\n",
              "       [-1.27494458],\n",
              "       [ 0.46454269],\n",
              "       [ 2.82753448],\n",
              "       [-0.77794821],\n",
              "       [ 0.21604451],\n",
              "       [ 0.49842881],\n",
              "       [-0.807768  ],\n",
              "       [-2.14242913],\n",
              "       [-0.06298167],\n",
              "       [-0.16347998],\n",
              "       [-0.70061558],\n",
              "       [ 1.2559597 ],\n",
              "       [ 1.22975556],\n",
              "       [-1.17102715],\n",
              "       [-0.23577036],\n",
              "       [-0.57463152],\n",
              "       [ 0.59105086],\n",
              "       [ 0.12044726],\n",
              "       [-1.10099585],\n",
              "       [ 0.32764278],\n",
              "       [-0.58818597],\n",
              "       [ 1.32073188],\n",
              "       [-0.59722226],\n",
              "       [-1.21846772],\n",
              "       [-1.67028259],\n",
              "       [-0.10665523],\n",
              "       [-0.54339078],\n",
              "       [ 0.74692699],\n",
              "       [ 0.0759819 ],\n",
              "       [ 1.63926136],\n",
              "       [-1.5460335 ],\n",
              "       [-1.31786699],\n",
              "       [-0.86605211],\n",
              "       [ 0.22282173],\n",
              "       [ 2.20628903],\n",
              "       [-0.36679668],\n",
              "       [ 0.38321601],\n",
              "       [ 0.57465224],\n",
              "       [-0.81183433],\n",
              "       [ 0.81922414],\n",
              "       [ 0.73187703],\n",
              "       [ 0.5484481 ],\n",
              "       [-0.6559582 ],\n",
              "       [ 0.7056729 ],\n",
              "       [-0.08667146],\n",
              "       [ 4.06550724],\n",
              "       [-0.46800321],\n",
              "       [ 0.0579093 ],\n",
              "       [-0.07763516],\n",
              "       [ 1.54438024],\n",
              "       [-1.1845816 ],\n",
              "       [ 0.10083172],\n",
              "       [ 1.31710267],\n",
              "       [ 0.14149506],\n",
              "       [ 1.09000019],\n",
              "       [ 0.2339985 ],\n",
              "       [-0.33742871],\n",
              "       [ 1.12825874],\n",
              "       [-1.42856163],\n",
              "       [-1.22298586],\n",
              "       [ 0.97057535],\n",
              "       [ 0.54586937],\n",
              "       [-2.02495727],\n",
              "       [-0.51718665],\n",
              "       [ 1.09000019],\n",
              "       [-0.2764337 ],\n",
              "       [ 1.20355143],\n",
              "       [-0.28117776]])"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "!wandb login"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ndb5XDN9nwsC",
        "outputId": "9360a9f7-c9b6-4c4c-cbbd-27c57a5f8c3f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wandb in /usr/local/lib/python3.7/dist-packages (0.12.7)\n",
            "Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: subprocess32>=3.5.3 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.5.4)\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.2)\n",
            "Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (0.4.0)\n",
            "Requirement already satisfied: configparser>=3.8.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.2.0)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n",
            "Requirement already satisfied: shortuuid>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.0.8)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: yaspin>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.1.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.5.1)\n",
            "Requirement already satisfied: pathtools in /usr/local/lib/python3.7/dist-packages (from wandb) (0.1.2)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: GitPython>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.1.24)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (3.10.0.2)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33msipoczlaszlo\u001b[0m (use `wandb login --relogin` to force relogin)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "\n",
        "wandb.init(project=\"Blood_Measure_Normalized\", entity=\"sipoczlaszlo\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 426,
          "referenced_widgets": [
            "8405c12ce35441d9a090bd729fad7b72",
            "656d1a42a3ae46c39884c60d5950bb73",
            "78ae1bcb951e4fb18f57ccb5317763d4",
            "382f3ad2d7554e399dce3f9a13787f94",
            "b7b5b493f043401e970fecf1a6e91b99",
            "77abdfb340ae49c8bca47fa5874929bc",
            "f94bb599dc1a44f480cb59eea492b34c",
            "b0dd560ef7bf4e5e9e2b52c621ab0541"
          ]
        },
        "id": "3Rn8vSnroH4R",
        "outputId": "52773c89-3fc0-4e84-c58e-738a5d4eb7df"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Finishing last run (ID:paaldja1) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<br/>Waiting for W&B process to finish, PID 66861... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8405c12ce35441d9a090bd729fad7b72",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "VBox(children=(Label(value=' 0.83MB of 0.83MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=1.0, max=1.0)…"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\">\n",
              "<h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>MSE</td><td>█▅▅▄▃▃▄▃▄▃▃▃▄▂▂▃▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▁▂▁</td></tr><tr><td>epoch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███</td></tr><tr><td>loss</td><td>█▆▅▄▄▃▃▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>lr</td><td>█████████████▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_MSE</td><td>█▁▂▂▂▂▂▂▂▃▆▂▃▁▁▁▂▃▁▂▂▂▂▁▂▂▂▃▂▂▃▁▃▂▂▂▂▁▁▂</td></tr><tr><td>val_loss</td><td>█▅▅▄▃▃▂▂▂▂▃▂▂▁▁▁▁▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\">\n",
              "<h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>MSE</td><td>0.82759</td></tr><tr><td>best_epoch</td><td>119</td></tr><tr><td>best_val_loss</td><td>1.32344</td></tr><tr><td>epoch</td><td>157</td></tr><tr><td>loss</td><td>0.85752</td></tr><tr><td>lr</td><td>0.005</td></tr><tr><td>val_MSE</td><td>1.4551</td></tr><tr><td>val_loss</td><td>1.48523</td></tr></table>\n",
              "</div></div>\n",
              "Synced 5 W&B file(s), 1 media file(s), 0 artifact file(s) and 1 other file(s)\n",
              "<br/>Synced <strong style=\"color:#cdcd00\">peachy-snowflake-8</strong>: <a href=\"https://wandb.ai/sipoczlaszlo/Blood_Measure_Normalized/runs/paaldja1\" target=\"_blank\">https://wandb.ai/sipoczlaszlo/Blood_Measure_Normalized/runs/paaldja1</a><br/>\n",
              "Find logs at: <code>./wandb/run-20211213_235241-paaldja1/logs</code><br/>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Successfully finished last run (ID:paaldja1). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "                    Syncing run <strong><a href=\"https://wandb.ai/sipoczlaszlo/Blood_Measure_Normalized/runs/1o95zaiz\" target=\"_blank\">rose-serenity-9</a></strong> to <a href=\"https://wandb.ai/sipoczlaszlo/Blood_Measure_Normalized\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">docs</a>).<br/>\n",
              "\n",
              "                "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<wandb.sdk.wandb_run.Run at 0x7f4a89cf3490>"
            ],
            "text/html": [
              "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src=\"https://wandb.ai/sipoczlaszlo/Blood_Measure_Normalized/runs/1o95zaiz?jupyter=true\" style=\"border:none;width:100%;height:420px;display:none;\"></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def blood_model_regression():\n",
        "    #clear_session()\n",
        "\n",
        "    keras.backend.clear_session()\n",
        "\n",
        "    input_len=170\n",
        "\n",
        "    print(input_len)\n",
        "    output_size=24\n",
        "    drop_frac0=0.05 \n",
        "    drop_frac1=0.05\n",
        "\n",
        "\n",
        "\n",
        "    input1=Input(shape=(input_len,))\n",
        "\n",
        "    #flatt=Flatten()(lstm1)\n",
        "\n",
        "    non=42\n",
        "    #initializer = tf.keras.initializers.LecunNormal()\n",
        "    #initializer=tf.keras.initializers.LecunUniform()\n",
        "    #initializer=tf.keras.initializers.HeUniform(    seed=None)\n",
        "    #initializer= tf.keras.initializers.RandomNormal(    mean=3.0, stddev=0.05, seed=None)\n",
        "\n",
        "    initializer=\"normal\"\n",
        "    d1=Dense(500,activation=\"relu\",kernel_initializer=initializer)(input1)\n",
        "    d1=Dropout(drop_frac0)(d1)\n",
        "    \n",
        "    d1=Dense(250,activation=\"tanh\",kernel_initializer=initializer)(d1)\n",
        "   \n",
        "\n",
        "    #softmax\n",
        "    pred=Dense(1, kernel_initializer=initializer)(d1)\n",
        "\n",
        "    \n",
        "    \n",
        "    model = Model(inputs=input1, outputs=pred)\n",
        "\n",
        "    opt = tf.keras.optimizers.SGD(learning_rate=0.0000001)\n",
        "\n",
        "\n",
        "    lossfn = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
        "\n",
        "    model.compile(loss=\"MSE\",\n",
        "        optimizer=opt,\n",
        "        metrics=[\"MSE\"])\n",
        "    return(model)"
      ],
      "metadata": {
        "id": "f3cQpWHeJ4IJ"
      },
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__DNN_MODE__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rsAoivPNaBOU",
        "outputId": "a2b65f92-e346-4fd9-ad01-10533af1bda5"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'regression'"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"classification\":\n",
        "    model_name=\"classification_\"\n",
        "    def scheduler(epoch, lr):\n",
        "        return 0.001\n",
        "\n",
        "    callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "    callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                            verbose=1, save_best_only=True, mode='min')]\n",
        "\n"
      ],
      "metadata": {
        "id": "DEVcX9Z5mwnz"
      },
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"regression\":\n",
        "    model_name=\"regression_\"\n",
        "    def scheduler(epoch, lr):\n",
        "        if epoch<50:\n",
        "            return 0.01\n",
        "        elif epoch <500:\n",
        "            return 0.005\n",
        "\n",
        "\n",
        "    callback_LR = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
        "\n",
        "    callbacks = [callback_LR,\n",
        "            \n",
        "            #savemodela,\n",
        "            ModelCheckpoint(filepath=model_name+\"_{loss:.5f}_{val_loss:.5f}_.hdf5\", monitor='val_loss',\n",
        "                            verbose=1, save_best_only=True, mode='min')]\n",
        "\n"
      ],
      "metadata": {
        "id": "zSCDNBu4aH9P"
      },
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"classification\":\n",
        "    new_model_2=blood_model_2()"
      ],
      "metadata": {
        "id": "yakOYRb_m07P"
      },
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"regression\":\n",
        "    new_model_2=blood_model_regression()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6IM-jEkwbG2A",
        "outputId": "23e707bb-b221-4454-cb1d-bbe5ff88fc7d"
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "170\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!rm *.hdf5"
      ],
      "metadata": {
        "id": "9-rSra7K3wVB"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Us0FJewhDKc",
        "outputId": "4ab59cb4-38b6-4265-b50e-b42b89b4cb23"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.36453647,  0.38885105,  0.39872672, ...,  1.82445842,\n",
              "         1.673298  ,  1.92809129],\n",
              "       [ 0.43579244,  0.39694511,  0.40961367, ...,  0.57719086,\n",
              "         0.45474503,  0.32195174],\n",
              "       [-0.9863988 , -0.96168426, -0.90536406, ...,  0.21397963,\n",
              "         0.08306335,  0.19148561],\n",
              "       ...,\n",
              "       [-1.07183495, -1.01721066, -1.05299893, ..., -1.18797608,\n",
              "        -1.15115283, -1.14202762],\n",
              "       [ 1.12166936,  1.0612296 ,  1.09288581, ..., -0.09962229,\n",
              "        -0.02788948, -0.05773341],\n",
              "       [-1.44973193, -1.46461933, -1.4970304 , ..., -1.13059767,\n",
              "        -1.05839207, -1.05836683]])"
            ]
          },
          "metadata": {},
          "execution_count": 110
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if __DNN_MODE__==\"regression\":\n",
        "    new_model_2.summary()\n",
        "    history=new_model_2.fit(X,\n",
        "          y,\n",
        "          epochs=1000, \n",
        "          batch_size=3,\n",
        "          validation_split=0.2,\n",
        "          verbose=1,\n",
        "          callbacks=[callbacks,WandbCallback()],\n",
        "          shuffle=True\n",
        "          \n",
        "          )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sHWVim-0vPxT",
        "outputId": "2f3dbe22-7d4b-483f-c93f-7d03d2e7f44f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 170)]             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 500)               85500     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 500)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 250)               125250    \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 251       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 211,001\n",
            "Trainable params: 211,001\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.5456 - MSE: 0.5456\n",
            "Epoch 00001: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5456 - MSE: 0.5456 - val_loss: 2.1739 - val_MSE: 2.1739 - lr: 0.0100\n",
            "Epoch 2/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.6513 - MSE: 0.6513\n",
            "Epoch 00002: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.6889 - MSE: 0.6889 - val_loss: 1.8420 - val_MSE: 1.8420 - lr: 0.0100\n",
            "Epoch 3/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.9517 - MSE: 0.9517\n",
            "Epoch 00003: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.9424 - MSE: 0.9424 - val_loss: 1.9990 - val_MSE: 1.9990 - lr: 0.0100\n",
            "Epoch 4/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.7295 - MSE: 0.7295\n",
            "Epoch 00004: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.7298 - MSE: 0.7298 - val_loss: 2.5856 - val_MSE: 2.5856 - lr: 0.0100\n",
            "Epoch 5/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.5650 - MSE: 0.5650\n",
            "Epoch 00005: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5601 - MSE: 0.5601 - val_loss: 1.9846 - val_MSE: 1.9846 - lr: 0.0100\n",
            "Epoch 6/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.5171 - MSE: 0.5171\n",
            "Epoch 00006: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5262 - MSE: 0.5262 - val_loss: 3.3709 - val_MSE: 3.3709 - lr: 0.0100\n",
            "Epoch 7/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.7274 - MSE: 0.7274\n",
            "Epoch 00007: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.7460 - MSE: 0.7460 - val_loss: 2.4333 - val_MSE: 2.4333 - lr: 0.0100\n",
            "Epoch 8/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.7016 - MSE: 0.7016\n",
            "Epoch 00008: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6940 - MSE: 0.6940 - val_loss: 1.5319 - val_MSE: 1.5319 - lr: 0.0100\n",
            "Epoch 9/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.6424 - MSE: 0.6424\n",
            "Epoch 00009: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6457 - MSE: 0.6457 - val_loss: 1.6794 - val_MSE: 1.6794 - lr: 0.0100\n",
            "Epoch 10/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.6159 - MSE: 0.6159\n",
            "Epoch 00010: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6263 - MSE: 0.6263 - val_loss: 2.0311 - val_MSE: 2.0311 - lr: 0.0100\n",
            "Epoch 11/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.6212 - MSE: 0.6212\n",
            "Epoch 00011: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6124 - MSE: 0.6124 - val_loss: 1.7206 - val_MSE: 1.7206 - lr: 0.0100\n",
            "Epoch 12/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.6141 - MSE: 0.6141\n",
            "Epoch 00012: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5899 - MSE: 0.5899 - val_loss: 1.7854 - val_MSE: 1.7854 - lr: 0.0100\n",
            "Epoch 13/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.6477 - MSE: 0.6477\n",
            "Epoch 00013: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6428 - MSE: 0.6428 - val_loss: 1.7172 - val_MSE: 1.7172 - lr: 0.0100\n",
            "Epoch 14/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.6513 - MSE: 0.6513\n",
            "Epoch 00014: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6503 - MSE: 0.6503 - val_loss: 1.8859 - val_MSE: 1.8859 - lr: 0.0100\n",
            "Epoch 15/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.6723 - MSE: 0.6723\n",
            "Epoch 00015: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6700 - MSE: 0.6700 - val_loss: 1.8188 - val_MSE: 1.8188 - lr: 0.0100\n",
            "Epoch 16/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.5573 - MSE: 0.5573\n",
            "Epoch 00016: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5559 - MSE: 0.5559 - val_loss: 1.8359 - val_MSE: 1.8359 - lr: 0.0100\n",
            "Epoch 17/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.5129 - MSE: 0.5129\n",
            "Epoch 00017: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5353 - MSE: 0.5353 - val_loss: 1.7436 - val_MSE: 1.7436 - lr: 0.0100\n",
            "Epoch 18/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.5845 - MSE: 0.5845\n",
            "Epoch 00018: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.5813 - MSE: 0.5813 - val_loss: 1.8733 - val_MSE: 1.8733 - lr: 0.0100\n",
            "Epoch 19/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.4790 - MSE: 0.4790\n",
            "Epoch 00019: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.4834 - MSE: 0.4834 - val_loss: 1.8976 - val_MSE: 1.8976 - lr: 0.0100\n",
            "Epoch 20/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.4357 - MSE: 0.4357\n",
            "Epoch 00020: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.4441 - MSE: 0.4441 - val_loss: 1.9894 - val_MSE: 1.9894 - lr: 0.0100\n",
            "Epoch 21/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.6254 - MSE: 0.6254\n",
            "Epoch 00021: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6221 - MSE: 0.6221 - val_loss: 1.8636 - val_MSE: 1.8636 - lr: 0.0100\n",
            "Epoch 22/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.7194 - MSE: 0.7194\n",
            "Epoch 00022: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.7302 - MSE: 0.7302 - val_loss: 1.8196 - val_MSE: 1.8196 - lr: 0.0100\n",
            "Epoch 23/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.6780 - MSE: 0.6780\n",
            "Epoch 00023: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6773 - MSE: 0.6773 - val_loss: 2.6273 - val_MSE: 2.6273 - lr: 0.0100\n",
            "Epoch 24/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.6668 - MSE: 0.6668\n",
            "Epoch 00024: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6635 - MSE: 0.6635 - val_loss: 2.0438 - val_MSE: 2.0438 - lr: 0.0100\n",
            "Epoch 25/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.6610 - MSE: 0.6610\n",
            "Epoch 00025: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6592 - MSE: 0.6592 - val_loss: 1.5982 - val_MSE: 1.5982 - lr: 0.0100\n",
            "Epoch 26/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.6213 - MSE: 0.6213\n",
            "Epoch 00026: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.6199 - MSE: 0.6199 - val_loss: 1.8053 - val_MSE: 1.8053 - lr: 0.0100\n",
            "Epoch 27/1000\n",
            "131/145 [==========================>...] - ETA: 0s - loss: 0.5532 - MSE: 0.5532\n",
            "Epoch 00027: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5459 - MSE: 0.5459 - val_loss: 1.7771 - val_MSE: 1.7771 - lr: 0.0100\n",
            "Epoch 28/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.5587 - MSE: 0.5587\n",
            "Epoch 00028: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.5587 - MSE: 0.5587 - val_loss: 2.9071 - val_MSE: 2.9071 - lr: 0.0100\n",
            "Epoch 29/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.5842 - MSE: 0.5842\n",
            "Epoch 00029: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5842 - MSE: 0.5842 - val_loss: 1.9583 - val_MSE: 1.9583 - lr: 0.0100\n",
            "Epoch 30/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.5244 - MSE: 0.5244\n",
            "Epoch 00030: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.5166 - MSE: 0.5166 - val_loss: 1.6805 - val_MSE: 1.6805 - lr: 0.0100\n",
            "Epoch 31/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.5268 - MSE: 0.5268\n",
            "Epoch 00031: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5268 - MSE: 0.5268 - val_loss: 1.9587 - val_MSE: 1.9587 - lr: 0.0100\n",
            "Epoch 32/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.5598 - MSE: 0.5598\n",
            "Epoch 00032: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5635 - MSE: 0.5635 - val_loss: 2.5366 - val_MSE: 2.5366 - lr: 0.0100\n",
            "Epoch 33/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.6358 - MSE: 0.6358\n",
            "Epoch 00033: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.6263 - MSE: 0.6263 - val_loss: 1.9716 - val_MSE: 1.9716 - lr: 0.0100\n",
            "Epoch 34/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.5663 - MSE: 0.5663\n",
            "Epoch 00034: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5674 - MSE: 0.5674 - val_loss: 2.1016 - val_MSE: 2.1016 - lr: 0.0100\n",
            "Epoch 35/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.5869 - MSE: 0.5869\n",
            "Epoch 00035: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5784 - MSE: 0.5784 - val_loss: 1.9699 - val_MSE: 1.9699 - lr: 0.0100\n",
            "Epoch 36/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.4988 - MSE: 0.4988\n",
            "Epoch 00036: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.5059 - MSE: 0.5059 - val_loss: 2.0430 - val_MSE: 2.0430 - lr: 0.0100\n",
            "Epoch 37/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.5217 - MSE: 0.5217\n",
            "Epoch 00037: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.5189 - MSE: 0.5189 - val_loss: 2.0698 - val_MSE: 2.0698 - lr: 0.0100\n",
            "Epoch 38/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.4671 - MSE: 0.4671\n",
            "Epoch 00038: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.4729 - MSE: 0.4729 - val_loss: 1.7252 - val_MSE: 1.7252 - lr: 0.0100\n",
            "Epoch 39/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.4224 - MSE: 0.4224\n",
            "Epoch 00039: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.4211 - MSE: 0.4211 - val_loss: 1.8262 - val_MSE: 1.8262 - lr: 0.0100\n",
            "Epoch 40/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.4221 - MSE: 0.4221\n",
            "Epoch 00040: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.4216 - MSE: 0.4216 - val_loss: 1.6622 - val_MSE: 1.6622 - lr: 0.0100\n",
            "Epoch 41/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.4532 - MSE: 0.4532\n",
            "Epoch 00041: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.4490 - MSE: 0.4490 - val_loss: 1.8513 - val_MSE: 1.8513 - lr: 0.0100\n",
            "Epoch 42/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.4200 - MSE: 0.4200\n",
            "Epoch 00042: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.4233 - MSE: 0.4233 - val_loss: 1.8636 - val_MSE: 1.8636 - lr: 0.0100\n",
            "Epoch 43/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.5797 - MSE: 0.5797\n",
            "Epoch 00043: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5834 - MSE: 0.5834 - val_loss: 3.0183 - val_MSE: 3.0183 - lr: 0.0100\n",
            "Epoch 44/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.7758 - MSE: 0.7758\n",
            "Epoch 00044: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.7705 - MSE: 0.7705 - val_loss: 1.9527 - val_MSE: 1.9527 - lr: 0.0100\n",
            "Epoch 45/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.6444 - MSE: 0.6444\n",
            "Epoch 00045: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.6315 - MSE: 0.6315 - val_loss: 2.6337 - val_MSE: 2.6337 - lr: 0.0100\n",
            "Epoch 46/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.5070 - MSE: 0.5070\n",
            "Epoch 00046: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.5132 - MSE: 0.5132 - val_loss: 1.7544 - val_MSE: 1.7544 - lr: 0.0100\n",
            "Epoch 47/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.4567 - MSE: 0.4567\n",
            "Epoch 00047: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.4714 - MSE: 0.4714 - val_loss: 1.6995 - val_MSE: 1.6995 - lr: 0.0100\n",
            "Epoch 48/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.4485 - MSE: 0.4485\n",
            "Epoch 00048: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.4514 - MSE: 0.4514 - val_loss: 1.9707 - val_MSE: 1.9707 - lr: 0.0100\n",
            "Epoch 49/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.4551 - MSE: 0.4551\n",
            "Epoch 00049: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.4505 - MSE: 0.4505 - val_loss: 1.8107 - val_MSE: 1.8107 - lr: 0.0100\n",
            "Epoch 50/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.4716 - MSE: 0.4716\n",
            "Epoch 00050: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.4716 - MSE: 0.4716 - val_loss: 2.6536 - val_MSE: 2.6536 - lr: 0.0100\n",
            "Epoch 51/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.3538 - MSE: 0.3538\n",
            "Epoch 00051: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.3597 - MSE: 0.3597 - val_loss: 1.8600 - val_MSE: 1.8600 - lr: 0.0050\n",
            "Epoch 52/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.2694 - MSE: 0.2694\n",
            "Epoch 00052: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2687 - MSE: 0.2687 - val_loss: 2.0564 - val_MSE: 2.0564 - lr: 0.0050\n",
            "Epoch 53/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.2580 - MSE: 0.2580\n",
            "Epoch 00053: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2533 - MSE: 0.2533 - val_loss: 1.9028 - val_MSE: 1.9028 - lr: 0.0050\n",
            "Epoch 54/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.2628 - MSE: 0.2628\n",
            "Epoch 00054: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2659 - MSE: 0.2659 - val_loss: 1.9282 - val_MSE: 1.9282 - lr: 0.0050\n",
            "Epoch 55/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.2467 - MSE: 0.2467\n",
            "Epoch 00055: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2463 - MSE: 0.2463 - val_loss: 1.8033 - val_MSE: 1.8033 - lr: 0.0050\n",
            "Epoch 56/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.2524 - MSE: 0.2524\n",
            "Epoch 00056: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2560 - MSE: 0.2560 - val_loss: 2.0921 - val_MSE: 2.0921 - lr: 0.0050\n",
            "Epoch 57/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.2598 - MSE: 0.2598\n",
            "Epoch 00057: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2595 - MSE: 0.2595 - val_loss: 2.0469 - val_MSE: 2.0469 - lr: 0.0050\n",
            "Epoch 58/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.2476 - MSE: 0.2476\n",
            "Epoch 00058: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2451 - MSE: 0.2451 - val_loss: 2.0465 - val_MSE: 2.0465 - lr: 0.0050\n",
            "Epoch 59/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.2482 - MSE: 0.2482\n",
            "Epoch 00059: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2484 - MSE: 0.2484 - val_loss: 2.0008 - val_MSE: 2.0008 - lr: 0.0050\n",
            "Epoch 60/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.2329 - MSE: 0.2329\n",
            "Epoch 00060: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2370 - MSE: 0.2370 - val_loss: 2.0791 - val_MSE: 2.0791 - lr: 0.0050\n",
            "Epoch 61/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.2231 - MSE: 0.2231\n",
            "Epoch 00061: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2202 - MSE: 0.2202 - val_loss: 2.0866 - val_MSE: 2.0866 - lr: 0.0050\n",
            "Epoch 62/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.2073 - MSE: 0.2073\n",
            "Epoch 00062: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2063 - MSE: 0.2063 - val_loss: 2.3267 - val_MSE: 2.3267 - lr: 0.0050\n",
            "Epoch 63/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.2158 - MSE: 0.2158\n",
            "Epoch 00063: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2106 - MSE: 0.2106 - val_loss: 2.0203 - val_MSE: 2.0203 - lr: 0.0050\n",
            "Epoch 64/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.2402 - MSE: 0.2402\n",
            "Epoch 00064: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2374 - MSE: 0.2374 - val_loss: 1.8763 - val_MSE: 1.8763 - lr: 0.0050\n",
            "Epoch 65/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.2043 - MSE: 0.2043\n",
            "Epoch 00065: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2128 - MSE: 0.2128 - val_loss: 1.9503 - val_MSE: 1.9503 - lr: 0.0050\n",
            "Epoch 66/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.2326 - MSE: 0.2326\n",
            "Epoch 00066: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2321 - MSE: 0.2321 - val_loss: 1.9169 - val_MSE: 1.9169 - lr: 0.0050\n",
            "Epoch 67/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.2306 - MSE: 0.2306\n",
            "Epoch 00067: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2304 - MSE: 0.2304 - val_loss: 2.1242 - val_MSE: 2.1242 - lr: 0.0050\n",
            "Epoch 68/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.2230 - MSE: 0.2230\n",
            "Epoch 00068: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2223 - MSE: 0.2223 - val_loss: 2.0801 - val_MSE: 2.0801 - lr: 0.0050\n",
            "Epoch 69/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.2342 - MSE: 0.2342\n",
            "Epoch 00069: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2359 - MSE: 0.2359 - val_loss: 1.8847 - val_MSE: 1.8847 - lr: 0.0050\n",
            "Epoch 70/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.1888 - MSE: 0.1888\n",
            "Epoch 00070: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2086 - MSE: 0.2086 - val_loss: 1.9575 - val_MSE: 1.9575 - lr: 0.0050\n",
            "Epoch 71/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.2030 - MSE: 0.2030\n",
            "Epoch 00071: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1998 - MSE: 0.1998 - val_loss: 2.1322 - val_MSE: 2.1322 - lr: 0.0050\n",
            "Epoch 72/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1988 - MSE: 0.1988\n",
            "Epoch 00072: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1937 - MSE: 0.1937 - val_loss: 1.9467 - val_MSE: 1.9467 - lr: 0.0050\n",
            "Epoch 73/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.2050 - MSE: 0.2050\n",
            "Epoch 00073: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2057 - MSE: 0.2057 - val_loss: 2.5088 - val_MSE: 2.5088 - lr: 0.0050\n",
            "Epoch 74/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.2260 - MSE: 0.2260\n",
            "Epoch 00074: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2300 - MSE: 0.2300 - val_loss: 2.1008 - val_MSE: 2.1008 - lr: 0.0050\n",
            "Epoch 75/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.2393 - MSE: 0.2393\n",
            "Epoch 00075: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2368 - MSE: 0.2368 - val_loss: 2.1008 - val_MSE: 2.1008 - lr: 0.0050\n",
            "Epoch 76/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.2054 - MSE: 0.2054\n",
            "Epoch 00076: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2000 - MSE: 0.2000 - val_loss: 2.1864 - val_MSE: 2.1864 - lr: 0.0050\n",
            "Epoch 77/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.2095 - MSE: 0.2095\n",
            "Epoch 00077: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2062 - MSE: 0.2062 - val_loss: 2.0892 - val_MSE: 2.0892 - lr: 0.0050\n",
            "Epoch 78/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.1916 - MSE: 0.1916\n",
            "Epoch 00078: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1911 - MSE: 0.1911 - val_loss: 2.2718 - val_MSE: 2.2718 - lr: 0.0050\n",
            "Epoch 79/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1897 - MSE: 0.1897\n",
            "Epoch 00079: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1916 - MSE: 0.1916 - val_loss: 2.1795 - val_MSE: 2.1795 - lr: 0.0050\n",
            "Epoch 80/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.2027 - MSE: 0.2027\n",
            "Epoch 00080: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1989 - MSE: 0.1989 - val_loss: 2.1864 - val_MSE: 2.1864 - lr: 0.0050\n",
            "Epoch 81/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.1982 - MSE: 0.1982\n",
            "Epoch 00081: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1929 - MSE: 0.1929 - val_loss: 1.9395 - val_MSE: 1.9395 - lr: 0.0050\n",
            "Epoch 82/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.2073 - MSE: 0.2073\n",
            "Epoch 00082: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2079 - MSE: 0.2079 - val_loss: 2.2141 - val_MSE: 2.2141 - lr: 0.0050\n",
            "Epoch 83/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1856 - MSE: 0.1856\n",
            "Epoch 00083: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1923 - MSE: 0.1923 - val_loss: 2.0241 - val_MSE: 2.0241 - lr: 0.0050\n",
            "Epoch 84/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.1997 - MSE: 0.1997\n",
            "Epoch 00084: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1997 - MSE: 0.1997 - val_loss: 2.2178 - val_MSE: 2.2178 - lr: 0.0050\n",
            "Epoch 85/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.2110 - MSE: 0.2110\n",
            "Epoch 00085: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2117 - MSE: 0.2117 - val_loss: 2.0795 - val_MSE: 2.0795 - lr: 0.0050\n",
            "Epoch 86/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1951 - MSE: 0.1951\n",
            "Epoch 00086: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1944 - MSE: 0.1944 - val_loss: 2.2472 - val_MSE: 2.2472 - lr: 0.0050\n",
            "Epoch 87/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1860 - MSE: 0.1860\n",
            "Epoch 00087: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1883 - MSE: 0.1883 - val_loss: 2.0079 - val_MSE: 2.0079 - lr: 0.0050\n",
            "Epoch 88/1000\n",
            "131/145 [==========================>...] - ETA: 0s - loss: 0.1743 - MSE: 0.1743\n",
            "Epoch 00088: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1746 - MSE: 0.1746 - val_loss: 1.8865 - val_MSE: 1.8865 - lr: 0.0050\n",
            "Epoch 89/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1851 - MSE: 0.1851\n",
            "Epoch 00089: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1850 - MSE: 0.1850 - val_loss: 2.0061 - val_MSE: 2.0061 - lr: 0.0050\n",
            "Epoch 90/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1883 - MSE: 0.1883\n",
            "Epoch 00090: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1850 - MSE: 0.1850 - val_loss: 2.2249 - val_MSE: 2.2249 - lr: 0.0050\n",
            "Epoch 91/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.2008 - MSE: 0.2008\n",
            "Epoch 00091: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2007 - MSE: 0.2007 - val_loss: 2.1206 - val_MSE: 2.1206 - lr: 0.0050\n",
            "Epoch 92/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.2100 - MSE: 0.2100\n",
            "Epoch 00092: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2141 - MSE: 0.2141 - val_loss: 2.1499 - val_MSE: 2.1499 - lr: 0.0050\n",
            "Epoch 93/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.2015 - MSE: 0.2015\n",
            "Epoch 00093: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1973 - MSE: 0.1973 - val_loss: 2.0965 - val_MSE: 2.0965 - lr: 0.0050\n",
            "Epoch 94/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.2047 - MSE: 0.2047\n",
            "Epoch 00094: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2046 - MSE: 0.2046 - val_loss: 2.1192 - val_MSE: 2.1192 - lr: 0.0050\n",
            "Epoch 95/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1788 - MSE: 0.1788\n",
            "Epoch 00095: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1873 - MSE: 0.1873 - val_loss: 2.2193 - val_MSE: 2.2193 - lr: 0.0050\n",
            "Epoch 96/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1817 - MSE: 0.1817\n",
            "Epoch 00096: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1860 - MSE: 0.1860 - val_loss: 1.9422 - val_MSE: 1.9422 - lr: 0.0050\n",
            "Epoch 97/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.1781 - MSE: 0.1781\n",
            "Epoch 00097: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1837 - MSE: 0.1837 - val_loss: 2.2549 - val_MSE: 2.2549 - lr: 0.0050\n",
            "Epoch 98/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1843 - MSE: 0.1843\n",
            "Epoch 00098: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1841 - MSE: 0.1841 - val_loss: 1.9854 - val_MSE: 1.9854 - lr: 0.0050\n",
            "Epoch 99/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1614 - MSE: 0.1614\n",
            "Epoch 00099: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1610 - MSE: 0.1610 - val_loss: 2.0928 - val_MSE: 2.0928 - lr: 0.0050\n",
            "Epoch 100/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.1874 - MSE: 0.1874\n",
            "Epoch 00100: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1850 - MSE: 0.1850 - val_loss: 2.0971 - val_MSE: 2.0971 - lr: 0.0050\n",
            "Epoch 101/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1862 - MSE: 0.1862\n",
            "Epoch 00101: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1948 - MSE: 0.1948 - val_loss: 1.9570 - val_MSE: 1.9570 - lr: 0.0050\n",
            "Epoch 102/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.1893 - MSE: 0.1893\n",
            "Epoch 00102: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1997 - MSE: 0.1997 - val_loss: 2.1048 - val_MSE: 2.1048 - lr: 0.0050\n",
            "Epoch 103/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1811 - MSE: 0.1811\n",
            "Epoch 00103: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1805 - MSE: 0.1805 - val_loss: 2.2753 - val_MSE: 2.2753 - lr: 0.0050\n",
            "Epoch 104/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1624 - MSE: 0.1624\n",
            "Epoch 00104: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1691 - MSE: 0.1691 - val_loss: 2.0474 - val_MSE: 2.0474 - lr: 0.0050\n",
            "Epoch 105/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1739 - MSE: 0.1739\n",
            "Epoch 00105: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1770 - MSE: 0.1770 - val_loss: 2.0213 - val_MSE: 2.0213 - lr: 0.0050\n",
            "Epoch 106/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1327 - MSE: 0.1327\n",
            "Epoch 00106: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1326 - MSE: 0.1326 - val_loss: 2.1347 - val_MSE: 2.1347 - lr: 0.0050\n",
            "Epoch 107/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.1541 - MSE: 0.1541\n",
            "Epoch 00107: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1541 - MSE: 0.1541 - val_loss: 2.1362 - val_MSE: 2.1362 - lr: 0.0050\n",
            "Epoch 108/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1655 - MSE: 0.1655\n",
            "Epoch 00108: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1640 - MSE: 0.1640 - val_loss: 1.9196 - val_MSE: 1.9196 - lr: 0.0050\n",
            "Epoch 109/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1468 - MSE: 0.1468\n",
            "Epoch 00109: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1491 - MSE: 0.1491 - val_loss: 1.9090 - val_MSE: 1.9090 - lr: 0.0050\n",
            "Epoch 110/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.1565 - MSE: 0.1565\n",
            "Epoch 00110: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1569 - MSE: 0.1569 - val_loss: 1.9291 - val_MSE: 1.9291 - lr: 0.0050\n",
            "Epoch 111/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1910 - MSE: 0.1910\n",
            "Epoch 00111: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1891 - MSE: 0.1891 - val_loss: 1.9455 - val_MSE: 1.9455 - lr: 0.0050\n",
            "Epoch 112/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.1583 - MSE: 0.1583\n",
            "Epoch 00112: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1576 - MSE: 0.1576 - val_loss: 2.1573 - val_MSE: 2.1573 - lr: 0.0050\n",
            "Epoch 113/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.1554 - MSE: 0.1554\n",
            "Epoch 00113: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1563 - MSE: 0.1563 - val_loss: 2.5487 - val_MSE: 2.5487 - lr: 0.0050\n",
            "Epoch 114/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.2095 - MSE: 0.2095\n",
            "Epoch 00114: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.2042 - MSE: 0.2042 - val_loss: 2.0439 - val_MSE: 2.0439 - lr: 0.0050\n",
            "Epoch 115/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.1726 - MSE: 0.1726\n",
            "Epoch 00115: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1720 - MSE: 0.1720 - val_loss: 2.0717 - val_MSE: 2.0717 - lr: 0.0050\n",
            "Epoch 116/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.2097 - MSE: 0.2097\n",
            "Epoch 00116: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.2081 - MSE: 0.2081 - val_loss: 2.1234 - val_MSE: 2.1234 - lr: 0.0050\n",
            "Epoch 117/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.1927 - MSE: 0.1927\n",
            "Epoch 00117: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1843 - MSE: 0.1843 - val_loss: 1.9986 - val_MSE: 1.9986 - lr: 0.0050\n",
            "Epoch 118/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.1471 - MSE: 0.1471\n",
            "Epoch 00118: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1471 - MSE: 0.1471 - val_loss: 2.0800 - val_MSE: 2.0800 - lr: 0.0050\n",
            "Epoch 119/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1622 - MSE: 0.1622\n",
            "Epoch 00119: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1601 - MSE: 0.1601 - val_loss: 1.9932 - val_MSE: 1.9932 - lr: 0.0050\n",
            "Epoch 120/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1520 - MSE: 0.1520\n",
            "Epoch 00120: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1522 - MSE: 0.1522 - val_loss: 1.8762 - val_MSE: 1.8762 - lr: 0.0050\n",
            "Epoch 121/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.1723 - MSE: 0.1723\n",
            "Epoch 00121: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1723 - MSE: 0.1723 - val_loss: 2.0731 - val_MSE: 2.0731 - lr: 0.0050\n",
            "Epoch 122/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.1270 - MSE: 0.1270\n",
            "Epoch 00122: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1307 - MSE: 0.1307 - val_loss: 1.9539 - val_MSE: 1.9539 - lr: 0.0050\n",
            "Epoch 123/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1388 - MSE: 0.1388\n",
            "Epoch 00123: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1367 - MSE: 0.1367 - val_loss: 2.3360 - val_MSE: 2.3360 - lr: 0.0050\n",
            "Epoch 124/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1422 - MSE: 0.1422\n",
            "Epoch 00124: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1399 - MSE: 0.1399 - val_loss: 2.1613 - val_MSE: 2.1613 - lr: 0.0050\n",
            "Epoch 125/1000\n",
            "131/145 [==========================>...] - ETA: 0s - loss: 0.1429 - MSE: 0.1429\n",
            "Epoch 00125: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1452 - MSE: 0.1452 - val_loss: 2.3234 - val_MSE: 2.3234 - lr: 0.0050\n",
            "Epoch 126/1000\n",
            "131/145 [==========================>...] - ETA: 0s - loss: 0.1411 - MSE: 0.1411\n",
            "Epoch 00126: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1383 - MSE: 0.1383 - val_loss: 2.1080 - val_MSE: 2.1080 - lr: 0.0050\n",
            "Epoch 127/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.1652 - MSE: 0.1652\n",
            "Epoch 00127: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1630 - MSE: 0.1630 - val_loss: 2.1558 - val_MSE: 2.1558 - lr: 0.0050\n",
            "Epoch 128/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.1423 - MSE: 0.1423\n",
            "Epoch 00128: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1365 - MSE: 0.1365 - val_loss: 2.1259 - val_MSE: 2.1259 - lr: 0.0050\n",
            "Epoch 129/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.1531 - MSE: 0.1531\n",
            "Epoch 00129: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1575 - MSE: 0.1575 - val_loss: 2.0447 - val_MSE: 2.0447 - lr: 0.0050\n",
            "Epoch 130/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1195 - MSE: 0.1195\n",
            "Epoch 00130: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1215 - MSE: 0.1215 - val_loss: 1.9615 - val_MSE: 1.9615 - lr: 0.0050\n",
            "Epoch 131/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1643 - MSE: 0.1643\n",
            "Epoch 00131: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1684 - MSE: 0.1684 - val_loss: 2.1575 - val_MSE: 2.1575 - lr: 0.0050\n",
            "Epoch 132/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.1472 - MSE: 0.1472\n",
            "Epoch 00132: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1472 - MSE: 0.1472 - val_loss: 2.1003 - val_MSE: 2.1003 - lr: 0.0050\n",
            "Epoch 133/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.1270 - MSE: 0.1270\n",
            "Epoch 00133: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1301 - MSE: 0.1301 - val_loss: 2.2042 - val_MSE: 2.2042 - lr: 0.0050\n",
            "Epoch 134/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.1735 - MSE: 0.1735\n",
            "Epoch 00134: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1804 - MSE: 0.1804 - val_loss: 2.0600 - val_MSE: 2.0600 - lr: 0.0050\n",
            "Epoch 135/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1411 - MSE: 0.1411\n",
            "Epoch 00135: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1401 - MSE: 0.1401 - val_loss: 2.4270 - val_MSE: 2.4270 - lr: 0.0050\n",
            "Epoch 136/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.1590 - MSE: 0.1590\n",
            "Epoch 00136: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1541 - MSE: 0.1541 - val_loss: 1.9703 - val_MSE: 1.9703 - lr: 0.0050\n",
            "Epoch 137/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1379 - MSE: 0.1379\n",
            "Epoch 00137: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1371 - MSE: 0.1371 - val_loss: 1.9809 - val_MSE: 1.9809 - lr: 0.0050\n",
            "Epoch 138/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.1347 - MSE: 0.1347\n",
            "Epoch 00138: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1358 - MSE: 0.1358 - val_loss: 1.9519 - val_MSE: 1.9519 - lr: 0.0050\n",
            "Epoch 139/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1277 - MSE: 0.1277\n",
            "Epoch 00139: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1326 - MSE: 0.1326 - val_loss: 1.9052 - val_MSE: 1.9052 - lr: 0.0050\n",
            "Epoch 140/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1245 - MSE: 0.1245\n",
            "Epoch 00140: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1276 - MSE: 0.1276 - val_loss: 2.2263 - val_MSE: 2.2263 - lr: 0.0050\n",
            "Epoch 141/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1530 - MSE: 0.1530\n",
            "Epoch 00141: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1505 - MSE: 0.1505 - val_loss: 1.8831 - val_MSE: 1.8831 - lr: 0.0050\n",
            "Epoch 142/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1630 - MSE: 0.1630\n",
            "Epoch 00142: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1636 - MSE: 0.1636 - val_loss: 1.9059 - val_MSE: 1.9059 - lr: 0.0050\n",
            "Epoch 143/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1407 - MSE: 0.1407\n",
            "Epoch 00143: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1418 - MSE: 0.1418 - val_loss: 1.9582 - val_MSE: 1.9582 - lr: 0.0050\n",
            "Epoch 144/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1145 - MSE: 0.1145\n",
            "Epoch 00144: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1200 - MSE: 0.1200 - val_loss: 2.0841 - val_MSE: 2.0841 - lr: 0.0050\n",
            "Epoch 145/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.1590 - MSE: 0.1590\n",
            "Epoch 00145: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1583 - MSE: 0.1583 - val_loss: 2.2243 - val_MSE: 2.2243 - lr: 0.0050\n",
            "Epoch 146/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1200 - MSE: 0.1200\n",
            "Epoch 00146: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1203 - MSE: 0.1203 - val_loss: 1.8361 - val_MSE: 1.8361 - lr: 0.0050\n",
            "Epoch 147/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.1211 - MSE: 0.1211\n",
            "Epoch 00147: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1258 - MSE: 0.1258 - val_loss: 2.2220 - val_MSE: 2.2220 - lr: 0.0050\n",
            "Epoch 148/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1352 - MSE: 0.1352\n",
            "Epoch 00148: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1362 - MSE: 0.1362 - val_loss: 1.9587 - val_MSE: 1.9587 - lr: 0.0050\n",
            "Epoch 149/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1961 - MSE: 0.1961\n",
            "Epoch 00149: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1914 - MSE: 0.1914 - val_loss: 2.2152 - val_MSE: 2.2152 - lr: 0.0050\n",
            "Epoch 150/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1210 - MSE: 0.1210\n",
            "Epoch 00150: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1213 - MSE: 0.1213 - val_loss: 1.9526 - val_MSE: 1.9526 - lr: 0.0050\n",
            "Epoch 151/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1155 - MSE: 0.1155\n",
            "Epoch 00151: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1147 - MSE: 0.1147 - val_loss: 2.0431 - val_MSE: 2.0431 - lr: 0.0050\n",
            "Epoch 152/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1466 - MSE: 0.1466\n",
            "Epoch 00152: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1478 - MSE: 0.1478 - val_loss: 1.8975 - val_MSE: 1.8975 - lr: 0.0050\n",
            "Epoch 153/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1228 - MSE: 0.1228\n",
            "Epoch 00153: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1224 - MSE: 0.1224 - val_loss: 2.0900 - val_MSE: 2.0900 - lr: 0.0050\n",
            "Epoch 154/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1305 - MSE: 0.1305\n",
            "Epoch 00154: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1309 - MSE: 0.1309 - val_loss: 1.9931 - val_MSE: 1.9931 - lr: 0.0050\n",
            "Epoch 155/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1224 - MSE: 0.1224\n",
            "Epoch 00155: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1234 - MSE: 0.1234 - val_loss: 1.9515 - val_MSE: 1.9515 - lr: 0.0050\n",
            "Epoch 156/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1094 - MSE: 0.1094\n",
            "Epoch 00156: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1107 - MSE: 0.1107 - val_loss: 2.1674 - val_MSE: 2.1674 - lr: 0.0050\n",
            "Epoch 157/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1338 - MSE: 0.1338\n",
            "Epoch 00157: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1342 - MSE: 0.1342 - val_loss: 2.0347 - val_MSE: 2.0347 - lr: 0.0050\n",
            "Epoch 158/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.1511 - MSE: 0.1511\n",
            "Epoch 00158: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1528 - MSE: 0.1528 - val_loss: 2.0873 - val_MSE: 2.0873 - lr: 0.0050\n",
            "Epoch 159/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1072 - MSE: 0.1072\n",
            "Epoch 00159: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1092 - MSE: 0.1092 - val_loss: 2.1751 - val_MSE: 2.1751 - lr: 0.0050\n",
            "Epoch 160/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1226 - MSE: 0.1226\n",
            "Epoch 00160: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1214 - MSE: 0.1214 - val_loss: 2.0435 - val_MSE: 2.0435 - lr: 0.0050\n",
            "Epoch 161/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1110 - MSE: 0.1110\n",
            "Epoch 00161: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1138 - MSE: 0.1138 - val_loss: 2.0117 - val_MSE: 2.0117 - lr: 0.0050\n",
            "Epoch 162/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1158 - MSE: 0.1158\n",
            "Epoch 00162: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1153 - MSE: 0.1153 - val_loss: 2.4215 - val_MSE: 2.4215 - lr: 0.0050\n",
            "Epoch 163/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1307 - MSE: 0.1307\n",
            "Epoch 00163: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1299 - MSE: 0.1299 - val_loss: 1.9291 - val_MSE: 1.9291 - lr: 0.0050\n",
            "Epoch 164/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0981 - MSE: 0.0981\n",
            "Epoch 00164: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0954 - MSE: 0.0954 - val_loss: 1.9978 - val_MSE: 1.9978 - lr: 0.0050\n",
            "Epoch 165/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1050 - MSE: 0.1050\n",
            "Epoch 00165: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1011 - MSE: 0.1011 - val_loss: 2.0303 - val_MSE: 2.0303 - lr: 0.0050\n",
            "Epoch 166/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0967 - MSE: 0.0967\n",
            "Epoch 00166: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0996 - MSE: 0.0996 - val_loss: 1.8425 - val_MSE: 1.8425 - lr: 0.0050\n",
            "Epoch 167/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1034 - MSE: 0.1034\n",
            "Epoch 00167: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1057 - MSE: 0.1057 - val_loss: 1.9645 - val_MSE: 1.9645 - lr: 0.0050\n",
            "Epoch 168/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1324 - MSE: 0.1324\n",
            "Epoch 00168: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1362 - MSE: 0.1362 - val_loss: 2.1326 - val_MSE: 2.1326 - lr: 0.0050\n",
            "Epoch 169/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1020 - MSE: 0.1020\n",
            "Epoch 00169: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1047 - MSE: 0.1047 - val_loss: 2.1345 - val_MSE: 2.1345 - lr: 0.0050\n",
            "Epoch 170/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1114 - MSE: 0.1114\n",
            "Epoch 00170: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1105 - MSE: 0.1105 - val_loss: 1.9811 - val_MSE: 1.9811 - lr: 0.0050\n",
            "Epoch 171/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0860 - MSE: 0.0860\n",
            "Epoch 00171: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0880 - MSE: 0.0880 - val_loss: 2.0281 - val_MSE: 2.0281 - lr: 0.0050\n",
            "Epoch 172/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0995 - MSE: 0.0995\n",
            "Epoch 00172: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1023 - MSE: 0.1023 - val_loss: 2.0296 - val_MSE: 2.0296 - lr: 0.0050\n",
            "Epoch 173/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1007 - MSE: 0.1007\n",
            "Epoch 00173: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0988 - MSE: 0.0988 - val_loss: 1.9864 - val_MSE: 1.9864 - lr: 0.0050\n",
            "Epoch 174/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.1024 - MSE: 0.1024\n",
            "Epoch 00174: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1032 - MSE: 0.1032 - val_loss: 1.9487 - val_MSE: 1.9487 - lr: 0.0050\n",
            "Epoch 175/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.1159 - MSE: 0.1159\n",
            "Epoch 00175: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1140 - MSE: 0.1140 - val_loss: 2.0636 - val_MSE: 2.0636 - lr: 0.0050\n",
            "Epoch 176/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1092 - MSE: 0.1092\n",
            "Epoch 00176: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1076 - MSE: 0.1076 - val_loss: 2.0139 - val_MSE: 2.0139 - lr: 0.0050\n",
            "Epoch 177/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0952 - MSE: 0.0952\n",
            "Epoch 00177: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1012 - MSE: 0.1012 - val_loss: 1.9180 - val_MSE: 1.9180 - lr: 0.0050\n",
            "Epoch 178/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.1076 - MSE: 0.1076\n",
            "Epoch 00178: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1076 - MSE: 0.1076 - val_loss: 1.9045 - val_MSE: 1.9045 - lr: 0.0050\n",
            "Epoch 179/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1097 - MSE: 0.1097\n",
            "Epoch 00179: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1106 - MSE: 0.1106 - val_loss: 1.9671 - val_MSE: 1.9671 - lr: 0.0050\n",
            "Epoch 180/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.1106 - MSE: 0.1106\n",
            "Epoch 00180: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1164 - MSE: 0.1164 - val_loss: 2.2259 - val_MSE: 2.2259 - lr: 0.0050\n",
            "Epoch 181/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.1272 - MSE: 0.1272\n",
            "Epoch 00181: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1261 - MSE: 0.1261 - val_loss: 1.9816 - val_MSE: 1.9816 - lr: 0.0050\n",
            "Epoch 182/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0961 - MSE: 0.0961\n",
            "Epoch 00182: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0959 - MSE: 0.0959 - val_loss: 1.8744 - val_MSE: 1.8744 - lr: 0.0050\n",
            "Epoch 183/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1365 - MSE: 0.1365\n",
            "Epoch 00183: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1353 - MSE: 0.1353 - val_loss: 2.2359 - val_MSE: 2.2359 - lr: 0.0050\n",
            "Epoch 184/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.1034 - MSE: 0.1034\n",
            "Epoch 00184: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1029 - MSE: 0.1029 - val_loss: 1.9363 - val_MSE: 1.9363 - lr: 0.0050\n",
            "Epoch 185/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1214 - MSE: 0.1214\n",
            "Epoch 00185: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1180 - MSE: 0.1180 - val_loss: 1.9315 - val_MSE: 1.9315 - lr: 0.0050\n",
            "Epoch 186/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0950 - MSE: 0.0950\n",
            "Epoch 00186: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0931 - MSE: 0.0931 - val_loss: 2.0746 - val_MSE: 2.0746 - lr: 0.0050\n",
            "Epoch 187/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0938 - MSE: 0.0938\n",
            "Epoch 00187: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0951 - MSE: 0.0951 - val_loss: 1.9584 - val_MSE: 1.9584 - lr: 0.0050\n",
            "Epoch 188/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0987 - MSE: 0.0987\n",
            "Epoch 00188: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1004 - MSE: 0.1004 - val_loss: 2.0249 - val_MSE: 2.0249 - lr: 0.0050\n",
            "Epoch 189/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0986 - MSE: 0.0986\n",
            "Epoch 00189: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1007 - MSE: 0.1007 - val_loss: 1.9740 - val_MSE: 1.9740 - lr: 0.0050\n",
            "Epoch 190/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1055 - MSE: 0.1055\n",
            "Epoch 00190: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1084 - MSE: 0.1084 - val_loss: 1.8810 - val_MSE: 1.8810 - lr: 0.0050\n",
            "Epoch 191/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1050 - MSE: 0.1050\n",
            "Epoch 00191: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1048 - MSE: 0.1048 - val_loss: 1.9736 - val_MSE: 1.9736 - lr: 0.0050\n",
            "Epoch 192/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0862 - MSE: 0.0862\n",
            "Epoch 00192: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0956 - MSE: 0.0956 - val_loss: 1.8500 - val_MSE: 1.8500 - lr: 0.0050\n",
            "Epoch 193/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.1154 - MSE: 0.1154\n",
            "Epoch 00193: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1150 - MSE: 0.1150 - val_loss: 1.9075 - val_MSE: 1.9075 - lr: 0.0050\n",
            "Epoch 194/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0965 - MSE: 0.0965\n",
            "Epoch 00194: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0959 - MSE: 0.0959 - val_loss: 1.9135 - val_MSE: 1.9135 - lr: 0.0050\n",
            "Epoch 195/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1067 - MSE: 0.1067\n",
            "Epoch 00195: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1097 - MSE: 0.1097 - val_loss: 2.1166 - val_MSE: 2.1166 - lr: 0.0050\n",
            "Epoch 196/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.1372 - MSE: 0.1372\n",
            "Epoch 00196: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1334 - MSE: 0.1334 - val_loss: 2.0909 - val_MSE: 2.0909 - lr: 0.0050\n",
            "Epoch 197/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1212 - MSE: 0.1212\n",
            "Epoch 00197: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1210 - MSE: 0.1210 - val_loss: 1.8618 - val_MSE: 1.8618 - lr: 0.0050\n",
            "Epoch 198/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0983 - MSE: 0.0983\n",
            "Epoch 00198: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0983 - MSE: 0.0983 - val_loss: 1.9730 - val_MSE: 1.9730 - lr: 0.0050\n",
            "Epoch 199/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0896 - MSE: 0.0896\n",
            "Epoch 00199: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0884 - MSE: 0.0884 - val_loss: 1.9556 - val_MSE: 1.9556 - lr: 0.0050\n",
            "Epoch 200/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0978 - MSE: 0.0978\n",
            "Epoch 00200: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0979 - MSE: 0.0979 - val_loss: 1.9473 - val_MSE: 1.9473 - lr: 0.0050\n",
            "Epoch 201/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0861 - MSE: 0.0861\n",
            "Epoch 00201: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0858 - MSE: 0.0858 - val_loss: 2.0719 - val_MSE: 2.0719 - lr: 0.0050\n",
            "Epoch 202/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1017 - MSE: 0.1017\n",
            "Epoch 00202: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1015 - MSE: 0.1015 - val_loss: 2.0984 - val_MSE: 2.0984 - lr: 0.0050\n",
            "Epoch 203/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0988 - MSE: 0.0988\n",
            "Epoch 00203: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0981 - MSE: 0.0981 - val_loss: 1.8658 - val_MSE: 1.8658 - lr: 0.0050\n",
            "Epoch 204/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0945 - MSE: 0.0945\n",
            "Epoch 00204: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0953 - MSE: 0.0953 - val_loss: 1.9849 - val_MSE: 1.9849 - lr: 0.0050\n",
            "Epoch 205/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0808 - MSE: 0.0808\n",
            "Epoch 00205: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0799 - MSE: 0.0799 - val_loss: 1.8982 - val_MSE: 1.8982 - lr: 0.0050\n",
            "Epoch 206/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0914 - MSE: 0.0914\n",
            "Epoch 00206: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0908 - MSE: 0.0908 - val_loss: 2.0089 - val_MSE: 2.0089 - lr: 0.0050\n",
            "Epoch 207/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0940 - MSE: 0.0940\n",
            "Epoch 00207: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0928 - MSE: 0.0928 - val_loss: 1.9896 - val_MSE: 1.9896 - lr: 0.0050\n",
            "Epoch 208/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0817 - MSE: 0.0817\n",
            "Epoch 00208: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0797 - MSE: 0.0797 - val_loss: 2.0209 - val_MSE: 2.0209 - lr: 0.0050\n",
            "Epoch 209/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0876 - MSE: 0.0876\n",
            "Epoch 00209: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0850 - MSE: 0.0850 - val_loss: 2.1768 - val_MSE: 2.1768 - lr: 0.0050\n",
            "Epoch 210/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1100 - MSE: 0.1100\n",
            "Epoch 00210: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1104 - MSE: 0.1104 - val_loss: 1.9340 - val_MSE: 1.9340 - lr: 0.0050\n",
            "Epoch 211/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0838 - MSE: 0.0838\n",
            "Epoch 00211: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0816 - MSE: 0.0816 - val_loss: 1.9571 - val_MSE: 1.9571 - lr: 0.0050\n",
            "Epoch 212/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.1178 - MSE: 0.1178\n",
            "Epoch 00212: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1160 - MSE: 0.1160 - val_loss: 2.1199 - val_MSE: 2.1199 - lr: 0.0050\n",
            "Epoch 213/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0892 - MSE: 0.0892\n",
            "Epoch 00213: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0888 - MSE: 0.0888 - val_loss: 1.8982 - val_MSE: 1.8982 - lr: 0.0050\n",
            "Epoch 214/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0775 - MSE: 0.0775\n",
            "Epoch 00214: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0777 - MSE: 0.0777 - val_loss: 1.9930 - val_MSE: 1.9930 - lr: 0.0050\n",
            "Epoch 215/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.1090 - MSE: 0.1090\n",
            "Epoch 00215: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1088 - MSE: 0.1088 - val_loss: 1.9273 - val_MSE: 1.9273 - lr: 0.0050\n",
            "Epoch 216/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0748 - MSE: 0.0748\n",
            "Epoch 00216: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0741 - MSE: 0.0741 - val_loss: 1.9588 - val_MSE: 1.9588 - lr: 0.0050\n",
            "Epoch 217/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0978 - MSE: 0.0978\n",
            "Epoch 00217: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0974 - MSE: 0.0974 - val_loss: 1.9608 - val_MSE: 1.9608 - lr: 0.0050\n",
            "Epoch 218/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0913 - MSE: 0.0913\n",
            "Epoch 00218: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0906 - MSE: 0.0906 - val_loss: 1.8571 - val_MSE: 1.8571 - lr: 0.0050\n",
            "Epoch 219/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0866 - MSE: 0.0866\n",
            "Epoch 00219: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0869 - MSE: 0.0869 - val_loss: 2.2778 - val_MSE: 2.2778 - lr: 0.0050\n",
            "Epoch 220/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.1020 - MSE: 0.1020\n",
            "Epoch 00220: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.1011 - MSE: 0.1011 - val_loss: 1.8588 - val_MSE: 1.8588 - lr: 0.0050\n",
            "Epoch 221/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.0806 - MSE: 0.0806\n",
            "Epoch 00221: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0789 - MSE: 0.0789 - val_loss: 1.9043 - val_MSE: 1.9043 - lr: 0.0050\n",
            "Epoch 222/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0722 - MSE: 0.0722\n",
            "Epoch 00222: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0723 - MSE: 0.0723 - val_loss: 2.0161 - val_MSE: 2.0161 - lr: 0.0050\n",
            "Epoch 223/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.1121 - MSE: 0.1121\n",
            "Epoch 00223: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1108 - MSE: 0.1108 - val_loss: 1.9718 - val_MSE: 1.9718 - lr: 0.0050\n",
            "Epoch 224/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0764 - MSE: 0.0764\n",
            "Epoch 00224: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0765 - MSE: 0.0765 - val_loss: 1.9724 - val_MSE: 1.9724 - lr: 0.0050\n",
            "Epoch 225/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0762 - MSE: 0.0762\n",
            "Epoch 00225: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0854 - MSE: 0.0854 - val_loss: 2.0193 - val_MSE: 2.0193 - lr: 0.0050\n",
            "Epoch 226/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0750 - MSE: 0.0750\n",
            "Epoch 00226: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0731 - MSE: 0.0731 - val_loss: 1.8649 - val_MSE: 1.8649 - lr: 0.0050\n",
            "Epoch 227/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0879 - MSE: 0.0879\n",
            "Epoch 00227: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0866 - MSE: 0.0866 - val_loss: 2.0772 - val_MSE: 2.0772 - lr: 0.0050\n",
            "Epoch 228/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0785 - MSE: 0.0785\n",
            "Epoch 00228: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0818 - MSE: 0.0818 - val_loss: 2.2508 - val_MSE: 2.2508 - lr: 0.0050\n",
            "Epoch 229/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0738 - MSE: 0.0738\n",
            "Epoch 00229: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0753 - MSE: 0.0753 - val_loss: 2.1788 - val_MSE: 2.1788 - lr: 0.0050\n",
            "Epoch 230/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0735 - MSE: 0.0735\n",
            "Epoch 00230: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0732 - MSE: 0.0732 - val_loss: 1.9149 - val_MSE: 1.9149 - lr: 0.0050\n",
            "Epoch 231/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0759 - MSE: 0.0759\n",
            "Epoch 00231: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0774 - MSE: 0.0774 - val_loss: 1.8702 - val_MSE: 1.8702 - lr: 0.0050\n",
            "Epoch 232/1000\n",
            "130/145 [=========================>....] - ETA: 0s - loss: 0.0764 - MSE: 0.0764\n",
            "Epoch 00232: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0739 - MSE: 0.0739 - val_loss: 1.9527 - val_MSE: 1.9527 - lr: 0.0050\n",
            "Epoch 233/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0804 - MSE: 0.0804\n",
            "Epoch 00233: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0799 - MSE: 0.0799 - val_loss: 1.9586 - val_MSE: 1.9586 - lr: 0.0050\n",
            "Epoch 234/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0838 - MSE: 0.0838\n",
            "Epoch 00234: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0838 - MSE: 0.0838 - val_loss: 2.0791 - val_MSE: 2.0791 - lr: 0.0050\n",
            "Epoch 235/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0985 - MSE: 0.0985\n",
            "Epoch 00235: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0971 - MSE: 0.0971 - val_loss: 1.9411 - val_MSE: 1.9411 - lr: 0.0050\n",
            "Epoch 236/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0922 - MSE: 0.0922\n",
            "Epoch 00236: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0954 - MSE: 0.0954 - val_loss: 2.0946 - val_MSE: 2.0946 - lr: 0.0050\n",
            "Epoch 237/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0806 - MSE: 0.0806\n",
            "Epoch 00237: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0802 - MSE: 0.0802 - val_loss: 1.8841 - val_MSE: 1.8841 - lr: 0.0050\n",
            "Epoch 238/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0666 - MSE: 0.0666\n",
            "Epoch 00238: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0655 - MSE: 0.0655 - val_loss: 1.9101 - val_MSE: 1.9101 - lr: 0.0050\n",
            "Epoch 239/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0732 - MSE: 0.0732\n",
            "Epoch 00239: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 4ms/step - loss: 0.0727 - MSE: 0.0727 - val_loss: 2.0230 - val_MSE: 2.0230 - lr: 0.0050\n",
            "Epoch 240/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0637 - MSE: 0.0637\n",
            "Epoch 00240: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0650 - MSE: 0.0650 - val_loss: 1.9780 - val_MSE: 1.9780 - lr: 0.0050\n",
            "Epoch 241/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0871 - MSE: 0.0871\n",
            "Epoch 00241: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0870 - MSE: 0.0870 - val_loss: 1.9919 - val_MSE: 1.9919 - lr: 0.0050\n",
            "Epoch 242/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0779 - MSE: 0.0779\n",
            "Epoch 00242: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0779 - MSE: 0.0779 - val_loss: 1.8910 - val_MSE: 1.8910 - lr: 0.0050\n",
            "Epoch 243/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0692 - MSE: 0.0692\n",
            "Epoch 00243: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0686 - MSE: 0.0686 - val_loss: 1.9083 - val_MSE: 1.9083 - lr: 0.0050\n",
            "Epoch 244/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0747 - MSE: 0.0747\n",
            "Epoch 00244: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0743 - MSE: 0.0743 - val_loss: 1.8546 - val_MSE: 1.8546 - lr: 0.0050\n",
            "Epoch 245/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0690 - MSE: 0.0690\n",
            "Epoch 00245: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0692 - MSE: 0.0692 - val_loss: 1.8806 - val_MSE: 1.8806 - lr: 0.0050\n",
            "Epoch 246/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0797 - MSE: 0.0797\n",
            "Epoch 00246: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0781 - MSE: 0.0781 - val_loss: 1.9849 - val_MSE: 1.9849 - lr: 0.0050\n",
            "Epoch 247/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0864 - MSE: 0.0864\n",
            "Epoch 00247: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0864 - MSE: 0.0864 - val_loss: 1.8331 - val_MSE: 1.8331 - lr: 0.0050\n",
            "Epoch 248/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0853 - MSE: 0.0853\n",
            "Epoch 00248: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0852 - MSE: 0.0852 - val_loss: 1.9297 - val_MSE: 1.9297 - lr: 0.0050\n",
            "Epoch 249/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0585 - MSE: 0.0585\n",
            "Epoch 00249: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0600 - MSE: 0.0600 - val_loss: 1.8952 - val_MSE: 1.8952 - lr: 0.0050\n",
            "Epoch 250/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0686 - MSE: 0.0686\n",
            "Epoch 00250: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0676 - MSE: 0.0676 - val_loss: 1.8587 - val_MSE: 1.8587 - lr: 0.0050\n",
            "Epoch 251/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0766 - MSE: 0.0766\n",
            "Epoch 00251: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0755 - MSE: 0.0755 - val_loss: 1.8092 - val_MSE: 1.8092 - lr: 0.0050\n",
            "Epoch 252/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0889 - MSE: 0.0889\n",
            "Epoch 00252: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0882 - MSE: 0.0882 - val_loss: 1.8268 - val_MSE: 1.8268 - lr: 0.0050\n",
            "Epoch 253/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0977 - MSE: 0.0977\n",
            "Epoch 00253: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0975 - MSE: 0.0975 - val_loss: 2.0181 - val_MSE: 2.0181 - lr: 0.0050\n",
            "Epoch 254/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0888 - MSE: 0.0888\n",
            "Epoch 00254: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0880 - MSE: 0.0880 - val_loss: 1.9048 - val_MSE: 1.9048 - lr: 0.0050\n",
            "Epoch 255/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.0709 - MSE: 0.0709\n",
            "Epoch 00255: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0759 - MSE: 0.0759 - val_loss: 1.9542 - val_MSE: 1.9542 - lr: 0.0050\n",
            "Epoch 256/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.1050 - MSE: 0.1050\n",
            "Epoch 00256: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1051 - MSE: 0.1051 - val_loss: 2.1377 - val_MSE: 2.1377 - lr: 0.0050\n",
            "Epoch 257/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0906 - MSE: 0.0906\n",
            "Epoch 00257: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0899 - MSE: 0.0899 - val_loss: 2.1366 - val_MSE: 2.1366 - lr: 0.0050\n",
            "Epoch 258/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0755 - MSE: 0.0755\n",
            "Epoch 00258: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0748 - MSE: 0.0748 - val_loss: 2.0487 - val_MSE: 2.0487 - lr: 0.0050\n",
            "Epoch 259/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.0858 - MSE: 0.0858\n",
            "Epoch 00259: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0860 - MSE: 0.0860 - val_loss: 2.0543 - val_MSE: 2.0543 - lr: 0.0050\n",
            "Epoch 260/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0740 - MSE: 0.0740\n",
            "Epoch 00260: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0729 - MSE: 0.0729 - val_loss: 2.0855 - val_MSE: 2.0855 - lr: 0.0050\n",
            "Epoch 261/1000\n",
            "132/145 [==========================>...] - ETA: 0s - loss: 0.0697 - MSE: 0.0697\n",
            "Epoch 00261: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0692 - MSE: 0.0692 - val_loss: 1.8824 - val_MSE: 1.8824 - lr: 0.0050\n",
            "Epoch 262/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0741 - MSE: 0.0741\n",
            "Epoch 00262: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0746 - MSE: 0.0746 - val_loss: 1.8408 - val_MSE: 1.8408 - lr: 0.0050\n",
            "Epoch 263/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0815 - MSE: 0.0815\n",
            "Epoch 00263: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0811 - MSE: 0.0811 - val_loss: 1.8578 - val_MSE: 1.8578 - lr: 0.0050\n",
            "Epoch 264/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0708 - MSE: 0.0708\n",
            "Epoch 00264: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0712 - MSE: 0.0712 - val_loss: 1.9130 - val_MSE: 1.9130 - lr: 0.0050\n",
            "Epoch 265/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0679 - MSE: 0.0679\n",
            "Epoch 00265: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0654 - MSE: 0.0654 - val_loss: 1.8837 - val_MSE: 1.8837 - lr: 0.0050\n",
            "Epoch 266/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0592 - MSE: 0.0592\n",
            "Epoch 00266: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0621 - MSE: 0.0621 - val_loss: 1.9886 - val_MSE: 1.9886 - lr: 0.0050\n",
            "Epoch 267/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0660 - MSE: 0.0660\n",
            "Epoch 00267: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0652 - MSE: 0.0652 - val_loss: 1.9544 - val_MSE: 1.9544 - lr: 0.0050\n",
            "Epoch 268/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0714 - MSE: 0.0714\n",
            "Epoch 00268: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0700 - MSE: 0.0700 - val_loss: 1.9905 - val_MSE: 1.9905 - lr: 0.0050\n",
            "Epoch 269/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0505 - MSE: 0.0505\n",
            "Epoch 00269: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0556 - MSE: 0.0556 - val_loss: 1.8215 - val_MSE: 1.8215 - lr: 0.0050\n",
            "Epoch 270/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0609 - MSE: 0.0609\n",
            "Epoch 00270: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0624 - MSE: 0.0624 - val_loss: 2.0057 - val_MSE: 2.0057 - lr: 0.0050\n",
            "Epoch 271/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0746 - MSE: 0.0746\n",
            "Epoch 00271: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 6ms/step - loss: 0.0745 - MSE: 0.0745 - val_loss: 1.9758 - val_MSE: 1.9758 - lr: 0.0050\n",
            "Epoch 272/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0698 - MSE: 0.0698\n",
            "Epoch 00272: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0688 - MSE: 0.0688 - val_loss: 1.9604 - val_MSE: 1.9604 - lr: 0.0050\n",
            "Epoch 273/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0821 - MSE: 0.0821\n",
            "Epoch 00273: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0826 - MSE: 0.0826 - val_loss: 1.9196 - val_MSE: 1.9196 - lr: 0.0050\n",
            "Epoch 274/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0779 - MSE: 0.0779\n",
            "Epoch 00274: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0765 - MSE: 0.0765 - val_loss: 2.0421 - val_MSE: 2.0421 - lr: 0.0050\n",
            "Epoch 275/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0779 - MSE: 0.0779\n",
            "Epoch 00275: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0770 - MSE: 0.0770 - val_loss: 1.9623 - val_MSE: 1.9623 - lr: 0.0050\n",
            "Epoch 276/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0588 - MSE: 0.0588\n",
            "Epoch 00276: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0590 - MSE: 0.0590 - val_loss: 1.8802 - val_MSE: 1.8802 - lr: 0.0050\n",
            "Epoch 277/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0641 - MSE: 0.0641\n",
            "Epoch 00277: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0622 - MSE: 0.0622 - val_loss: 1.9078 - val_MSE: 1.9078 - lr: 0.0050\n",
            "Epoch 278/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0643 - MSE: 0.0643\n",
            "Epoch 00278: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0642 - MSE: 0.0642 - val_loss: 1.9750 - val_MSE: 1.9750 - lr: 0.0050\n",
            "Epoch 279/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0768 - MSE: 0.0768\n",
            "Epoch 00279: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0754 - MSE: 0.0754 - val_loss: 1.9518 - val_MSE: 1.9518 - lr: 0.0050\n",
            "Epoch 280/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0814 - MSE: 0.0814\n",
            "Epoch 00280: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0815 - MSE: 0.0815 - val_loss: 2.0054 - val_MSE: 2.0054 - lr: 0.0050\n",
            "Epoch 281/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0777 - MSE: 0.0777\n",
            "Epoch 00281: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0777 - MSE: 0.0777 - val_loss: 1.7523 - val_MSE: 1.7523 - lr: 0.0050\n",
            "Epoch 282/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0827 - MSE: 0.0827\n",
            "Epoch 00282: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0825 - MSE: 0.0825 - val_loss: 2.0060 - val_MSE: 2.0060 - lr: 0.0050\n",
            "Epoch 283/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0750 - MSE: 0.0750\n",
            "Epoch 00283: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0765 - MSE: 0.0765 - val_loss: 1.8931 - val_MSE: 1.8931 - lr: 0.0050\n",
            "Epoch 284/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0684 - MSE: 0.0684\n",
            "Epoch 00284: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0684 - MSE: 0.0684 - val_loss: 1.9879 - val_MSE: 1.9879 - lr: 0.0050\n",
            "Epoch 285/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0794 - MSE: 0.0794\n",
            "Epoch 00285: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0778 - MSE: 0.0778 - val_loss: 1.9573 - val_MSE: 1.9573 - lr: 0.0050\n",
            "Epoch 286/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0678 - MSE: 0.0678\n",
            "Epoch 00286: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0677 - MSE: 0.0677 - val_loss: 1.8572 - val_MSE: 1.8572 - lr: 0.0050\n",
            "Epoch 287/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0642 - MSE: 0.0642\n",
            "Epoch 00287: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0637 - MSE: 0.0637 - val_loss: 2.0222 - val_MSE: 2.0222 - lr: 0.0050\n",
            "Epoch 288/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0639 - MSE: 0.0639\n",
            "Epoch 00288: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0659 - MSE: 0.0659 - val_loss: 1.8195 - val_MSE: 1.8195 - lr: 0.0050\n",
            "Epoch 289/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0666 - MSE: 0.0666\n",
            "Epoch 00289: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0653 - MSE: 0.0653 - val_loss: 1.9588 - val_MSE: 1.9588 - lr: 0.0050\n",
            "Epoch 290/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0633 - MSE: 0.0633\n",
            "Epoch 00290: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0617 - MSE: 0.0617 - val_loss: 1.8781 - val_MSE: 1.8781 - lr: 0.0050\n",
            "Epoch 291/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0558 - MSE: 0.0558\n",
            "Epoch 00291: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0558 - MSE: 0.0558 - val_loss: 1.9295 - val_MSE: 1.9295 - lr: 0.0050\n",
            "Epoch 292/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0811 - MSE: 0.0811\n",
            "Epoch 00292: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0791 - MSE: 0.0791 - val_loss: 1.9759 - val_MSE: 1.9759 - lr: 0.0050\n",
            "Epoch 293/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0659 - MSE: 0.0659\n",
            "Epoch 00293: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0661 - MSE: 0.0661 - val_loss: 1.8755 - val_MSE: 1.8755 - lr: 0.0050\n",
            "Epoch 294/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0664 - MSE: 0.0664\n",
            "Epoch 00294: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0664 - MSE: 0.0664 - val_loss: 1.8816 - val_MSE: 1.8816 - lr: 0.0050\n",
            "Epoch 295/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0755 - MSE: 0.0755\n",
            "Epoch 00295: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0771 - MSE: 0.0771 - val_loss: 1.9465 - val_MSE: 1.9465 - lr: 0.0050\n",
            "Epoch 296/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0670 - MSE: 0.0670\n",
            "Epoch 00296: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0672 - MSE: 0.0672 - val_loss: 1.8397 - val_MSE: 1.8397 - lr: 0.0050\n",
            "Epoch 297/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0664 - MSE: 0.0664\n",
            "Epoch 00297: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0679 - MSE: 0.0679 - val_loss: 1.8198 - val_MSE: 1.8198 - lr: 0.0050\n",
            "Epoch 298/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0637 - MSE: 0.0637\n",
            "Epoch 00298: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0641 - MSE: 0.0641 - val_loss: 1.7718 - val_MSE: 1.7718 - lr: 0.0050\n",
            "Epoch 299/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0585 - MSE: 0.0585\n",
            "Epoch 00299: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0581 - MSE: 0.0581 - val_loss: 1.9524 - val_MSE: 1.9524 - lr: 0.0050\n",
            "Epoch 300/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0492 - MSE: 0.0492\n",
            "Epoch 00300: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0486 - MSE: 0.0486 - val_loss: 1.8295 - val_MSE: 1.8295 - lr: 0.0050\n",
            "Epoch 301/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0509 - MSE: 0.0509\n",
            "Epoch 00301: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0507 - MSE: 0.0507 - val_loss: 1.9049 - val_MSE: 1.9049 - lr: 0.0050\n",
            "Epoch 302/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0612 - MSE: 0.0612\n",
            "Epoch 00302: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0616 - MSE: 0.0616 - val_loss: 2.0244 - val_MSE: 2.0244 - lr: 0.0050\n",
            "Epoch 303/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0608 - MSE: 0.0608\n",
            "Epoch 00303: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0630 - MSE: 0.0630 - val_loss: 1.8459 - val_MSE: 1.8459 - lr: 0.0050\n",
            "Epoch 304/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0605 - MSE: 0.0605\n",
            "Epoch 00304: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0597 - MSE: 0.0597 - val_loss: 1.9066 - val_MSE: 1.9066 - lr: 0.0050\n",
            "Epoch 305/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0664 - MSE: 0.0664\n",
            "Epoch 00305: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0661 - MSE: 0.0661 - val_loss: 1.8633 - val_MSE: 1.8633 - lr: 0.0050\n",
            "Epoch 306/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0544 - MSE: 0.0544\n",
            "Epoch 00306: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0548 - MSE: 0.0548 - val_loss: 1.9065 - val_MSE: 1.9065 - lr: 0.0050\n",
            "Epoch 307/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0631 - MSE: 0.0631\n",
            "Epoch 00307: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0681 - MSE: 0.0681 - val_loss: 1.8201 - val_MSE: 1.8201 - lr: 0.0050\n",
            "Epoch 308/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0722 - MSE: 0.0722\n",
            "Epoch 00308: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0706 - MSE: 0.0706 - val_loss: 1.9045 - val_MSE: 1.9045 - lr: 0.0050\n",
            "Epoch 309/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0840 - MSE: 0.0840\n",
            "Epoch 00309: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 6ms/step - loss: 0.0840 - MSE: 0.0840 - val_loss: 1.8582 - val_MSE: 1.8582 - lr: 0.0050\n",
            "Epoch 310/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0612 - MSE: 0.0612\n",
            "Epoch 00310: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0610 - MSE: 0.0610 - val_loss: 1.9456 - val_MSE: 1.9456 - lr: 0.0050\n",
            "Epoch 311/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0593 - MSE: 0.0593\n",
            "Epoch 00311: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0592 - MSE: 0.0592 - val_loss: 1.8490 - val_MSE: 1.8490 - lr: 0.0050\n",
            "Epoch 312/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0615 - MSE: 0.0615\n",
            "Epoch 00312: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0624 - MSE: 0.0624 - val_loss: 1.8689 - val_MSE: 1.8689 - lr: 0.0050\n",
            "Epoch 313/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0684 - MSE: 0.0684\n",
            "Epoch 00313: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0670 - MSE: 0.0670 - val_loss: 1.9319 - val_MSE: 1.9319 - lr: 0.0050\n",
            "Epoch 314/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0676 - MSE: 0.0676\n",
            "Epoch 00314: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0671 - MSE: 0.0671 - val_loss: 1.8727 - val_MSE: 1.8727 - lr: 0.0050\n",
            "Epoch 315/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0581 - MSE: 0.0581\n",
            "Epoch 00315: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0579 - MSE: 0.0579 - val_loss: 1.9660 - val_MSE: 1.9660 - lr: 0.0050\n",
            "Epoch 316/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0507 - MSE: 0.0507\n",
            "Epoch 00316: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0506 - MSE: 0.0506 - val_loss: 1.8627 - val_MSE: 1.8627 - lr: 0.0050\n",
            "Epoch 317/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0521 - MSE: 0.0521\n",
            "Epoch 00317: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0520 - MSE: 0.0520 - val_loss: 1.8893 - val_MSE: 1.8893 - lr: 0.0050\n",
            "Epoch 318/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0711 - MSE: 0.0711\n",
            "Epoch 00318: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0715 - MSE: 0.0715 - val_loss: 1.8935 - val_MSE: 1.8935 - lr: 0.0050\n",
            "Epoch 319/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0726 - MSE: 0.0726\n",
            "Epoch 00319: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0739 - MSE: 0.0739 - val_loss: 1.7201 - val_MSE: 1.7201 - lr: 0.0050\n",
            "Epoch 320/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0542 - MSE: 0.0542\n",
            "Epoch 00320: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0538 - MSE: 0.0538 - val_loss: 1.9224 - val_MSE: 1.9224 - lr: 0.0050\n",
            "Epoch 321/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0534 - MSE: 0.0534\n",
            "Epoch 00321: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 6ms/step - loss: 0.0534 - MSE: 0.0534 - val_loss: 1.8161 - val_MSE: 1.8161 - lr: 0.0050\n",
            "Epoch 322/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0556 - MSE: 0.0556\n",
            "Epoch 00322: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0556 - MSE: 0.0556 - val_loss: 1.9012 - val_MSE: 1.9012 - lr: 0.0050\n",
            "Epoch 323/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0488 - MSE: 0.0488\n",
            "Epoch 00323: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0496 - MSE: 0.0496 - val_loss: 2.0273 - val_MSE: 2.0273 - lr: 0.0050\n",
            "Epoch 324/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0642 - MSE: 0.0642\n",
            "Epoch 00324: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0653 - MSE: 0.0653 - val_loss: 1.7493 - val_MSE: 1.7493 - lr: 0.0050\n",
            "Epoch 325/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.1095 - MSE: 0.1095\n",
            "Epoch 00325: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.1116 - MSE: 0.1116 - val_loss: 1.7698 - val_MSE: 1.7698 - lr: 0.0050\n",
            "Epoch 326/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0711 - MSE: 0.0711\n",
            "Epoch 00326: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0699 - MSE: 0.0699 - val_loss: 1.9611 - val_MSE: 1.9611 - lr: 0.0050\n",
            "Epoch 327/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0622 - MSE: 0.0622\n",
            "Epoch 00327: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0620 - MSE: 0.0620 - val_loss: 1.8201 - val_MSE: 1.8201 - lr: 0.0050\n",
            "Epoch 328/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0500 - MSE: 0.0500\n",
            "Epoch 00328: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0516 - MSE: 0.0516 - val_loss: 1.7071 - val_MSE: 1.7071 - lr: 0.0050\n",
            "Epoch 329/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0750 - MSE: 0.0750\n",
            "Epoch 00329: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0740 - MSE: 0.0740 - val_loss: 1.7966 - val_MSE: 1.7966 - lr: 0.0050\n",
            "Epoch 330/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0524 - MSE: 0.0524\n",
            "Epoch 00330: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0512 - MSE: 0.0512 - val_loss: 1.8668 - val_MSE: 1.8668 - lr: 0.0050\n",
            "Epoch 331/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0534 - MSE: 0.0534\n",
            "Epoch 00331: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0537 - MSE: 0.0537 - val_loss: 1.9144 - val_MSE: 1.9144 - lr: 0.0050\n",
            "Epoch 332/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0442 - MSE: 0.0442\n",
            "Epoch 00332: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0478 - MSE: 0.0478 - val_loss: 1.9166 - val_MSE: 1.9166 - lr: 0.0050\n",
            "Epoch 333/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0520 - MSE: 0.0520\n",
            "Epoch 00333: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0530 - MSE: 0.0530 - val_loss: 1.7623 - val_MSE: 1.7623 - lr: 0.0050\n",
            "Epoch 334/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0587 - MSE: 0.0587\n",
            "Epoch 00334: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0583 - MSE: 0.0583 - val_loss: 1.9647 - val_MSE: 1.9647 - lr: 0.0050\n",
            "Epoch 335/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0497 - MSE: 0.0497\n",
            "Epoch 00335: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0507 - MSE: 0.0507 - val_loss: 1.8203 - val_MSE: 1.8203 - lr: 0.0050\n",
            "Epoch 336/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0476 - MSE: 0.0476\n",
            "Epoch 00336: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0474 - MSE: 0.0474 - val_loss: 1.9843 - val_MSE: 1.9843 - lr: 0.0050\n",
            "Epoch 337/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0712 - MSE: 0.0712\n",
            "Epoch 00337: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0721 - MSE: 0.0721 - val_loss: 1.9955 - val_MSE: 1.9955 - lr: 0.0050\n",
            "Epoch 338/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0504 - MSE: 0.0504\n",
            "Epoch 00338: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0504 - MSE: 0.0504 - val_loss: 2.0761 - val_MSE: 2.0761 - lr: 0.0050\n",
            "Epoch 339/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0670 - MSE: 0.0670\n",
            "Epoch 00339: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0666 - MSE: 0.0666 - val_loss: 1.8849 - val_MSE: 1.8849 - lr: 0.0050\n",
            "Epoch 340/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0539 - MSE: 0.0539\n",
            "Epoch 00340: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0522 - MSE: 0.0522 - val_loss: 1.9024 - val_MSE: 1.9024 - lr: 0.0050\n",
            "Epoch 341/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0617 - MSE: 0.0617\n",
            "Epoch 00341: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0615 - MSE: 0.0615 - val_loss: 1.9299 - val_MSE: 1.9299 - lr: 0.0050\n",
            "Epoch 342/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0536 - MSE: 0.0536\n",
            "Epoch 00342: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0531 - MSE: 0.0531 - val_loss: 1.9103 - val_MSE: 1.9103 - lr: 0.0050\n",
            "Epoch 343/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0468 - MSE: 0.0468\n",
            "Epoch 00343: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0466 - MSE: 0.0466 - val_loss: 1.9265 - val_MSE: 1.9265 - lr: 0.0050\n",
            "Epoch 344/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0556 - MSE: 0.0556\n",
            "Epoch 00344: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0574 - MSE: 0.0574 - val_loss: 1.9031 - val_MSE: 1.9031 - lr: 0.0050\n",
            "Epoch 345/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0541 - MSE: 0.0541\n",
            "Epoch 00345: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0538 - MSE: 0.0538 - val_loss: 1.9633 - val_MSE: 1.9633 - lr: 0.0050\n",
            "Epoch 346/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0729 - MSE: 0.0729\n",
            "Epoch 00346: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0726 - MSE: 0.0726 - val_loss: 1.8919 - val_MSE: 1.8919 - lr: 0.0050\n",
            "Epoch 347/1000\n",
            "133/145 [==========================>...] - ETA: 0s - loss: 0.0644 - MSE: 0.0644\n",
            "Epoch 00347: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0655 - MSE: 0.0655 - val_loss: 1.8525 - val_MSE: 1.8525 - lr: 0.0050\n",
            "Epoch 348/1000\n",
            "143/145 [============================>.] - ETA: 0s - loss: 0.0613 - MSE: 0.0613\n",
            "Epoch 00348: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0608 - MSE: 0.0608 - val_loss: 1.8526 - val_MSE: 1.8526 - lr: 0.0050\n",
            "Epoch 349/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0558 - MSE: 0.0558\n",
            "Epoch 00349: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0558 - MSE: 0.0558 - val_loss: 1.8024 - val_MSE: 1.8024 - lr: 0.0050\n",
            "Epoch 350/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0679 - MSE: 0.0679\n",
            "Epoch 00350: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0676 - MSE: 0.0676 - val_loss: 1.8736 - val_MSE: 1.8736 - lr: 0.0050\n",
            "Epoch 351/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0507 - MSE: 0.0507\n",
            "Epoch 00351: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0498 - MSE: 0.0498 - val_loss: 1.9523 - val_MSE: 1.9523 - lr: 0.0050\n",
            "Epoch 352/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0639 - MSE: 0.0639\n",
            "Epoch 00352: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0634 - MSE: 0.0634 - val_loss: 2.1252 - val_MSE: 2.1252 - lr: 0.0050\n",
            "Epoch 353/1000\n",
            "141/145 [============================>.] - ETA: 0s - loss: 0.0622 - MSE: 0.0622\n",
            "Epoch 00353: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0623 - MSE: 0.0623 - val_loss: 1.8535 - val_MSE: 1.8535 - lr: 0.0050\n",
            "Epoch 354/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0657 - MSE: 0.0657\n",
            "Epoch 00354: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0656 - MSE: 0.0656 - val_loss: 1.8959 - val_MSE: 1.8959 - lr: 0.0050\n",
            "Epoch 355/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0585 - MSE: 0.0585\n",
            "Epoch 00355: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0652 - MSE: 0.0652 - val_loss: 1.8122 - val_MSE: 1.8122 - lr: 0.0050\n",
            "Epoch 356/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0520 - MSE: 0.0520\n",
            "Epoch 00356: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0522 - MSE: 0.0522 - val_loss: 1.8319 - val_MSE: 1.8319 - lr: 0.0050\n",
            "Epoch 357/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0432 - MSE: 0.0432\n",
            "Epoch 00357: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0458 - MSE: 0.0458 - val_loss: 1.8788 - val_MSE: 1.8788 - lr: 0.0050\n",
            "Epoch 358/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0558 - MSE: 0.0558\n",
            "Epoch 00358: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0551 - MSE: 0.0551 - val_loss: 1.8444 - val_MSE: 1.8444 - lr: 0.0050\n",
            "Epoch 359/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0545 - MSE: 0.0545\n",
            "Epoch 00359: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0545 - MSE: 0.0545 - val_loss: 1.8063 - val_MSE: 1.8063 - lr: 0.0050\n",
            "Epoch 360/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0452 - MSE: 0.0452\n",
            "Epoch 00360: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0486 - MSE: 0.0486 - val_loss: 1.7678 - val_MSE: 1.7678 - lr: 0.0050\n",
            "Epoch 361/1000\n",
            "139/145 [===========================>..] - ETA: 0s - loss: 0.0500 - MSE: 0.0500\n",
            "Epoch 00361: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0486 - MSE: 0.0486 - val_loss: 1.8081 - val_MSE: 1.8081 - lr: 0.0050\n",
            "Epoch 362/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0478 - MSE: 0.0478\n",
            "Epoch 00362: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0484 - MSE: 0.0484 - val_loss: 1.9641 - val_MSE: 1.9641 - lr: 0.0050\n",
            "Epoch 363/1000\n",
            "145/145 [==============================] - ETA: 0s - loss: 0.0483 - MSE: 0.0483\n",
            "Epoch 00363: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0483 - MSE: 0.0483 - val_loss: 1.8826 - val_MSE: 1.8826 - lr: 0.0050\n",
            "Epoch 364/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0435 - MSE: 0.0435\n",
            "Epoch 00364: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0472 - MSE: 0.0472 - val_loss: 1.9096 - val_MSE: 1.9096 - lr: 0.0050\n",
            "Epoch 365/1000\n",
            "135/145 [==========================>...] - ETA: 0s - loss: 0.0583 - MSE: 0.0583\n",
            "Epoch 00365: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0587 - MSE: 0.0587 - val_loss: 1.8621 - val_MSE: 1.8621 - lr: 0.0050\n",
            "Epoch 366/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0581 - MSE: 0.0581\n",
            "Epoch 00366: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0576 - MSE: 0.0576 - val_loss: 1.8650 - val_MSE: 1.8650 - lr: 0.0050\n",
            "Epoch 367/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0587 - MSE: 0.0587\n",
            "Epoch 00367: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0579 - MSE: 0.0579 - val_loss: 1.9049 - val_MSE: 1.9049 - lr: 0.0050\n",
            "Epoch 368/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0539 - MSE: 0.0539\n",
            "Epoch 00368: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0539 - MSE: 0.0539 - val_loss: 1.9320 - val_MSE: 1.9320 - lr: 0.0050\n",
            "Epoch 369/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0569 - MSE: 0.0569\n",
            "Epoch 00369: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0572 - MSE: 0.0572 - val_loss: 1.9154 - val_MSE: 1.9154 - lr: 0.0050\n",
            "Epoch 370/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0459 - MSE: 0.0459\n",
            "Epoch 00370: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0513 - MSE: 0.0513 - val_loss: 1.9269 - val_MSE: 1.9269 - lr: 0.0050\n",
            "Epoch 371/1000\n",
            "142/145 [============================>.] - ETA: 0s - loss: 0.0570 - MSE: 0.0570\n",
            "Epoch 00371: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0563 - MSE: 0.0563 - val_loss: 1.8316 - val_MSE: 1.8316 - lr: 0.0050\n",
            "Epoch 372/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0449 - MSE: 0.0449\n",
            "Epoch 00372: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0451 - MSE: 0.0451 - val_loss: 1.9189 - val_MSE: 1.9189 - lr: 0.0050\n",
            "Epoch 373/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0438 - MSE: 0.0438\n",
            "Epoch 00373: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0446 - MSE: 0.0446 - val_loss: 1.9136 - val_MSE: 1.9136 - lr: 0.0050\n",
            "Epoch 374/1000\n",
            "137/145 [===========================>..] - ETA: 0s - loss: 0.0527 - MSE: 0.0527\n",
            "Epoch 00374: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0510 - MSE: 0.0510 - val_loss: 1.8972 - val_MSE: 1.8972 - lr: 0.0050\n",
            "Epoch 375/1000\n",
            "136/145 [===========================>..] - ETA: 0s - loss: 0.0514 - MSE: 0.0514\n",
            "Epoch 00375: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0513 - MSE: 0.0513 - val_loss: 1.9052 - val_MSE: 1.9052 - lr: 0.0050\n",
            "Epoch 376/1000\n",
            "134/145 [==========================>...] - ETA: 0s - loss: 0.0438 - MSE: 0.0438\n",
            "Epoch 00376: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0446 - MSE: 0.0446 - val_loss: 1.9466 - val_MSE: 1.9466 - lr: 0.0050\n",
            "Epoch 377/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0515 - MSE: 0.0515\n",
            "Epoch 00377: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0518 - MSE: 0.0518 - val_loss: 1.7970 - val_MSE: 1.7970 - lr: 0.0050\n",
            "Epoch 378/1000\n",
            "144/145 [============================>.] - ETA: 0s - loss: 0.0464 - MSE: 0.0464\n",
            "Epoch 00378: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0464 - MSE: 0.0464 - val_loss: 1.9144 - val_MSE: 1.9144 - lr: 0.0050\n",
            "Epoch 379/1000\n",
            "140/145 [===========================>..] - ETA: 0s - loss: 0.0537 - MSE: 0.0537\n",
            "Epoch 00379: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0536 - MSE: 0.0536 - val_loss: 1.9017 - val_MSE: 1.9017 - lr: 0.0050\n",
            "Epoch 380/1000\n",
            "138/145 [===========================>..] - ETA: 0s - loss: 0.0608 - MSE: 0.0608\n",
            "Epoch 00380: val_loss did not improve from 1.28736\n",
            "145/145 [==============================] - 1s 5ms/step - loss: 0.0603 - MSE: 0.0603 - val_loss: 2.0156 - val_MSE: 2.0156 - lr: 0.0050\n",
            "Epoch 381/1000\n",
            "101/145 [===================>..........] - ETA: 0s - loss: 0.0446 - MSE: 0.0446"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ihqdnY6Vm2Yd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#new_model_2.load_weights(\"./_col_0.00098_0.00173_.xhdf5\")"
      ],
      "metadata": {
        "id": "6lS9W3aAu2v8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "eSVDad6L_Fgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_model_2.load_weights(\"./regression__0.04629_0.03222_.hdf5\")"
      ],
      "metadata": {
        "id": "BKodgIskMiCb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred=new_model_2.predict(X)\n",
        "result_df=pd.DataFrame()\n",
        "y_pred_col=[i[0] for i in y_pred]\n",
        "y_col=[i[0] for i in y]\n",
        "\n",
        "result_df[\"y\"]=y_col\n",
        "result_df[\"y_pred\"]=y_pred_col"
      ],
      "metadata": {
        "id": "4YcPfNhoDkVz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import plotly.express as px\n",
        "\n",
        "fig = px.scatter(result_df, x=\"y\", y=\"y_pred\",width=600, height=400 )\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "8JvY2HWxIQQA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "hGqdbRn7lYIb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.metrics import accuracy_score,mean_absolute_error\n",
        "\n",
        "\n",
        "clf = RandomForestRegressor(max_depth=70, random_state=0,n_estimators=300)\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "randomforest_predict=clf.predict(X_train)\n"
      ],
      "metadata": {
        "id": "CBgYvOPdECYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Accuracy: {mean_absolute_error(y_train,randomforest_predict)}\")"
      ],
      "metadata": {
        "id": "fxJVYoR0EJ5j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "randomforest_predict=clf.predict(X_test)\n",
        "print(f\"Accuracy: {mean_absolute_error(y_test,randomforest_predict)}\")"
      ],
      "metadata": {
        "id": "ski4ZlVBExqH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "3cjQyvPz8Tz0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y"
      ],
      "metadata": {
        "id": "4HFebOdZ_4Vp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clf = LazyClassifier(verbose=1,ignore_warnings=True, custom_metric=None)\n",
        "models,predictions = clf.fit(X[:-100], X[-100:], y[:-100], y[-100:])"
      ],
      "metadata": {
        "id": "Kls-KXivMt5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(models)"
      ],
      "metadata": {
        "id": "JDWbhHRM_r_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "yo=[i[0] for i in y_]\n",
        "yo"
      ],
      "metadata": {
        "id": "QuoQjjVD_Bwg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X"
      ],
      "metadata": {
        "id": "apnbaoAkC09G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from lazypredict.Supervised import LazyRegressor\n",
        "reg = LazyRegressor(verbose=2, ignore_warnings=False, custom_metric=None)\n",
        "models,predictions = clf.fit(X[:-100], X[-100:], y[:-100], y[-100:])"
      ],
      "metadata": {
        "id": "6KPqXIiH-Ycc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "v4OdEx1rEAqx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(models)"
      ],
      "metadata": {
        "id": "9Wn6o2lY80q5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_counter=1"
      ],
      "metadata": {
        "id": "sHoy7pnWM84V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=X[:-1*x_counter]\n",
        "X_test=X[-1*x_counter:]\n",
        "y_train=yo[:-1*x_counter]\n",
        "y_test =yo[-1*x_counter:]\n"
      ],
      "metadata": {
        "id": "3tl2wZDeEWsD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}